{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Optimization of Neural Networks - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recall from the last lab that we had a training accuracy close to 90% and a test set accuracy close to 76%.\n",
    "\n",
    "As with our previous machine learning work, we should be asking a couple of questions:\n",
    "- Is there a high bias? yes/no\n",
    "- Is there a high variance? yes/no\n",
    "\n",
    "Also recall that \"high bias\" is a relative concept. Knowing we have 7 classes and the topics are related, we'll assume that a 90% accuracy is pretty good and the bias on the training set is low. (We've also discussed concepts like precision, recall as well as AUC and ROC curves.)   \n",
    "\n",
    "In this lab, we'll use the notion of training/validation/test set to get better insights of how we can mitigate our variance, and we'll look at a few regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. Then, just before you go on to train the model, we'll introduce how to include a validation set. You'll then define and compile the model as before. This time, when you are presented with the `history` dictionary of the model, you will have additional data entries for not only the train and test, but the train, test and validation  and then defigning, compiling and training the model. \n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Construct and run a basic model in Keras\n",
    "* Construct a validation set and explain potential benefits\n",
    "* Apply L1 and L2 regularization\n",
    "* Aplly dropout regularization\n",
    "* Observe and comment on the effect of using more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "As usual, start by importing some of the packages and modules that you intend to use. The first thing we'll be doing is importing the data and taking a random sample, so that should clue you in to what tools to import. If you need more tools down the line, you can always import additional packages later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; import some packages/modules you plan to use\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import models, layers, optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "As with the previous lab, the data is stored in a file **Bank_complaints.csv**. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am being contacted by a debt collector for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I cosigned XXXX student loans at SallieMae for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>Navient has sytematically and illegally failed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My wife became eligible for XXXX Loan Forgiven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
       "1  Student loan  I am being contacted by a debt collector for p...\n",
       "2  Student loan  I cosigned XXXX student loans at SallieMae for...\n",
       "3  Student loan  Navient has sytematically and illegally failed...\n",
       "4  Student loan  My wife became eligible for XXXX Loan Forgiven..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; load and preview the dataset\n",
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before we begin to practice some of our new tools regarding regularization and optimization, let's practice munging some data as we did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* One-hot encoding our complaint text\n",
    "* Transforming our category labels\n",
    "* Train - test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since we have quite a bit of data and training networks takes a substantial amount of time and resources, we will downsample in order to test our initial pipeline. Going forward, these can be interesting areas of investigation: how does our models performance change as we increase (or decrease) the size of our dataset?  \n",
    "\n",
    "Generate the random sample using seed 123 for consistency of results. Make your new sample have 10,000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "np.random.seed = 123\n",
    "df = df.sample(10000, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding of the Complaints\n",
    "\n",
    "As before, we need to do some preprocessing and data manipulationg before building the neural network. Last time, we guided you through the process, and now its time for you to practice that pipeline independently.  \n",
    "\n",
    "Only keep 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences type: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "complaints = df['Consumer complaint narrative']\n",
    "\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "print('sequences type:',type(sequences))\n",
    "\n",
    "one_hot_results = tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "word_lookup = {v:k for k,v in word_index.items()}\n",
    "#Your code here; use one-hot encoding to reformat the complaints into a matrix of vectors.\n",
    "#Only keep the 2000 most common words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "  (Note: this is similar to our previous work with dummy variables: each of the various product categories will be its own column, and each observation will be a row. Each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Student loan',\n",
       " 'Consumer Loan',\n",
       " 'Credit card',\n",
       " 'Bank account or service',\n",
       " 'Mortgage',\n",
       " 'Credit reporting',\n",
       " 'Checking or savings account']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.Product.value_counts().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; transform the product labels to numerical values\n",
    "#Then transform these integer values into a matrix of binary flags\n",
    "product = df.Product\n",
    "complaints = df['Consumer complaint narrative']\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n",
    "le.fit(product)\n",
    "product_cat = le.transform(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "product_one_hot = to_categorical(product_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split\n",
    "\n",
    "Now onto the ever familiar train-test split! Be sure to split both the complaint data (now transformed into word vectors) as well as their associated labels. Perform an appropriate train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "X_train, X_test,y_train, y_test = train_test_split(one_hot_results, product_one_hot, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we mentioned that in deep learning, we generally keep aside a validation set, which is used during hyperparameter tuning. Then when we have made the final model decision, the test set is used to define the final model perforance. \n",
    "\n",
    "In this example, let's take the first 1000 cases out of the training set to become the validation set. You should do this for both `train` and `label_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just run this block of code \n",
    "np.random.seed = 123\n",
    "val = X_train[:1000]\n",
    "train_final = X_train[1000:]\n",
    "label_val = y_train[:1000]\n",
    "label_train_final = y_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because we are dealing with a multiclass problem (classifying the complaints into 7 classes), we use a use a softmax classifyer in order to output 7 class probabilities per case.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; build a neural network using Keras as described above.\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,)))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "In the compiler, you'll be passing the optimizer, loss function, and metrics. Train the model for 120 epochs in mini-batches of 256 samples. This time, let's include the argument `validation_data` and assign it `(val, label_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "model.compile(optimizer='SGD', loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Code Along\n",
    "\n",
    "The remaining portion of this lab will introduce you to code snippets for a myriad of different methods discussed in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Ok, now for the resource intensive part: time to train our model! Note that this is where we also introduce the validation data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\james\\Anaconda3\\envs\\learn-env-ext\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 6500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "6500/6500 [==============================] - 1s 85us/step - loss: 1.9401 - acc: 0.1537 - val_loss: 1.9299 - val_acc: 0.1830\n",
      "Epoch 2/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.9192 - acc: 0.1918 - val_loss: 1.9120 - val_acc: 0.2050\n",
      "Epoch 3/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.9014 - acc: 0.2234 - val_loss: 1.8944 - val_acc: 0.2120\n",
      "Epoch 4/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.8822 - acc: 0.2422 - val_loss: 1.8743 - val_acc: 0.2350\n",
      "Epoch 5/120\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.8599 - acc: 0.2691 - val_loss: 1.8508 - val_acc: 0.2580\n",
      "Epoch 6/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.8338 - acc: 0.2895 - val_loss: 1.8236 - val_acc: 0.2800\n",
      "Epoch 7/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.8033 - acc: 0.3157 - val_loss: 1.7921 - val_acc: 0.3160\n",
      "Epoch 8/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.7684 - acc: 0.3491 - val_loss: 1.7569 - val_acc: 0.3510\n",
      "Epoch 9/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.7297 - acc: 0.3809 - val_loss: 1.7188 - val_acc: 0.3710\n",
      "Epoch 10/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.6881 - acc: 0.4062 - val_loss: 1.6801 - val_acc: 0.4120\n",
      "Epoch 11/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.6446 - acc: 0.4338 - val_loss: 1.6364 - val_acc: 0.4250\n",
      "Epoch 12/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 1.5986 - acc: 0.4549 - val_loss: 1.5933 - val_acc: 0.4520\n",
      "Epoch 13/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.5513 - acc: 0.4858 - val_loss: 1.5479 - val_acc: 0.4810\n",
      "Epoch 14/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.5030 - acc: 0.5057 - val_loss: 1.5014 - val_acc: 0.5050\n",
      "Epoch 15/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.4545 - acc: 0.5314 - val_loss: 1.4559 - val_acc: 0.5210\n",
      "Epoch 16/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.4058 - acc: 0.5494 - val_loss: 1.4109 - val_acc: 0.5370\n",
      "Epoch 17/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.3582 - acc: 0.5734 - val_loss: 1.3660 - val_acc: 0.5570\n",
      "Epoch 18/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.3116 - acc: 0.5917 - val_loss: 1.3237 - val_acc: 0.5750\n",
      "Epoch 19/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.2668 - acc: 0.6069 - val_loss: 1.2832 - val_acc: 0.5850\n",
      "Epoch 20/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.2241 - acc: 0.6225 - val_loss: 1.2447 - val_acc: 0.6000\n",
      "Epoch 21/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.1838 - acc: 0.6358 - val_loss: 1.2076 - val_acc: 0.6100\n",
      "Epoch 22/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.1455 - acc: 0.6448 - val_loss: 1.1723 - val_acc: 0.6230\n",
      "Epoch 23/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.1093 - acc: 0.6552 - val_loss: 1.1411 - val_acc: 0.6330\n",
      "Epoch 24/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.0762 - acc: 0.6617 - val_loss: 1.1104 - val_acc: 0.6390\n",
      "Epoch 25/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.0450 - acc: 0.6680 - val_loss: 1.0823 - val_acc: 0.6430\n",
      "Epoch 26/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.0159 - acc: 0.6769 - val_loss: 1.0583 - val_acc: 0.6480\n",
      "Epoch 27/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9889 - acc: 0.6791 - val_loss: 1.0339 - val_acc: 0.6590\n",
      "Epoch 28/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.9641 - acc: 0.6846 - val_loss: 1.0097 - val_acc: 0.6590\n",
      "Epoch 29/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9406 - acc: 0.6895 - val_loss: 0.9901 - val_acc: 0.6640\n",
      "Epoch 30/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.9194 - acc: 0.6971 - val_loss: 0.9714 - val_acc: 0.6660\n",
      "Epoch 31/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8995 - acc: 0.6995 - val_loss: 0.9558 - val_acc: 0.6690\n",
      "Epoch 32/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8805 - acc: 0.7031 - val_loss: 0.9373 - val_acc: 0.6730\n",
      "Epoch 33/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8630 - acc: 0.7083 - val_loss: 0.9262 - val_acc: 0.6760\n",
      "Epoch 34/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8465 - acc: 0.7115 - val_loss: 0.9076 - val_acc: 0.6730\n",
      "Epoch 35/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8310 - acc: 0.7197 - val_loss: 0.8956 - val_acc: 0.6850\n",
      "Epoch 36/120\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.8165 - acc: 0.7220 - val_loss: 0.8852 - val_acc: 0.6830\n",
      "Epoch 37/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8029 - acc: 0.7260 - val_loss: 0.8714 - val_acc: 0.6830\n",
      "Epoch 38/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.7896 - acc: 0.7277 - val_loss: 0.8612 - val_acc: 0.6840\n",
      "Epoch 39/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.7775 - acc: 0.7349 - val_loss: 0.8512 - val_acc: 0.6880\n",
      "Epoch 40/120\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.7662 - acc: 0.7355 - val_loss: 0.8439 - val_acc: 0.6930\n",
      "Epoch 41/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.7547 - acc: 0.7400 - val_loss: 0.8366 - val_acc: 0.6900\n",
      "Epoch 42/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.7443 - acc: 0.7415 - val_loss: 0.8240 - val_acc: 0.6970\n",
      "Epoch 43/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.7342 - acc: 0.7442 - val_loss: 0.8182 - val_acc: 0.6920\n",
      "Epoch 44/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.7252 - acc: 0.7466 - val_loss: 0.8095 - val_acc: 0.6960\n",
      "Epoch 45/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.7155 - acc: 0.7512 - val_loss: 0.7993 - val_acc: 0.7020\n",
      "Epoch 46/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.7064 - acc: 0.7535 - val_loss: 0.7939 - val_acc: 0.7040\n",
      "Epoch 47/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6981 - acc: 0.7542 - val_loss: 0.7898 - val_acc: 0.7020\n",
      "Epoch 48/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.6897 - acc: 0.7569 - val_loss: 0.7842 - val_acc: 0.7020\n",
      "Epoch 49/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.6821 - acc: 0.7615 - val_loss: 0.7755 - val_acc: 0.7080\n",
      "Epoch 50/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6743 - acc: 0.7631 - val_loss: 0.7717 - val_acc: 0.7090\n",
      "Epoch 51/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.6665 - acc: 0.7666 - val_loss: 0.7661 - val_acc: 0.7070\n",
      "Epoch 52/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6600 - acc: 0.7683 - val_loss: 0.7597 - val_acc: 0.7190\n",
      "Epoch 53/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6528 - acc: 0.7712 - val_loss: 0.7585 - val_acc: 0.7100\n",
      "Epoch 54/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.6463 - acc: 0.7745 - val_loss: 0.7517 - val_acc: 0.7110\n",
      "Epoch 55/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6397 - acc: 0.7755 - val_loss: 0.7455 - val_acc: 0.7150\n",
      "Epoch 56/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.6332 - acc: 0.7774 - val_loss: 0.7425 - val_acc: 0.7120\n",
      "Epoch 57/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6270 - acc: 0.7802 - val_loss: 0.7391 - val_acc: 0.7200\n",
      "Epoch 58/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6211 - acc: 0.7843 - val_loss: 0.7362 - val_acc: 0.7210\n",
      "Epoch 59/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.6151 - acc: 0.7828 - val_loss: 0.7308 - val_acc: 0.7200\n",
      "Epoch 60/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6098 - acc: 0.7882 - val_loss: 0.7286 - val_acc: 0.7180\n",
      "Epoch 61/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.6040 - acc: 0.7895 - val_loss: 0.7249 - val_acc: 0.7250\n",
      "Epoch 62/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.5987 - acc: 0.7895 - val_loss: 0.7205 - val_acc: 0.7250\n",
      "Epoch 63/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.5932 - acc: 0.7912 - val_loss: 0.7178 - val_acc: 0.7270\n",
      "Epoch 64/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.5880 - acc: 0.7982 - val_loss: 0.7157 - val_acc: 0.7210\n",
      "Epoch 65/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.5832 - acc: 0.7968 - val_loss: 0.7127 - val_acc: 0.7300\n",
      "Epoch 66/120\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.5779 - acc: 0.8000 - val_loss: 0.7100 - val_acc: 0.7280\n",
      "Epoch 67/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.5728 - acc: 0.8051 - val_loss: 0.7095 - val_acc: 0.7260\n",
      "Epoch 68/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.5680 - acc: 0.8022 - val_loss: 0.7050 - val_acc: 0.7310\n",
      "Epoch 69/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.5634 - acc: 0.8058 - val_loss: 0.7079 - val_acc: 0.7310\n",
      "Epoch 70/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.5587 - acc: 0.8077 - val_loss: 0.7029 - val_acc: 0.7340\n",
      "Epoch 71/120\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.5542 - acc: 0.8094 - val_loss: 0.6987 - val_acc: 0.7350\n",
      "Epoch 72/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.5497 - acc: 0.8105 - val_loss: 0.6981 - val_acc: 0.7380\n",
      "Epoch 73/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.5453 - acc: 0.8111 - val_loss: 0.6953 - val_acc: 0.7360\n",
      "Epoch 74/120\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.5410 - acc: 0.8146 - val_loss: 0.6944 - val_acc: 0.7350\n",
      "Epoch 75/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.5365 - acc: 0.8128 - val_loss: 0.6943 - val_acc: 0.7380\n",
      "Epoch 76/120\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.5328 - acc: 0.8146 - val_loss: 0.6890 - val_acc: 0.7410\n",
      "Epoch 77/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.5282 - acc: 0.8169 - val_loss: 0.6874 - val_acc: 0.7460\n",
      "Epoch 78/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.5242 - acc: 0.8197 - val_loss: 0.6875 - val_acc: 0.7480\n",
      "Epoch 79/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.5203 - acc: 0.8202 - val_loss: 0.6863 - val_acc: 0.7460\n",
      "Epoch 80/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.5163 - acc: 0.8228 - val_loss: 0.6862 - val_acc: 0.7380\n",
      "Epoch 81/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.5122 - acc: 0.8237 - val_loss: 0.6806 - val_acc: 0.7420\n",
      "Epoch 82/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.5080 - acc: 0.8262 - val_loss: 0.6842 - val_acc: 0.7460\n",
      "Epoch 83/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.5044 - acc: 0.8295 - val_loss: 0.6774 - val_acc: 0.7510\n",
      "Epoch 84/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.5006 - acc: 0.8294 - val_loss: 0.6776 - val_acc: 0.7470\n",
      "Epoch 85/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.4976 - acc: 0.8294 - val_loss: 0.6772 - val_acc: 0.7490\n",
      "Epoch 86/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.4933 - acc: 0.8343 - val_loss: 0.6754 - val_acc: 0.7490\n",
      "Epoch 87/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.4898 - acc: 0.8343 - val_loss: 0.6725 - val_acc: 0.7560\n",
      "Epoch 88/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.4866 - acc: 0.8354 - val_loss: 0.6729 - val_acc: 0.7550\n",
      "Epoch 89/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.4825 - acc: 0.8394 - val_loss: 0.6731 - val_acc: 0.7530\n",
      "Epoch 90/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.4786 - acc: 0.8385 - val_loss: 0.6733 - val_acc: 0.7520\n",
      "Epoch 91/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.4759 - acc: 0.8386 - val_loss: 0.6698 - val_acc: 0.7560\n",
      "Epoch 92/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.4719 - acc: 0.8403 - val_loss: 0.6702 - val_acc: 0.7570\n",
      "Epoch 93/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.4685 - acc: 0.8429 - val_loss: 0.6694 - val_acc: 0.7590\n",
      "Epoch 94/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.4652 - acc: 0.8442 - val_loss: 0.6690 - val_acc: 0.7600\n",
      "Epoch 95/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.4620 - acc: 0.8443 - val_loss: 0.6659 - val_acc: 0.7640\n",
      "Epoch 96/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.4584 - acc: 0.8474 - val_loss: 0.6658 - val_acc: 0.7620\n",
      "Epoch 97/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.4552 - acc: 0.8477 - val_loss: 0.6639 - val_acc: 0.7640\n",
      "Epoch 98/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.4516 - acc: 0.8480 - val_loss: 0.6648 - val_acc: 0.7690\n",
      "Epoch 99/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.4488 - acc: 0.8494 - val_loss: 0.6656 - val_acc: 0.7640\n",
      "Epoch 100/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.4454 - acc: 0.8515 - val_loss: 0.6675 - val_acc: 0.7640\n",
      "Epoch 101/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.4424 - acc: 0.8522 - val_loss: 0.6658 - val_acc: 0.7610\n",
      "Epoch 102/120\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.4394 - acc: 0.8508 - val_loss: 0.6616 - val_acc: 0.7640\n",
      "Epoch 103/120\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 0.4363 - acc: 0.8546 - val_loss: 0.6625 - val_acc: 0.7640\n",
      "Epoch 104/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.4328 - acc: 0.8571 - val_loss: 0.6647 - val_acc: 0.7670\n",
      "Epoch 105/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.4302 - acc: 0.8580 - val_loss: 0.6611 - val_acc: 0.7620\n",
      "Epoch 106/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.4272 - acc: 0.8591 - val_loss: 0.6606 - val_acc: 0.7700\n",
      "Epoch 107/120\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.4235 - acc: 0.8611 - val_loss: 0.6592 - val_acc: 0.7630\n",
      "Epoch 108/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.4210 - acc: 0.8611 - val_loss: 0.6572 - val_acc: 0.7670\n",
      "Epoch 109/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.4183 - acc: 0.8620 - val_loss: 0.6606 - val_acc: 0.7740\n",
      "Epoch 110/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.4150 - acc: 0.8612 - val_loss: 0.6610 - val_acc: 0.7700\n",
      "Epoch 111/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.4120 - acc: 0.8637 - val_loss: 0.6573 - val_acc: 0.7650\n",
      "Epoch 112/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.4094 - acc: 0.8654 - val_loss: 0.6565 - val_acc: 0.7730\n",
      "Epoch 113/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.4065 - acc: 0.8678 - val_loss: 0.6583 - val_acc: 0.7660\n",
      "Epoch 114/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.4035 - acc: 0.8698 - val_loss: 0.6627 - val_acc: 0.7670\n",
      "Epoch 115/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.4012 - acc: 0.8683 - val_loss: 0.6574 - val_acc: 0.7700\n",
      "Epoch 116/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.3980 - acc: 0.8712 - val_loss: 0.6570 - val_acc: 0.7710\n",
      "Epoch 117/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.3951 - acc: 0.8711 - val_loss: 0.6573 - val_acc: 0.7700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.3925 - acc: 0.8732 - val_loss: 0.6569 - val_acc: 0.7740\n",
      "Epoch 119/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.3898 - acc: 0.8715 - val_loss: 0.6570 - val_acc: 0.7750\n",
      "Epoch 120/120\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.3868 - acc: 0.8740 - val_loss: 0.6541 - val_acc: 0.7720\n"
     ]
    }
   ],
   "source": [
    "#Code provided; note the extra validation parameter passed.\n",
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Performance Results: the `history` dictionary\n",
    "\n",
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 35us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 0s 32us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3844599550137153, 0.8756923076923077]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6566007752895355, 0.7636]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result isn't exactly the same as before. Note that this because the training set is slightly different! We remove 1000 instances for validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result similarly to what we have done in the previous lab. This time though, let's include the training and the validation loss in the same plot. We'll do the same thing for the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'blue', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'blue', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a status quo around the 60th epoch. This means that we're actually **overfitting** to the train data when we do as many epochs as we were doing. Luckily, you learned how to tackle overfitting in the previous lecture! For starters, it does seem clear that we are training too long. So let's stop training at the 60th epoch first (so-called \"early stopping\") before we move to more advanced regularization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6500 samples, validate on 1000 samples\n",
      "Epoch 1/60\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 1.9496 - acc: 0.1611 - val_loss: 1.9440 - val_acc: 0.1880\n",
      "Epoch 2/60\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.9345 - acc: 0.1909 - val_loss: 1.9316 - val_acc: 0.2110\n",
      "Epoch 3/60\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.9220 - acc: 0.2158 - val_loss: 1.9197 - val_acc: 0.2300\n",
      "Epoch 4/60\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.9101 - acc: 0.2312 - val_loss: 1.9079 - val_acc: 0.2350\n",
      "Epoch 5/60\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.8978 - acc: 0.2457 - val_loss: 1.8956 - val_acc: 0.2510\n",
      "Epoch 6/60\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.8844 - acc: 0.2622 - val_loss: 1.8824 - val_acc: 0.2610\n",
      "Epoch 7/60\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.8691 - acc: 0.2771 - val_loss: 1.8670 - val_acc: 0.2750\n",
      "Epoch 8/60\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.8515 - acc: 0.2911 - val_loss: 1.8488 - val_acc: 0.2920\n",
      "Epoch 9/60\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.8309 - acc: 0.3072 - val_loss: 1.8279 - val_acc: 0.3080\n",
      "Epoch 10/60\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.8072 - acc: 0.3252 - val_loss: 1.8038 - val_acc: 0.3220\n",
      "Epoch 11/60\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.7813 - acc: 0.3408 - val_loss: 1.7771 - val_acc: 0.3360\n",
      "Epoch 12/60\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.7529 - acc: 0.3578 - val_loss: 1.7470 - val_acc: 0.3480\n",
      "Epoch 13/60\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.7214 - acc: 0.3746 - val_loss: 1.7156 - val_acc: 0.3650\n",
      "Epoch 14/60\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.6878 - acc: 0.3985 - val_loss: 1.6820 - val_acc: 0.3750\n",
      "Epoch 15/60\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.6528 - acc: 0.4138 - val_loss: 1.6472 - val_acc: 0.4060\n",
      "Epoch 16/60\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.6163 - acc: 0.4397 - val_loss: 1.6110 - val_acc: 0.4230\n",
      "Epoch 17/60\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.5787 - acc: 0.4598 - val_loss: 1.5739 - val_acc: 0.4420\n",
      "Epoch 18/60\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.5402 - acc: 0.4806 - val_loss: 1.5369 - val_acc: 0.4700\n",
      "Epoch 19/60\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.5012 - acc: 0.5026 - val_loss: 1.4983 - val_acc: 0.4980\n",
      "Epoch 20/60\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.4609 - acc: 0.5295 - val_loss: 1.4597 - val_acc: 0.5120\n",
      "Epoch 21/60\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.4200 - acc: 0.5525 - val_loss: 1.4192 - val_acc: 0.5310\n",
      "Epoch 22/60\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.3782 - acc: 0.5729 - val_loss: 1.3802 - val_acc: 0.5540\n",
      "Epoch 23/60\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.3367 - acc: 0.5952 - val_loss: 1.3398 - val_acc: 0.5690\n",
      "Epoch 24/60\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.2962 - acc: 0.6131 - val_loss: 1.3018 - val_acc: 0.5750\n",
      "Epoch 25/60\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 1.2564 - acc: 0.6309 - val_loss: 1.2648 - val_acc: 0.5910\n",
      "Epoch 26/60\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.2180 - acc: 0.6443 - val_loss: 1.2282 - val_acc: 0.6060\n",
      "Epoch 27/60\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.1806 - acc: 0.6585 - val_loss: 1.1925 - val_acc: 0.6200\n",
      "Epoch 28/60\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.1447 - acc: 0.6706 - val_loss: 1.1612 - val_acc: 0.6320\n",
      "Epoch 29/60\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.1105 - acc: 0.6814 - val_loss: 1.1307 - val_acc: 0.6450\n",
      "Epoch 30/60\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.0781 - acc: 0.6886 - val_loss: 1.0990 - val_acc: 0.6520\n",
      "Epoch 31/60\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.0473 - acc: 0.6966 - val_loss: 1.0696 - val_acc: 0.6480\n",
      "Epoch 32/60\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.0181 - acc: 0.7018 - val_loss: 1.0447 - val_acc: 0.6620\n",
      "Epoch 33/60\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.9905 - acc: 0.7102 - val_loss: 1.0188 - val_acc: 0.6670\n",
      "Epoch 34/60\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9646 - acc: 0.7145 - val_loss: 0.9962 - val_acc: 0.6730\n",
      "Epoch 35/60\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9404 - acc: 0.7203 - val_loss: 0.9747 - val_acc: 0.6710\n",
      "Epoch 36/60\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9174 - acc: 0.7245 - val_loss: 0.9553 - val_acc: 0.6830\n",
      "Epoch 37/60\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8959 - acc: 0.7300 - val_loss: 0.9358 - val_acc: 0.6850\n",
      "Epoch 38/60\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8757 - acc: 0.7331 - val_loss: 0.9208 - val_acc: 0.6910\n",
      "Epoch 39/60\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8565 - acc: 0.7392 - val_loss: 0.9019 - val_acc: 0.6890\n",
      "Epoch 40/60\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8390 - acc: 0.7429 - val_loss: 0.8857 - val_acc: 0.6940\n",
      "Epoch 41/60\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8223 - acc: 0.7477 - val_loss: 0.8741 - val_acc: 0.6890\n",
      "Epoch 42/60\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8065 - acc: 0.7505 - val_loss: 0.8589 - val_acc: 0.6930\n",
      "Epoch 43/60\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.7914 - acc: 0.7526 - val_loss: 0.8471 - val_acc: 0.7100\n",
      "Epoch 44/60\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.7772 - acc: 0.7566 - val_loss: 0.8366 - val_acc: 0.7110\n",
      "Epoch 45/60\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.7644 - acc: 0.7595 - val_loss: 0.8260 - val_acc: 0.7110\n",
      "Epoch 46/60\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.7513 - acc: 0.7632 - val_loss: 0.8159 - val_acc: 0.7130\n",
      "Epoch 47/60\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.7396 - acc: 0.7651 - val_loss: 0.8062 - val_acc: 0.7130\n",
      "Epoch 48/60\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.7277 - acc: 0.7691 - val_loss: 0.7972 - val_acc: 0.7150\n",
      "Epoch 49/60\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.7175 - acc: 0.7683 - val_loss: 0.7898 - val_acc: 0.7150\n",
      "Epoch 50/60\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.7068 - acc: 0.7720 - val_loss: 0.7818 - val_acc: 0.7160\n",
      "Epoch 51/60\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.6972 - acc: 0.7725 - val_loss: 0.7746 - val_acc: 0.7170\n",
      "Epoch 52/60\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6875 - acc: 0.7777 - val_loss: 0.7712 - val_acc: 0.7160\n",
      "Epoch 53/60\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.6786 - acc: 0.7811 - val_loss: 0.7609 - val_acc: 0.7180\n",
      "Epoch 54/60\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.6702 - acc: 0.7826 - val_loss: 0.7579 - val_acc: 0.7200\n",
      "Epoch 55/60\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.6617 - acc: 0.7846 - val_loss: 0.7502 - val_acc: 0.7220\n",
      "Epoch 56/60\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6541 - acc: 0.7872 - val_loss: 0.7444 - val_acc: 0.7200\n",
      "Epoch 57/60\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6461 - acc: 0.7903 - val_loss: 0.7398 - val_acc: 0.7230\n",
      "Epoch 58/60\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.6385 - acc: 0.7917 - val_loss: 0.7342 - val_acc: 0.7260\n",
      "Epoch 59/60\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.6319 - acc: 0.7929 - val_loss: 0.7309 - val_acc: 0.7250\n",
      "Epoch 60/60\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.6244 - acc: 0.7955 - val_loss: 0.7269 - val_acc: 0.7230\n"
     ]
    }
   ],
   "source": [
    "np.random.seed=123\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the test set to make label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 26us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 0s 22us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6187797145843505, 0.7972307692307692]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7231449565887451, 0.7488]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! Our test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs one we fitted before.\n",
    "\n",
    "Now, let's see what else we can do to improve the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include L2 regularization. You can easily do this in keras adding the argument kernel_regulizers.l2 and adding a value for the regularization parameter lambda between parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "6500/6500 [==============================] - 0s 66us/step - loss: 2.6067 - acc: 0.1515 - val_loss: 2.6024 - val_acc: 0.1490\n",
      "Epoch 2/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 2.5842 - acc: 0.1720 - val_loss: 2.5828 - val_acc: 0.1660\n",
      "Epoch 3/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 2.5652 - acc: 0.1922 - val_loss: 2.5644 - val_acc: 0.1860\n",
      "Epoch 4/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 2.5465 - acc: 0.2197 - val_loss: 2.5463 - val_acc: 0.2060\n",
      "Epoch 5/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 2.5270 - acc: 0.2452 - val_loss: 2.5269 - val_acc: 0.2210\n",
      "Epoch 6/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 2.5060 - acc: 0.2703 - val_loss: 2.5061 - val_acc: 0.2500\n",
      "Epoch 7/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 2.4834 - acc: 0.2915 - val_loss: 2.4844 - val_acc: 0.2810\n",
      "Epoch 8/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 2.4598 - acc: 0.3106 - val_loss: 2.4615 - val_acc: 0.3010\n",
      "Epoch 9/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 2.4346 - acc: 0.3331 - val_loss: 2.4369 - val_acc: 0.3250\n",
      "Epoch 10/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 2.4080 - acc: 0.3506 - val_loss: 2.4100 - val_acc: 0.3520\n",
      "Epoch 11/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 2.3791 - acc: 0.3712 - val_loss: 2.3818 - val_acc: 0.3720\n",
      "Epoch 12/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 2.3482 - acc: 0.3935 - val_loss: 2.3519 - val_acc: 0.3890\n",
      "Epoch 13/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 2.3151 - acc: 0.4158 - val_loss: 2.3195 - val_acc: 0.4100\n",
      "Epoch 14/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 2.2801 - acc: 0.4366 - val_loss: 2.2852 - val_acc: 0.4250\n",
      "Epoch 15/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 2.2436 - acc: 0.4586 - val_loss: 2.2489 - val_acc: 0.4410\n",
      "Epoch 16/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 2.2048 - acc: 0.4755 - val_loss: 2.2112 - val_acc: 0.4670\n",
      "Epoch 17/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 2.1649 - acc: 0.5022 - val_loss: 2.1711 - val_acc: 0.4890\n",
      "Epoch 18/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 2.1234 - acc: 0.5206 - val_loss: 2.1301 - val_acc: 0.5050\n",
      "Epoch 19/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 2.0806 - acc: 0.5389 - val_loss: 2.0882 - val_acc: 0.5320\n",
      "Epoch 20/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 2.0379 - acc: 0.5637 - val_loss: 2.0466 - val_acc: 0.5400\n",
      "Epoch 21/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.9946 - acc: 0.5775 - val_loss: 2.0037 - val_acc: 0.5510\n",
      "Epoch 22/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.9518 - acc: 0.5960 - val_loss: 1.9644 - val_acc: 0.5680\n",
      "Epoch 23/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.9105 - acc: 0.6094 - val_loss: 1.9234 - val_acc: 0.5750\n",
      "Epoch 24/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.8700 - acc: 0.6163 - val_loss: 1.8845 - val_acc: 0.5990\n",
      "Epoch 25/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.8308 - acc: 0.6328 - val_loss: 1.8476 - val_acc: 0.6080\n",
      "Epoch 26/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 1.7934 - acc: 0.6452 - val_loss: 1.8117 - val_acc: 0.6220\n",
      "Epoch 27/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.7576 - acc: 0.6555 - val_loss: 1.7799 - val_acc: 0.6270\n",
      "Epoch 28/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.7235 - acc: 0.6677 - val_loss: 1.7467 - val_acc: 0.6300\n",
      "Epoch 29/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.6911 - acc: 0.6734 - val_loss: 1.7176 - val_acc: 0.6440\n",
      "Epoch 30/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.6600 - acc: 0.6798 - val_loss: 1.6886 - val_acc: 0.6500\n",
      "Epoch 31/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.6307 - acc: 0.6917 - val_loss: 1.6614 - val_acc: 0.6590\n",
      "Epoch 32/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.6034 - acc: 0.7002 - val_loss: 1.6371 - val_acc: 0.6620\n",
      "Epoch 33/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.5773 - acc: 0.7052 - val_loss: 1.6141 - val_acc: 0.6630\n",
      "Epoch 34/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.5528 - acc: 0.7100 - val_loss: 1.5932 - val_acc: 0.6690\n",
      "Epoch 35/120\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 1.5295 - acc: 0.7135 - val_loss: 1.5715 - val_acc: 0.6780\n",
      "Epoch 36/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.5083 - acc: 0.7198 - val_loss: 1.5529 - val_acc: 0.6760\n",
      "Epoch 37/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.4875 - acc: 0.7212 - val_loss: 1.5359 - val_acc: 0.6770\n",
      "Epoch 38/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.4685 - acc: 0.7255 - val_loss: 1.5177 - val_acc: 0.6830\n",
      "Epoch 39/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.4504 - acc: 0.7291 - val_loss: 1.5012 - val_acc: 0.6920\n",
      "Epoch 40/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.4329 - acc: 0.7332 - val_loss: 1.4857 - val_acc: 0.6980\n",
      "Epoch 41/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.4167 - acc: 0.7377 - val_loss: 1.4732 - val_acc: 0.6960\n",
      "Epoch 42/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.4008 - acc: 0.7425 - val_loss: 1.4590 - val_acc: 0.7020\n",
      "Epoch 43/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.3858 - acc: 0.7426 - val_loss: 1.4475 - val_acc: 0.7000\n",
      "Epoch 44/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.3719 - acc: 0.7462 - val_loss: 1.4334 - val_acc: 0.7050\n",
      "Epoch 45/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.3581 - acc: 0.7529 - val_loss: 1.4237 - val_acc: 0.7140\n",
      "Epoch 46/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.3455 - acc: 0.7535 - val_loss: 1.4116 - val_acc: 0.7120\n",
      "Epoch 47/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.3329 - acc: 0.7543 - val_loss: 1.3999 - val_acc: 0.7120\n",
      "Epoch 48/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.3212 - acc: 0.7606 - val_loss: 1.3914 - val_acc: 0.7190\n",
      "Epoch 49/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.3094 - acc: 0.7589 - val_loss: 1.3834 - val_acc: 0.7170\n",
      "Epoch 50/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.2988 - acc: 0.7617 - val_loss: 1.3715 - val_acc: 0.7220\n",
      "Epoch 51/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.2880 - acc: 0.7669 - val_loss: 1.3639 - val_acc: 0.7280\n",
      "Epoch 52/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.2778 - acc: 0.7702 - val_loss: 1.3550 - val_acc: 0.7210\n",
      "Epoch 53/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.2678 - acc: 0.7737 - val_loss: 1.3478 - val_acc: 0.7260\n",
      "Epoch 54/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.2581 - acc: 0.7742 - val_loss: 1.3391 - val_acc: 0.7260\n",
      "Epoch 55/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.2489 - acc: 0.7791 - val_loss: 1.3339 - val_acc: 0.7210\n",
      "Epoch 56/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.2401 - acc: 0.7783 - val_loss: 1.3263 - val_acc: 0.7270\n",
      "Epoch 57/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.2310 - acc: 0.7789 - val_loss: 1.3184 - val_acc: 0.7270\n",
      "Epoch 58/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.2225 - acc: 0.7815 - val_loss: 1.3118 - val_acc: 0.7270\n",
      "Epoch 59/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.2143 - acc: 0.7857 - val_loss: 1.3055 - val_acc: 0.7290\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.2062 - acc: 0.7866 - val_loss: 1.3004 - val_acc: 0.7270\n",
      "Epoch 61/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.1982 - acc: 0.7872 - val_loss: 1.2928 - val_acc: 0.7340\n",
      "Epoch 62/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.1905 - acc: 0.7912 - val_loss: 1.2876 - val_acc: 0.7320\n",
      "Epoch 63/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.1833 - acc: 0.7928 - val_loss: 1.2835 - val_acc: 0.7310\n",
      "Epoch 64/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.1758 - acc: 0.7943 - val_loss: 1.2768 - val_acc: 0.7330\n",
      "Epoch 65/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.1687 - acc: 0.7942 - val_loss: 1.2718 - val_acc: 0.7290\n",
      "Epoch 66/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.1617 - acc: 0.7982 - val_loss: 1.2663 - val_acc: 0.7320\n",
      "Epoch 67/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.1549 - acc: 0.7983 - val_loss: 1.2616 - val_acc: 0.7340\n",
      "Epoch 68/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.1480 - acc: 0.8008 - val_loss: 1.2587 - val_acc: 0.7300\n",
      "Epoch 69/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.1414 - acc: 0.8000 - val_loss: 1.2510 - val_acc: 0.7390\n",
      "Epoch 70/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.1351 - acc: 0.8037 - val_loss: 1.2500 - val_acc: 0.7390\n",
      "Epoch 71/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.1287 - acc: 0.8068 - val_loss: 1.2441 - val_acc: 0.7340\n",
      "Epoch 72/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.1224 - acc: 0.8080 - val_loss: 1.2388 - val_acc: 0.7310\n",
      "Epoch 73/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.1163 - acc: 0.8071 - val_loss: 1.2359 - val_acc: 0.7400\n",
      "Epoch 74/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.1105 - acc: 0.8102 - val_loss: 1.2332 - val_acc: 0.7380\n",
      "Epoch 75/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.1044 - acc: 0.8111 - val_loss: 1.2283 - val_acc: 0.7360\n",
      "Epoch 76/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.0984 - acc: 0.8126 - val_loss: 1.2231 - val_acc: 0.7400\n",
      "Epoch 77/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.0927 - acc: 0.8125 - val_loss: 1.2200 - val_acc: 0.7380\n",
      "Epoch 78/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.0869 - acc: 0.8151 - val_loss: 1.2135 - val_acc: 0.7380\n",
      "Epoch 79/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.0816 - acc: 0.8189 - val_loss: 1.2137 - val_acc: 0.7350\n",
      "Epoch 80/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.0765 - acc: 0.8163 - val_loss: 1.2083 - val_acc: 0.7340\n",
      "Epoch 81/120\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.0707 - acc: 0.8205 - val_loss: 1.2072 - val_acc: 0.7400\n",
      "Epoch 82/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.0654 - acc: 0.8214 - val_loss: 1.2003 - val_acc: 0.7460\n",
      "Epoch 83/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.0598 - acc: 0.8235 - val_loss: 1.2003 - val_acc: 0.7440\n",
      "Epoch 84/120\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.0550 - acc: 0.8212 - val_loss: 1.1952 - val_acc: 0.7390\n",
      "Epoch 85/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.0499 - acc: 0.8249 - val_loss: 1.1921 - val_acc: 0.7450\n",
      "Epoch 86/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.0445 - acc: 0.8251 - val_loss: 1.1888 - val_acc: 0.7470\n",
      "Epoch 87/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.0395 - acc: 0.8298 - val_loss: 1.1882 - val_acc: 0.7370\n",
      "Epoch 88/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.0351 - acc: 0.8315 - val_loss: 1.1863 - val_acc: 0.7460\n",
      "Epoch 89/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.0304 - acc: 0.8303 - val_loss: 1.1785 - val_acc: 0.7450\n",
      "Epoch 90/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 1.0250 - acc: 0.8334 - val_loss: 1.1760 - val_acc: 0.7490\n",
      "Epoch 91/120\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.0207 - acc: 0.8340 - val_loss: 1.1726 - val_acc: 0.7480\n",
      "Epoch 92/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 1.0156 - acc: 0.8345 - val_loss: 1.1713 - val_acc: 0.7480\n",
      "Epoch 93/120\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.0110 - acc: 0.8389 - val_loss: 1.1709 - val_acc: 0.7400\n",
      "Epoch 94/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 1.0064 - acc: 0.8363 - val_loss: 1.1655 - val_acc: 0.7380\n",
      "Epoch 95/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.0020 - acc: 0.8397 - val_loss: 1.1645 - val_acc: 0.7450\n",
      "Epoch 96/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.9971 - acc: 0.8420 - val_loss: 1.1622 - val_acc: 0.7480\n",
      "Epoch 97/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.9929 - acc: 0.8417 - val_loss: 1.1571 - val_acc: 0.7450\n",
      "Epoch 98/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.9886 - acc: 0.8442 - val_loss: 1.1574 - val_acc: 0.7410\n",
      "Epoch 99/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.9840 - acc: 0.8435 - val_loss: 1.1546 - val_acc: 0.7420\n",
      "Epoch 100/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.9795 - acc: 0.8448 - val_loss: 1.1511 - val_acc: 0.7460\n",
      "Epoch 101/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9750 - acc: 0.8486 - val_loss: 1.1511 - val_acc: 0.7420\n",
      "Epoch 102/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9711 - acc: 0.8458 - val_loss: 1.1480 - val_acc: 0.7600\n",
      "Epoch 103/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9672 - acc: 0.8500 - val_loss: 1.1431 - val_acc: 0.7450\n",
      "Epoch 104/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.9626 - acc: 0.8471 - val_loss: 1.1440 - val_acc: 0.7450\n",
      "Epoch 105/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9584 - acc: 0.8529 - val_loss: 1.1385 - val_acc: 0.7480\n",
      "Epoch 106/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.9548 - acc: 0.8534 - val_loss: 1.1385 - val_acc: 0.7530\n",
      "Epoch 107/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9505 - acc: 0.8545 - val_loss: 1.1355 - val_acc: 0.7520\n",
      "Epoch 108/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.9464 - acc: 0.8562 - val_loss: 1.1312 - val_acc: 0.7570\n",
      "Epoch 109/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9421 - acc: 0.8577 - val_loss: 1.1327 - val_acc: 0.7540\n",
      "Epoch 110/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.9381 - acc: 0.8583 - val_loss: 1.1293 - val_acc: 0.7540\n",
      "Epoch 111/120\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9345 - acc: 0.8605 - val_loss: 1.1308 - val_acc: 0.7460\n",
      "Epoch 112/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9306 - acc: 0.8611 - val_loss: 1.1240 - val_acc: 0.7630\n",
      "Epoch 113/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.9267 - acc: 0.8625 - val_loss: 1.1226 - val_acc: 0.7590\n",
      "Epoch 114/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9228 - acc: 0.8648 - val_loss: 1.1248 - val_acc: 0.7480\n",
      "Epoch 115/120\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9194 - acc: 0.8620 - val_loss: 1.1200 - val_acc: 0.7500\n",
      "Epoch 116/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.9153 - acc: 0.8632 - val_loss: 1.1173 - val_acc: 0.7550\n",
      "Epoch 117/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9118 - acc: 0.8663 - val_loss: 1.1143 - val_acc: 0.7620\n",
      "Epoch 118/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9080 - acc: 0.8652 - val_loss: 1.1133 - val_acc: 0.7640\n",
      "Epoch 119/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.9042 - acc: 0.8680 - val_loss: 1.1126 - val_acc: 0.7510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.9004 - acc: 0.8677 - val_loss: 1.1106 - val_acc: 0.7580\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "import random\n",
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. We notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at L1 regularization. Will this work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "6500/6500 [==============================] - 0s 70us/step - loss: 16.0407 - acc: 0.1768 - val_loss: 15.6857 - val_acc: 0.1800\n",
      "Epoch 2/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 15.3804 - acc: 0.1888 - val_loss: 15.0398 - val_acc: 0.1790\n",
      "Epoch 3/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 14.7427 - acc: 0.1985 - val_loss: 14.4125 - val_acc: 0.1960\n",
      "Epoch 4/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 14.1223 - acc: 0.2085 - val_loss: 13.8012 - val_acc: 0.2070\n",
      "Epoch 5/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 13.5178 - acc: 0.2208 - val_loss: 13.2054 - val_acc: 0.2220\n",
      "Epoch 6/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 12.9285 - acc: 0.2305 - val_loss: 12.6247 - val_acc: 0.2280\n",
      "Epoch 7/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 12.3542 - acc: 0.2420 - val_loss: 12.0586 - val_acc: 0.2410\n",
      "Epoch 8/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 11.7942 - acc: 0.2475 - val_loss: 11.5065 - val_acc: 0.2470\n",
      "Epoch 9/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 11.2491 - acc: 0.2563 - val_loss: 10.9697 - val_acc: 0.2580\n",
      "Epoch 10/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 10.7192 - acc: 0.2666 - val_loss: 10.4477 - val_acc: 0.2700\n",
      "Epoch 11/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 10.2045 - acc: 0.2768 - val_loss: 9.9411 - val_acc: 0.2810\n",
      "Epoch 12/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 9.7048 - acc: 0.2908 - val_loss: 9.4494 - val_acc: 0.2970\n",
      "Epoch 13/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 9.2202 - acc: 0.3074 - val_loss: 8.9723 - val_acc: 0.3110\n",
      "Epoch 14/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 8.7508 - acc: 0.3262 - val_loss: 8.5105 - val_acc: 0.3410\n",
      "Epoch 15/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 8.2967 - acc: 0.3495 - val_loss: 8.0643 - val_acc: 0.3580\n",
      "Epoch 16/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 7.8582 - acc: 0.3718 - val_loss: 7.6342 - val_acc: 0.3860\n",
      "Epoch 17/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 7.4354 - acc: 0.4020 - val_loss: 7.2195 - val_acc: 0.3990\n",
      "Epoch 18/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 7.0285 - acc: 0.4160 - val_loss: 6.8216 - val_acc: 0.4360\n",
      "Epoch 19/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 6.6373 - acc: 0.4426 - val_loss: 6.4397 - val_acc: 0.4260\n",
      "Epoch 20/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 6.2620 - acc: 0.4565 - val_loss: 6.0726 - val_acc: 0.4670\n",
      "Epoch 21/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 5.9030 - acc: 0.4818 - val_loss: 5.7239 - val_acc: 0.4760\n",
      "Epoch 22/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 5.5617 - acc: 0.4954 - val_loss: 5.3929 - val_acc: 0.5000\n",
      "Epoch 23/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 5.2380 - acc: 0.5160 - val_loss: 5.0798 - val_acc: 0.5040\n",
      "Epoch 24/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 4.9312 - acc: 0.5318 - val_loss: 4.7817 - val_acc: 0.5300\n",
      "Epoch 25/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 4.6411 - acc: 0.5449 - val_loss: 4.5009 - val_acc: 0.5680\n",
      "Epoch 26/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 4.3680 - acc: 0.5672 - val_loss: 4.2362 - val_acc: 0.5670\n",
      "Epoch 27/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 4.1115 - acc: 0.5766 - val_loss: 3.9895 - val_acc: 0.5820\n",
      "Epoch 28/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 3.8717 - acc: 0.5897 - val_loss: 3.7587 - val_acc: 0.5930\n",
      "Epoch 29/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 3.6489 - acc: 0.6025 - val_loss: 3.5464 - val_acc: 0.5890\n",
      "Epoch 30/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 3.4427 - acc: 0.6102 - val_loss: 3.3480 - val_acc: 0.6070\n",
      "Epoch 31/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 3.2522 - acc: 0.6225 - val_loss: 3.1664 - val_acc: 0.6090\n",
      "Epoch 32/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 3.0785 - acc: 0.6228 - val_loss: 3.0014 - val_acc: 0.6180\n",
      "Epoch 33/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 2.9202 - acc: 0.6302 - val_loss: 2.8520 - val_acc: 0.6210\n",
      "Epoch 34/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 2.7776 - acc: 0.6398 - val_loss: 2.7170 - val_acc: 0.6270\n",
      "Epoch 35/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 2.6509 - acc: 0.6405 - val_loss: 2.5982 - val_acc: 0.6290\n",
      "Epoch 36/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 2.5389 - acc: 0.6435 - val_loss: 2.4942 - val_acc: 0.6330\n",
      "Epoch 37/120\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 2.4418 - acc: 0.6512 - val_loss: 2.4052 - val_acc: 0.6310\n",
      "Epoch 38/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 2.3593 - acc: 0.6531 - val_loss: 2.3314 - val_acc: 0.6290\n",
      "Epoch 39/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 2.2908 - acc: 0.6588 - val_loss: 2.2689 - val_acc: 0.6350\n",
      "Epoch 40/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 2.2350 - acc: 0.6594 - val_loss: 2.2191 - val_acc: 0.6310\n",
      "Epoch 41/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 2.1913 - acc: 0.6618 - val_loss: 2.1810 - val_acc: 0.6420\n",
      "Epoch 42/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 2.1569 - acc: 0.6642 - val_loss: 2.1510 - val_acc: 0.6400\n",
      "Epoch 43/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 2.1283 - acc: 0.6662 - val_loss: 2.1234 - val_acc: 0.6440\n",
      "Epoch 44/120\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 2.1043 - acc: 0.6688 - val_loss: 2.1009 - val_acc: 0.6450\n",
      "Epoch 45/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 2.0823 - acc: 0.6718 - val_loss: 2.0813 - val_acc: 0.6430\n",
      "Epoch 46/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 2.0616 - acc: 0.6694 - val_loss: 2.0619 - val_acc: 0.6420\n",
      "Epoch 47/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 2.0424 - acc: 0.6731 - val_loss: 2.0422 - val_acc: 0.6450\n",
      "Epoch 48/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 2.0239 - acc: 0.6726 - val_loss: 2.0245 - val_acc: 0.6480\n",
      "Epoch 49/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 2.0062 - acc: 0.6738 - val_loss: 2.0086 - val_acc: 0.6490\n",
      "Epoch 50/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.9894 - acc: 0.6760 - val_loss: 1.9939 - val_acc: 0.6410\n",
      "Epoch 51/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.9738 - acc: 0.6782 - val_loss: 1.9772 - val_acc: 0.6540\n",
      "Epoch 52/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.9581 - acc: 0.6780 - val_loss: 1.9615 - val_acc: 0.6500\n",
      "Epoch 53/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.9432 - acc: 0.6768 - val_loss: 1.9479 - val_acc: 0.6530\n",
      "Epoch 54/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.9292 - acc: 0.6792 - val_loss: 1.9327 - val_acc: 0.6560\n",
      "Epoch 55/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.9146 - acc: 0.6789 - val_loss: 1.9199 - val_acc: 0.6570\n",
      "Epoch 56/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.9016 - acc: 0.6812 - val_loss: 1.9066 - val_acc: 0.6570\n",
      "Epoch 57/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.8884 - acc: 0.6808 - val_loss: 1.8960 - val_acc: 0.6540\n",
      "Epoch 58/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.8761 - acc: 0.6802 - val_loss: 1.8852 - val_acc: 0.6540\n",
      "Epoch 59/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.8642 - acc: 0.6802 - val_loss: 1.8716 - val_acc: 0.6540\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.8515 - acc: 0.6803 - val_loss: 1.8615 - val_acc: 0.6550\n",
      "Epoch 61/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 1.8399 - acc: 0.6809 - val_loss: 1.8483 - val_acc: 0.6580\n",
      "Epoch 62/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.8285 - acc: 0.6815 - val_loss: 1.8391 - val_acc: 0.6510\n",
      "Epoch 63/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.8173 - acc: 0.6834 - val_loss: 1.8274 - val_acc: 0.6530\n",
      "Epoch 64/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.8064 - acc: 0.6814 - val_loss: 1.8167 - val_acc: 0.6560\n",
      "Epoch 65/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.7967 - acc: 0.6845 - val_loss: 1.8074 - val_acc: 0.6520\n",
      "Epoch 66/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.7863 - acc: 0.6818 - val_loss: 1.7971 - val_acc: 0.6560\n",
      "Epoch 67/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.7757 - acc: 0.6845 - val_loss: 1.7874 - val_acc: 0.6520\n",
      "Epoch 68/120\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.7663 - acc: 0.6817 - val_loss: 1.7774 - val_acc: 0.6530\n",
      "Epoch 69/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.7562 - acc: 0.6848 - val_loss: 1.7724 - val_acc: 0.6610\n",
      "Epoch 70/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.7476 - acc: 0.6857 - val_loss: 1.7607 - val_acc: 0.6530\n",
      "Epoch 71/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.7378 - acc: 0.6860 - val_loss: 1.7507 - val_acc: 0.6610\n",
      "Epoch 72/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.7289 - acc: 0.6843 - val_loss: 1.7418 - val_acc: 0.6570\n",
      "Epoch 73/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.7196 - acc: 0.6877 - val_loss: 1.7350 - val_acc: 0.6610\n",
      "Epoch 74/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.7109 - acc: 0.6857 - val_loss: 1.7272 - val_acc: 0.6580\n",
      "Epoch 75/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.7025 - acc: 0.6860 - val_loss: 1.7165 - val_acc: 0.6610\n",
      "Epoch 76/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.6942 - acc: 0.6871 - val_loss: 1.7058 - val_acc: 0.6630\n",
      "Epoch 77/120\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 1.6853 - acc: 0.6885 - val_loss: 1.7026 - val_acc: 0.6580\n",
      "Epoch 78/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.6774 - acc: 0.6900 - val_loss: 1.6910 - val_acc: 0.6640\n",
      "Epoch 79/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.6699 - acc: 0.6866 - val_loss: 1.6826 - val_acc: 0.6650\n",
      "Epoch 80/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.6613 - acc: 0.6891 - val_loss: 1.6756 - val_acc: 0.6590\n",
      "Epoch 81/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.6535 - acc: 0.6902 - val_loss: 1.6693 - val_acc: 0.6620\n",
      "Epoch 82/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.6461 - acc: 0.6905 - val_loss: 1.6607 - val_acc: 0.6600\n",
      "Epoch 83/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.6387 - acc: 0.6897 - val_loss: 1.6528 - val_acc: 0.6610\n",
      "Epoch 84/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.6306 - acc: 0.6908 - val_loss: 1.6452 - val_acc: 0.6660\n",
      "Epoch 85/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.6236 - acc: 0.6928 - val_loss: 1.6399 - val_acc: 0.6580\n",
      "Epoch 86/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.6166 - acc: 0.6909 - val_loss: 1.6327 - val_acc: 0.6620\n",
      "Epoch 87/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.6087 - acc: 0.6935 - val_loss: 1.6285 - val_acc: 0.6580\n",
      "Epoch 88/120\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.6028 - acc: 0.6934 - val_loss: 1.6174 - val_acc: 0.6620\n",
      "Epoch 89/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.5946 - acc: 0.6942 - val_loss: 1.6103 - val_acc: 0.6660\n",
      "Epoch 90/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 1.5878 - acc: 0.6940 - val_loss: 1.6026 - val_acc: 0.6670\n",
      "Epoch 91/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.5807 - acc: 0.6940 - val_loss: 1.5961 - val_acc: 0.6650\n",
      "Epoch 92/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.5745 - acc: 0.6952 - val_loss: 1.5962 - val_acc: 0.6690\n",
      "Epoch 93/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.5678 - acc: 0.6940 - val_loss: 1.5834 - val_acc: 0.6660\n",
      "Epoch 94/120\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.5613 - acc: 0.6958 - val_loss: 1.5790 - val_acc: 0.6680\n",
      "Epoch 95/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.5538 - acc: 0.6975 - val_loss: 1.5732 - val_acc: 0.6600\n",
      "Epoch 96/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.5478 - acc: 0.6966 - val_loss: 1.5659 - val_acc: 0.6670\n",
      "Epoch 97/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.5420 - acc: 0.6968 - val_loss: 1.5592 - val_acc: 0.6690\n",
      "Epoch 98/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.5350 - acc: 0.6982 - val_loss: 1.5568 - val_acc: 0.6680\n",
      "Epoch 99/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.5296 - acc: 0.6991 - val_loss: 1.5488 - val_acc: 0.6670\n",
      "Epoch 100/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.5231 - acc: 0.6983 - val_loss: 1.5411 - val_acc: 0.6650\n",
      "Epoch 101/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.5169 - acc: 0.6991 - val_loss: 1.5388 - val_acc: 0.6690\n",
      "Epoch 102/120\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.5112 - acc: 0.6991 - val_loss: 1.5300 - val_acc: 0.6660\n",
      "Epoch 103/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 1.5046 - acc: 0.6992 - val_loss: 1.5229 - val_acc: 0.6690\n",
      "Epoch 104/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.4986 - acc: 0.7015 - val_loss: 1.5184 - val_acc: 0.6630\n",
      "Epoch 105/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.4929 - acc: 0.7005 - val_loss: 1.5108 - val_acc: 0.6740\n",
      "Epoch 106/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 1.4869 - acc: 0.7014 - val_loss: 1.5054 - val_acc: 0.6740\n",
      "Epoch 107/120\n",
      "6500/6500 [==============================] - 0s 47us/step - loss: 1.4817 - acc: 0.7023 - val_loss: 1.4993 - val_acc: 0.6750\n",
      "Epoch 108/120\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 1.4754 - acc: 0.7017 - val_loss: 1.4941 - val_acc: 0.6750\n",
      "Epoch 109/120\n",
      "6500/6500 [==============================] - 0s 47us/step - loss: 1.4696 - acc: 0.7045 - val_loss: 1.4931 - val_acc: 0.6680\n",
      "Epoch 110/120\n",
      "6500/6500 [==============================] - 0s 48us/step - loss: 1.4643 - acc: 0.7046 - val_loss: 1.4851 - val_acc: 0.6740\n",
      "Epoch 111/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 1.4588 - acc: 0.7046 - val_loss: 1.4773 - val_acc: 0.6710\n",
      "Epoch 112/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.4532 - acc: 0.7066 - val_loss: 1.4730 - val_acc: 0.6750\n",
      "Epoch 113/120\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 1.4478 - acc: 0.7052 - val_loss: 1.4668 - val_acc: 0.6760\n",
      "Epoch 114/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.4424 - acc: 0.7080 - val_loss: 1.4622 - val_acc: 0.6750\n",
      "Epoch 115/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.4374 - acc: 0.7075 - val_loss: 1.4552 - val_acc: 0.6730\n",
      "Epoch 116/120\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.4319 - acc: 0.7080 - val_loss: 1.4521 - val_acc: 0.6790\n",
      "Epoch 117/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.4265 - acc: 0.7088 - val_loss: 1.4456 - val_acc: 0.6760\n",
      "Epoch 118/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.4212 - acc: 0.7086 - val_loss: 1.4399 - val_acc: 0.6770\n",
      "Epoch 119/120\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.4163 - acc: 0.7102 - val_loss: 1.4370 - val_acc: 0.6770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 1.4113 - acc: 0.7082 - val_loss: 1.4298 - val_acc: 0.6750\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how The training and validation accuracy don't diverge as much as before! Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like we can still improve the model by training much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6500 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      "6500/6500 [==============================] - 1s 97us/step - loss: 16.0690 - acc: 0.1737 - val_loss: 15.7153 - val_acc: 0.1930\n",
      "Epoch 2/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 15.4055 - acc: 0.1994 - val_loss: 15.0644 - val_acc: 0.2160\n",
      "Epoch 3/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 14.7627 - acc: 0.2115 - val_loss: 14.4322 - val_acc: 0.2250\n",
      "Epoch 4/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 14.1379 - acc: 0.2228 - val_loss: 13.8167 - val_acc: 0.2290\n",
      "Epoch 5/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 13.5293 - acc: 0.2329 - val_loss: 13.2171 - val_acc: 0.2390\n",
      "Epoch 6/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 12.9360 - acc: 0.2398 - val_loss: 12.6320 - val_acc: 0.2490\n",
      "Epoch 7/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 12.3572 - acc: 0.2478 - val_loss: 12.0621 - val_acc: 0.2660\n",
      "Epoch 8/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 11.7936 - acc: 0.2660 - val_loss: 11.5075 - val_acc: 0.2760\n",
      "Epoch 9/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 11.2454 - acc: 0.2809 - val_loss: 10.9681 - val_acc: 0.2940\n",
      "Epoch 10/1000\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 10.7124 - acc: 0.3011 - val_loss: 10.4444 - val_acc: 0.3260\n",
      "Epoch 11/1000\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 10.1948 - acc: 0.3297 - val_loss: 9.9353 - val_acc: 0.3420\n",
      "Epoch 12/1000\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 9.6932 - acc: 0.3506 - val_loss: 9.4429 - val_acc: 0.3710\n",
      "Epoch 13/1000\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 9.2075 - acc: 0.3851 - val_loss: 8.9654 - val_acc: 0.3840\n",
      "Epoch 14/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 8.7370 - acc: 0.4037 - val_loss: 8.5035 - val_acc: 0.3970\n",
      "Epoch 15/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 8.2821 - acc: 0.4340 - val_loss: 8.0576 - val_acc: 0.4430\n",
      "Epoch 16/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 7.8428 - acc: 0.4683 - val_loss: 7.6270 - val_acc: 0.4610\n",
      "Epoch 17/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 7.4194 - acc: 0.4960 - val_loss: 7.2129 - val_acc: 0.4780\n",
      "Epoch 18/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 7.0122 - acc: 0.5192 - val_loss: 6.8142 - val_acc: 0.5330\n",
      "Epoch 19/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 6.6216 - acc: 0.5574 - val_loss: 6.4325 - val_acc: 0.5430\n",
      "Epoch 20/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 6.2478 - acc: 0.5800 - val_loss: 6.0677 - val_acc: 0.5690\n",
      "Epoch 21/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 5.8909 - acc: 0.6014 - val_loss: 5.7192 - val_acc: 0.5860\n",
      "Epoch 22/1000\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 5.5501 - acc: 0.6209 - val_loss: 5.3873 - val_acc: 0.5890\n",
      "Epoch 23/1000\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 5.2256 - acc: 0.6283 - val_loss: 5.0729 - val_acc: 0.6080\n",
      "Epoch 24/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 4.9186 - acc: 0.6457 - val_loss: 4.7746 - val_acc: 0.6080\n",
      "Epoch 25/1000\n",
      "6500/6500 [==============================] - 0s 48us/step - loss: 4.6282 - acc: 0.6549 - val_loss: 4.4938 - val_acc: 0.6250\n",
      "Epoch 26/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 4.3553 - acc: 0.6629 - val_loss: 4.2292 - val_acc: 0.6390\n",
      "Epoch 27/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 4.0988 - acc: 0.6697 - val_loss: 3.9814 - val_acc: 0.6420\n",
      "Epoch 28/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 3.8588 - acc: 0.6765 - val_loss: 3.7491 - val_acc: 0.6420\n",
      "Epoch 29/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 3.6348 - acc: 0.6769 - val_loss: 3.5353 - val_acc: 0.6450\n",
      "Epoch 30/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 3.4267 - acc: 0.6808 - val_loss: 3.3342 - val_acc: 0.6490\n",
      "Epoch 31/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 3.2346 - acc: 0.6862 - val_loss: 3.1503 - val_acc: 0.6510\n",
      "Epoch 32/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 3.0592 - acc: 0.6905 - val_loss: 2.9841 - val_acc: 0.6440\n",
      "Epoch 33/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 2.8997 - acc: 0.6920 - val_loss: 2.8326 - val_acc: 0.6540\n",
      "Epoch 34/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 2.7563 - acc: 0.6962 - val_loss: 2.6968 - val_acc: 0.6500\n",
      "Epoch 35/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 2.6286 - acc: 0.6937 - val_loss: 2.5759 - val_acc: 0.6590\n",
      "Epoch 36/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 2.5154 - acc: 0.6946 - val_loss: 2.4715 - val_acc: 0.6500\n",
      "Epoch 37/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 2.4180 - acc: 0.6932 - val_loss: 2.3797 - val_acc: 0.6530\n",
      "Epoch 38/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 2.3341 - acc: 0.6934 - val_loss: 2.3045 - val_acc: 0.6620\n",
      "Epoch 39/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 2.2639 - acc: 0.6955 - val_loss: 2.2409 - val_acc: 0.6600\n",
      "Epoch 40/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 2.2043 - acc: 0.6949 - val_loss: 2.1845 - val_acc: 0.6560\n",
      "Epoch 41/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 2.1526 - acc: 0.6935 - val_loss: 2.1390 - val_acc: 0.6590\n",
      "Epoch 42/1000\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 2.1140 - acc: 0.6928 - val_loss: 2.1063 - val_acc: 0.6550\n",
      "Epoch 43/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 2.0836 - acc: 0.6948 - val_loss: 2.0790 - val_acc: 0.6520\n",
      "Epoch 44/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 2.0582 - acc: 0.6909 - val_loss: 2.0525 - val_acc: 0.6590\n",
      "Epoch 45/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 2.0351 - acc: 0.6951 - val_loss: 2.0328 - val_acc: 0.6510\n",
      "Epoch 46/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 2.0144 - acc: 0.6937 - val_loss: 2.0113 - val_acc: 0.6600\n",
      "Epoch 47/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.9952 - acc: 0.6934 - val_loss: 1.9921 - val_acc: 0.6610\n",
      "Epoch 48/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.9770 - acc: 0.6926 - val_loss: 1.9797 - val_acc: 0.6600\n",
      "Epoch 49/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.9606 - acc: 0.6960 - val_loss: 1.9586 - val_acc: 0.6570\n",
      "Epoch 50/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.9442 - acc: 0.6940 - val_loss: 1.9433 - val_acc: 0.6590\n",
      "Epoch 51/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.9288 - acc: 0.6922 - val_loss: 1.9308 - val_acc: 0.6600\n",
      "Epoch 52/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.9148 - acc: 0.6952 - val_loss: 1.9155 - val_acc: 0.6600\n",
      "Epoch 53/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.9004 - acc: 0.6940 - val_loss: 1.9027 - val_acc: 0.6640\n",
      "Epoch 54/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.8873 - acc: 0.6952 - val_loss: 1.8886 - val_acc: 0.6640\n",
      "Epoch 55/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.8744 - acc: 0.6943 - val_loss: 1.8777 - val_acc: 0.6610\n",
      "Epoch 56/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.8623 - acc: 0.6957 - val_loss: 1.8663 - val_acc: 0.6620\n",
      "Epoch 57/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.8501 - acc: 0.6945 - val_loss: 1.8541 - val_acc: 0.6550\n",
      "Epoch 58/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.8387 - acc: 0.6948 - val_loss: 1.8422 - val_acc: 0.6610\n",
      "Epoch 59/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.8277 - acc: 0.6949 - val_loss: 1.8378 - val_acc: 0.6660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.8176 - acc: 0.6957 - val_loss: 1.8239 - val_acc: 0.6630\n",
      "Epoch 61/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.8069 - acc: 0.6946 - val_loss: 1.8100 - val_acc: 0.6700\n",
      "Epoch 62/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.7971 - acc: 0.6960 - val_loss: 1.8048 - val_acc: 0.6630\n",
      "Epoch 63/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.7876 - acc: 0.6955 - val_loss: 1.7917 - val_acc: 0.6670\n",
      "Epoch 64/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.7786 - acc: 0.6957 - val_loss: 1.7847 - val_acc: 0.6660\n",
      "Epoch 65/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.7688 - acc: 0.6965 - val_loss: 1.7740 - val_acc: 0.6720\n",
      "Epoch 66/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.7602 - acc: 0.6969 - val_loss: 1.7659 - val_acc: 0.6700\n",
      "Epoch 67/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.7513 - acc: 0.6980 - val_loss: 1.7580 - val_acc: 0.6670\n",
      "Epoch 68/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.7431 - acc: 0.6965 - val_loss: 1.7500 - val_acc: 0.6690\n",
      "Epoch 69/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.7347 - acc: 0.6935 - val_loss: 1.7414 - val_acc: 0.6730\n",
      "Epoch 70/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.7264 - acc: 0.6951 - val_loss: 1.7341 - val_acc: 0.6730\n",
      "Epoch 71/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.7181 - acc: 0.6969 - val_loss: 1.7245 - val_acc: 0.6760\n",
      "Epoch 72/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.7102 - acc: 0.6974 - val_loss: 1.7177 - val_acc: 0.6720\n",
      "Epoch 73/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.7027 - acc: 0.6977 - val_loss: 1.7104 - val_acc: 0.6710\n",
      "Epoch 74/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.6952 - acc: 0.7012 - val_loss: 1.7024 - val_acc: 0.6740\n",
      "Epoch 75/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.6880 - acc: 0.7002 - val_loss: 1.6957 - val_acc: 0.6770\n",
      "Epoch 76/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.6808 - acc: 0.6985 - val_loss: 1.6874 - val_acc: 0.6760\n",
      "Epoch 77/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.6738 - acc: 0.7009 - val_loss: 1.6841 - val_acc: 0.6750\n",
      "Epoch 78/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.6667 - acc: 0.7014 - val_loss: 1.6755 - val_acc: 0.6790\n",
      "Epoch 79/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.6598 - acc: 0.7008 - val_loss: 1.6708 - val_acc: 0.6710\n",
      "Epoch 80/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.6531 - acc: 0.7018 - val_loss: 1.6623 - val_acc: 0.6770\n",
      "Epoch 81/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 1.6463 - acc: 0.7018 - val_loss: 1.6570 - val_acc: 0.6750\n",
      "Epoch 82/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.6402 - acc: 0.7025 - val_loss: 1.6495 - val_acc: 0.6770\n",
      "Epoch 83/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.6329 - acc: 0.7020 - val_loss: 1.6424 - val_acc: 0.6720\n",
      "Epoch 84/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 1.6266 - acc: 0.7029 - val_loss: 1.6361 - val_acc: 0.6770\n",
      "Epoch 85/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 1.6206 - acc: 0.7035 - val_loss: 1.6296 - val_acc: 0.6720\n",
      "Epoch 86/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.6146 - acc: 0.7028 - val_loss: 1.6223 - val_acc: 0.6730\n",
      "Epoch 87/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.6079 - acc: 0.7035 - val_loss: 1.6172 - val_acc: 0.6790\n",
      "Epoch 88/1000\n",
      "6500/6500 [==============================] - 0s 48us/step - loss: 1.6017 - acc: 0.7035 - val_loss: 1.6149 - val_acc: 0.6740\n",
      "Epoch 89/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.5959 - acc: 0.7057 - val_loss: 1.6064 - val_acc: 0.6750\n",
      "Epoch 90/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.5902 - acc: 0.7052 - val_loss: 1.5992 - val_acc: 0.6780\n",
      "Epoch 91/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 1.5841 - acc: 0.7040 - val_loss: 1.5955 - val_acc: 0.6810\n",
      "Epoch 92/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 1.5780 - acc: 0.7062 - val_loss: 1.5899 - val_acc: 0.6790\n",
      "Epoch 93/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 1.5730 - acc: 0.7045 - val_loss: 1.5819 - val_acc: 0.6780\n",
      "Epoch 94/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.5667 - acc: 0.7060 - val_loss: 1.5757 - val_acc: 0.6810\n",
      "Epoch 95/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.5610 - acc: 0.7063 - val_loss: 1.5716 - val_acc: 0.6830\n",
      "Epoch 96/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.5558 - acc: 0.7065 - val_loss: 1.5658 - val_acc: 0.6740\n",
      "Epoch 97/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.5504 - acc: 0.7074 - val_loss: 1.5616 - val_acc: 0.6810\n",
      "Epoch 98/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.5446 - acc: 0.7080 - val_loss: 1.5566 - val_acc: 0.6820\n",
      "Epoch 99/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.5393 - acc: 0.7068 - val_loss: 1.5508 - val_acc: 0.6800\n",
      "Epoch 100/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.5342 - acc: 0.7080 - val_loss: 1.5461 - val_acc: 0.6790\n",
      "Epoch 101/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.5287 - acc: 0.7082 - val_loss: 1.5406 - val_acc: 0.6810\n",
      "Epoch 102/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.5237 - acc: 0.7083 - val_loss: 1.5367 - val_acc: 0.6820\n",
      "Epoch 103/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.5182 - acc: 0.7068 - val_loss: 1.5342 - val_acc: 0.6790\n",
      "Epoch 104/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.5132 - acc: 0.7085 - val_loss: 1.5261 - val_acc: 0.6820\n",
      "Epoch 105/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.5090 - acc: 0.7086 - val_loss: 1.5211 - val_acc: 0.6810\n",
      "Epoch 106/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.5035 - acc: 0.7091 - val_loss: 1.5143 - val_acc: 0.6820\n",
      "Epoch 107/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.4982 - acc: 0.7097 - val_loss: 1.5113 - val_acc: 0.6870\n",
      "Epoch 108/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.4937 - acc: 0.7091 - val_loss: 1.5077 - val_acc: 0.6810\n",
      "Epoch 109/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.4889 - acc: 0.7095 - val_loss: 1.5034 - val_acc: 0.6830\n",
      "Epoch 110/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 1.4840 - acc: 0.7098 - val_loss: 1.4948 - val_acc: 0.6840\n",
      "Epoch 111/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.4795 - acc: 0.7098 - val_loss: 1.4936 - val_acc: 0.6780\n",
      "Epoch 112/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.4747 - acc: 0.7109 - val_loss: 1.4907 - val_acc: 0.6800\n",
      "Epoch 113/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.4698 - acc: 0.7095 - val_loss: 1.4820 - val_acc: 0.6870\n",
      "Epoch 114/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.4654 - acc: 0.7112 - val_loss: 1.4774 - val_acc: 0.6830\n",
      "Epoch 115/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.4605 - acc: 0.7112 - val_loss: 1.4772 - val_acc: 0.6840\n",
      "Epoch 116/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.4558 - acc: 0.7120 - val_loss: 1.4704 - val_acc: 0.6880\n",
      "Epoch 117/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.4513 - acc: 0.7109 - val_loss: 1.4640 - val_acc: 0.6820\n",
      "Epoch 118/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.4467 - acc: 0.7120 - val_loss: 1.4616 - val_acc: 0.6860\n",
      "Epoch 119/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.4419 - acc: 0.7114 - val_loss: 1.4550 - val_acc: 0.6860\n",
      "Epoch 120/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.4378 - acc: 0.7122 - val_loss: 1.4512 - val_acc: 0.6890\n",
      "Epoch 121/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.4336 - acc: 0.7131 - val_loss: 1.4465 - val_acc: 0.6880\n",
      "Epoch 122/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.4288 - acc: 0.7128 - val_loss: 1.4442 - val_acc: 0.6890\n",
      "Epoch 123/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.4252 - acc: 0.7123 - val_loss: 1.4386 - val_acc: 0.6840\n",
      "Epoch 124/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.4203 - acc: 0.7138 - val_loss: 1.4338 - val_acc: 0.6900\n",
      "Epoch 125/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.4162 - acc: 0.7135 - val_loss: 1.4302 - val_acc: 0.6860\n",
      "Epoch 126/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.4120 - acc: 0.7132 - val_loss: 1.4252 - val_acc: 0.6900\n",
      "Epoch 127/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.4078 - acc: 0.7123 - val_loss: 1.4214 - val_acc: 0.6860\n",
      "Epoch 128/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.4034 - acc: 0.7128 - val_loss: 1.4191 - val_acc: 0.6860\n",
      "Epoch 129/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 1.3994 - acc: 0.7155 - val_loss: 1.4137 - val_acc: 0.6880\n",
      "Epoch 130/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.3952 - acc: 0.7148 - val_loss: 1.4099 - val_acc: 0.6870\n",
      "Epoch 131/1000\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 1.3914 - acc: 0.7145 - val_loss: 1.4055 - val_acc: 0.6900\n",
      "Epoch 132/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.3872 - acc: 0.7143 - val_loss: 1.4026 - val_acc: 0.6890\n",
      "Epoch 133/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.3835 - acc: 0.7158 - val_loss: 1.3993 - val_acc: 0.6920\n",
      "Epoch 134/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.3791 - acc: 0.7149 - val_loss: 1.3936 - val_acc: 0.6890\n",
      "Epoch 135/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.3757 - acc: 0.7163 - val_loss: 1.3895 - val_acc: 0.6890\n",
      "Epoch 136/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.3719 - acc: 0.7160 - val_loss: 1.3879 - val_acc: 0.6860\n",
      "Epoch 137/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.3679 - acc: 0.7157 - val_loss: 1.3848 - val_acc: 0.6940\n",
      "Epoch 138/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.3636 - acc: 0.7143 - val_loss: 1.3785 - val_acc: 0.6870\n",
      "Epoch 139/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.3602 - acc: 0.7155 - val_loss: 1.3741 - val_acc: 0.6880\n",
      "Epoch 140/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.3563 - acc: 0.7162 - val_loss: 1.3725 - val_acc: 0.6880\n",
      "Epoch 141/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.3529 - acc: 0.7182 - val_loss: 1.3684 - val_acc: 0.6870\n",
      "Epoch 142/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.3489 - acc: 0.7174 - val_loss: 1.3654 - val_acc: 0.6930\n",
      "Epoch 143/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.3457 - acc: 0.7192 - val_loss: 1.3623 - val_acc: 0.6930\n",
      "Epoch 144/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.3418 - acc: 0.7175 - val_loss: 1.3606 - val_acc: 0.6850\n",
      "Epoch 145/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.3381 - acc: 0.7183 - val_loss: 1.3538 - val_acc: 0.6870\n",
      "Epoch 146/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.3348 - acc: 0.7177 - val_loss: 1.3519 - val_acc: 0.6890\n",
      "Epoch 147/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 1.3311 - acc: 0.7182 - val_loss: 1.3550 - val_acc: 0.6870\n",
      "Epoch 148/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.3280 - acc: 0.7180 - val_loss: 1.3480 - val_acc: 0.6950\n",
      "Epoch 149/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 1.3249 - acc: 0.7174 - val_loss: 1.3415 - val_acc: 0.6960\n",
      "Epoch 150/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.3213 - acc: 0.7188 - val_loss: 1.3413 - val_acc: 0.6930\n",
      "Epoch 151/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.3170 - acc: 0.7192 - val_loss: 1.3350 - val_acc: 0.6930\n",
      "Epoch 152/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.3144 - acc: 0.7192 - val_loss: 1.3314 - val_acc: 0.6880\n",
      "Epoch 153/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.3108 - acc: 0.7202 - val_loss: 1.3304 - val_acc: 0.6950\n",
      "Epoch 154/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.3082 - acc: 0.7195 - val_loss: 1.3238 - val_acc: 0.6910\n",
      "Epoch 155/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.3050 - acc: 0.7205 - val_loss: 1.3242 - val_acc: 0.6920\n",
      "Epoch 156/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.3019 - acc: 0.7203 - val_loss: 1.3185 - val_acc: 0.6950\n",
      "Epoch 157/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.2984 - acc: 0.7189 - val_loss: 1.3166 - val_acc: 0.6970\n",
      "Epoch 158/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.2952 - acc: 0.7226 - val_loss: 1.3194 - val_acc: 0.6920\n",
      "Epoch 159/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.2919 - acc: 0.7206 - val_loss: 1.3103 - val_acc: 0.6990\n",
      "Epoch 160/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.2892 - acc: 0.7203 - val_loss: 1.3060 - val_acc: 0.6950\n",
      "Epoch 161/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.2855 - acc: 0.7211 - val_loss: 1.3049 - val_acc: 0.6990\n",
      "Epoch 162/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.2829 - acc: 0.7208 - val_loss: 1.3000 - val_acc: 0.7010\n",
      "Epoch 163/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.2796 - acc: 0.7220 - val_loss: 1.2983 - val_acc: 0.6910\n",
      "Epoch 164/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.2770 - acc: 0.7228 - val_loss: 1.2970 - val_acc: 0.7010\n",
      "Epoch 165/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.2740 - acc: 0.7222 - val_loss: 1.2955 - val_acc: 0.6990\n",
      "Epoch 166/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.2710 - acc: 0.7220 - val_loss: 1.2920 - val_acc: 0.6970\n",
      "Epoch 167/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.2676 - acc: 0.7232 - val_loss: 1.2847 - val_acc: 0.7010\n",
      "Epoch 168/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.2654 - acc: 0.7226 - val_loss: 1.2838 - val_acc: 0.7030\n",
      "Epoch 169/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.2618 - acc: 0.7223 - val_loss: 1.2818 - val_acc: 0.7030\n",
      "Epoch 170/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.2596 - acc: 0.7225 - val_loss: 1.2801 - val_acc: 0.7020\n",
      "Epoch 171/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.2568 - acc: 0.7234 - val_loss: 1.2757 - val_acc: 0.6970\n",
      "Epoch 172/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.2543 - acc: 0.7226 - val_loss: 1.2729 - val_acc: 0.6930\n",
      "Epoch 173/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.2516 - acc: 0.7235 - val_loss: 1.2693 - val_acc: 0.7030\n",
      "Epoch 174/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.2480 - acc: 0.7260 - val_loss: 1.2692 - val_acc: 0.7030\n",
      "Epoch 175/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 1.2456 - acc: 0.7249 - val_loss: 1.2674 - val_acc: 0.6930\n",
      "Epoch 176/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.2434 - acc: 0.7243 - val_loss: 1.2615 - val_acc: 0.6960\n",
      "Epoch 177/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.2407 - acc: 0.7262 - val_loss: 1.2591 - val_acc: 0.7000\n",
      "Epoch 178/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.2379 - acc: 0.7262 - val_loss: 1.2597 - val_acc: 0.6960\n",
      "Epoch 179/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.2359 - acc: 0.7242 - val_loss: 1.2528 - val_acc: 0.6980\n",
      "Epoch 180/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 1.2326 - acc: 0.7272 - val_loss: 1.2589 - val_acc: 0.7010\n",
      "Epoch 181/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.2308 - acc: 0.7251 - val_loss: 1.2510 - val_acc: 0.7020\n",
      "Epoch 182/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.2280 - acc: 0.7238 - val_loss: 1.2475 - val_acc: 0.7050\n",
      "Epoch 183/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.2251 - acc: 0.7269 - val_loss: 1.2460 - val_acc: 0.7040\n",
      "Epoch 184/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.2224 - acc: 0.7266 - val_loss: 1.2432 - val_acc: 0.6980\n",
      "Epoch 185/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.2201 - acc: 0.7266 - val_loss: 1.2415 - val_acc: 0.7070\n",
      "Epoch 186/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.2185 - acc: 0.7263 - val_loss: 1.2387 - val_acc: 0.7030\n",
      "Epoch 187/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.2151 - acc: 0.7263 - val_loss: 1.2358 - val_acc: 0.7100\n",
      "Epoch 188/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.2133 - acc: 0.7277 - val_loss: 1.2343 - val_acc: 0.7050\n",
      "Epoch 189/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.2115 - acc: 0.7288 - val_loss: 1.2326 - val_acc: 0.7040\n",
      "Epoch 190/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.2084 - acc: 0.7275 - val_loss: 1.2290 - val_acc: 0.7040\n",
      "Epoch 191/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.2062 - acc: 0.7282 - val_loss: 1.2268 - val_acc: 0.7040\n",
      "Epoch 192/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.2031 - acc: 0.7275 - val_loss: 1.2284 - val_acc: 0.6960\n",
      "Epoch 193/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.2021 - acc: 0.7289 - val_loss: 1.2200 - val_acc: 0.7100\n",
      "Epoch 194/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.1991 - acc: 0.7283 - val_loss: 1.2203 - val_acc: 0.7030\n",
      "Epoch 195/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.1970 - acc: 0.7266 - val_loss: 1.2183 - val_acc: 0.7010\n",
      "Epoch 196/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.1945 - acc: 0.7291 - val_loss: 1.2184 - val_acc: 0.7120\n",
      "Epoch 197/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.1925 - acc: 0.7297 - val_loss: 1.2140 - val_acc: 0.7040\n",
      "Epoch 198/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 1.1904 - acc: 0.7298 - val_loss: 1.2134 - val_acc: 0.7040\n",
      "Epoch 199/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.1884 - acc: 0.7297 - val_loss: 1.2098 - val_acc: 0.7020\n",
      "Epoch 200/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.1861 - acc: 0.7285 - val_loss: 1.2098 - val_acc: 0.7070\n",
      "Epoch 201/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.1845 - acc: 0.7291 - val_loss: 1.2138 - val_acc: 0.7000\n",
      "Epoch 202/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.1823 - acc: 0.7298 - val_loss: 1.2031 - val_acc: 0.7090\n",
      "Epoch 203/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.1798 - acc: 0.7300 - val_loss: 1.2017 - val_acc: 0.7080\n",
      "Epoch 204/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.1781 - acc: 0.7317 - val_loss: 1.1993 - val_acc: 0.7100\n",
      "Epoch 205/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.1757 - acc: 0.7305 - val_loss: 1.1986 - val_acc: 0.7110\n",
      "Epoch 206/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.1743 - acc: 0.7309 - val_loss: 1.1967 - val_acc: 0.7070\n",
      "Epoch 207/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.1723 - acc: 0.7306 - val_loss: 1.1957 - val_acc: 0.7060\n",
      "Epoch 208/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.1702 - acc: 0.7300 - val_loss: 1.1934 - val_acc: 0.7020\n",
      "Epoch 209/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.1683 - acc: 0.7325 - val_loss: 1.1922 - val_acc: 0.7050\n",
      "Epoch 210/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.1664 - acc: 0.7323 - val_loss: 1.1914 - val_acc: 0.7100\n",
      "Epoch 211/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.1646 - acc: 0.7317 - val_loss: 1.1880 - val_acc: 0.7140\n",
      "Epoch 212/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.1630 - acc: 0.7314 - val_loss: 1.1860 - val_acc: 0.7140\n",
      "Epoch 213/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.1611 - acc: 0.7334 - val_loss: 1.1849 - val_acc: 0.7140\n",
      "Epoch 214/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.1597 - acc: 0.7322 - val_loss: 1.1809 - val_acc: 0.7130\n",
      "Epoch 215/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.1577 - acc: 0.7325 - val_loss: 1.1801 - val_acc: 0.7130\n",
      "Epoch 216/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.1560 - acc: 0.7329 - val_loss: 1.1788 - val_acc: 0.7160\n",
      "Epoch 217/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.1542 - acc: 0.7326 - val_loss: 1.1783 - val_acc: 0.7110\n",
      "Epoch 218/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.1525 - acc: 0.7331 - val_loss: 1.1752 - val_acc: 0.7090\n",
      "Epoch 219/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.1509 - acc: 0.7335 - val_loss: 1.1763 - val_acc: 0.7170\n",
      "Epoch 220/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.1497 - acc: 0.7334 - val_loss: 1.1726 - val_acc: 0.7090\n",
      "Epoch 221/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.1480 - acc: 0.7335 - val_loss: 1.1735 - val_acc: 0.7070\n",
      "Epoch 222/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.1460 - acc: 0.7332 - val_loss: 1.1700 - val_acc: 0.7130\n",
      "Epoch 223/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.1446 - acc: 0.7320 - val_loss: 1.1681 - val_acc: 0.7120\n",
      "Epoch 224/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.1429 - acc: 0.7348 - val_loss: 1.1691 - val_acc: 0.7150\n",
      "Epoch 225/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.1415 - acc: 0.7332 - val_loss: 1.1729 - val_acc: 0.7090\n",
      "Epoch 226/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.1404 - acc: 0.7358 - val_loss: 1.1637 - val_acc: 0.7160\n",
      "Epoch 227/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.1382 - acc: 0.7371 - val_loss: 1.1655 - val_acc: 0.7160\n",
      "Epoch 228/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.1373 - acc: 0.7335 - val_loss: 1.1632 - val_acc: 0.7140\n",
      "Epoch 229/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.1352 - acc: 0.7346 - val_loss: 1.1582 - val_acc: 0.7200\n",
      "Epoch 230/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.1344 - acc: 0.7377 - val_loss: 1.1595 - val_acc: 0.7150\n",
      "Epoch 231/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.1326 - acc: 0.7363 - val_loss: 1.1591 - val_acc: 0.7110\n",
      "Epoch 232/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.1315 - acc: 0.7340 - val_loss: 1.1570 - val_acc: 0.7110\n",
      "Epoch 233/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.1295 - acc: 0.7354 - val_loss: 1.1545 - val_acc: 0.7120\n",
      "Epoch 234/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.1282 - acc: 0.7369 - val_loss: 1.1543 - val_acc: 0.7170\n",
      "Epoch 235/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.1273 - acc: 0.7365 - val_loss: 1.1510 - val_acc: 0.7190\n",
      "Epoch 236/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 1.1256 - acc: 0.7378 - val_loss: 1.1502 - val_acc: 0.7190\n",
      "Epoch 237/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.1247 - acc: 0.7369 - val_loss: 1.1552 - val_acc: 0.7090\n",
      "Epoch 238/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.1227 - acc: 0.7368 - val_loss: 1.1461 - val_acc: 0.7190\n",
      "Epoch 239/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.1211 - acc: 0.7377 - val_loss: 1.1462 - val_acc: 0.7140\n",
      "Epoch 240/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.1199 - acc: 0.7365 - val_loss: 1.1489 - val_acc: 0.7120\n",
      "Epoch 241/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.1183 - acc: 0.7365 - val_loss: 1.1443 - val_acc: 0.7180\n",
      "Epoch 242/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.1174 - acc: 0.7369 - val_loss: 1.1434 - val_acc: 0.7190\n",
      "Epoch 243/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.1157 - acc: 0.7382 - val_loss: 1.1401 - val_acc: 0.7150\n",
      "Epoch 244/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.1145 - acc: 0.7368 - val_loss: 1.1426 - val_acc: 0.7150\n",
      "Epoch 245/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.1129 - acc: 0.7386 - val_loss: 1.1437 - val_acc: 0.7080\n",
      "Epoch 246/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.1121 - acc: 0.7380 - val_loss: 1.1382 - val_acc: 0.7190\n",
      "Epoch 247/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.1104 - acc: 0.7391 - val_loss: 1.1384 - val_acc: 0.7210\n",
      "Epoch 248/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.1091 - acc: 0.7382 - val_loss: 1.1352 - val_acc: 0.7190\n",
      "Epoch 249/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.1078 - acc: 0.7388 - val_loss: 1.1336 - val_acc: 0.7190\n",
      "Epoch 250/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.1062 - acc: 0.7388 - val_loss: 1.1356 - val_acc: 0.7150\n",
      "Epoch 251/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.1049 - acc: 0.7360 - val_loss: 1.1339 - val_acc: 0.7210\n",
      "Epoch 252/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.1041 - acc: 0.7395 - val_loss: 1.1327 - val_acc: 0.7120\n",
      "Epoch 253/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.1034 - acc: 0.7392 - val_loss: 1.1293 - val_acc: 0.7160\n",
      "Epoch 254/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.1011 - acc: 0.7408 - val_loss: 1.1285 - val_acc: 0.7230\n",
      "Epoch 255/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.1000 - acc: 0.7408 - val_loss: 1.1286 - val_acc: 0.7220\n",
      "Epoch 256/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.0989 - acc: 0.7414 - val_loss: 1.1280 - val_acc: 0.7210\n",
      "Epoch 257/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.0978 - acc: 0.7414 - val_loss: 1.1239 - val_acc: 0.7210\n",
      "Epoch 258/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.0956 - acc: 0.7414 - val_loss: 1.1224 - val_acc: 0.7220\n",
      "Epoch 259/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.0950 - acc: 0.7409 - val_loss: 1.1208 - val_acc: 0.7210\n",
      "Epoch 260/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.0934 - acc: 0.7406 - val_loss: 1.1239 - val_acc: 0.7240\n",
      "Epoch 261/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.0929 - acc: 0.7414 - val_loss: 1.1220 - val_acc: 0.7200\n",
      "Epoch 262/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.0912 - acc: 0.7422 - val_loss: 1.1228 - val_acc: 0.7110\n",
      "Epoch 263/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.0904 - acc: 0.7415 - val_loss: 1.1191 - val_acc: 0.7190\n",
      "Epoch 264/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.0893 - acc: 0.7408 - val_loss: 1.1174 - val_acc: 0.7180\n",
      "Epoch 265/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.0876 - acc: 0.7420 - val_loss: 1.1146 - val_acc: 0.7220\n",
      "Epoch 266/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.0871 - acc: 0.7426 - val_loss: 1.1175 - val_acc: 0.7230\n",
      "Epoch 267/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.0856 - acc: 0.7406 - val_loss: 1.1127 - val_acc: 0.7180\n",
      "Epoch 268/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.0843 - acc: 0.7422 - val_loss: 1.1141 - val_acc: 0.7240\n",
      "Epoch 269/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.0829 - acc: 0.7414 - val_loss: 1.1130 - val_acc: 0.7220\n",
      "Epoch 270/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.0814 - acc: 0.7451 - val_loss: 1.1107 - val_acc: 0.7180\n",
      "Epoch 271/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.0804 - acc: 0.7420 - val_loss: 1.1073 - val_acc: 0.7200\n",
      "Epoch 272/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.0791 - acc: 0.7426 - val_loss: 1.1071 - val_acc: 0.7240\n",
      "Epoch 273/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.0777 - acc: 0.7449 - val_loss: 1.1084 - val_acc: 0.7210\n",
      "Epoch 274/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.0776 - acc: 0.7438 - val_loss: 1.1049 - val_acc: 0.7240\n",
      "Epoch 275/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.0755 - acc: 0.7417 - val_loss: 1.1034 - val_acc: 0.7240\n",
      "Epoch 276/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.0744 - acc: 0.7432 - val_loss: 1.1063 - val_acc: 0.7220\n",
      "Epoch 277/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.0741 - acc: 0.7440 - val_loss: 1.1034 - val_acc: 0.7210\n",
      "Epoch 278/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.0730 - acc: 0.7437 - val_loss: 1.1055 - val_acc: 0.7170\n",
      "Epoch 279/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.0720 - acc: 0.7445 - val_loss: 1.1023 - val_acc: 0.7190\n",
      "Epoch 280/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 1.0713 - acc: 0.7417 - val_loss: 1.0993 - val_acc: 0.7240\n",
      "Epoch 281/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.0695 - acc: 0.7437 - val_loss: 1.0987 - val_acc: 0.7200\n",
      "Epoch 282/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.0686 - acc: 0.7438 - val_loss: 1.1005 - val_acc: 0.7190\n",
      "Epoch 283/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.0674 - acc: 0.7451 - val_loss: 1.0978 - val_acc: 0.7230\n",
      "Epoch 284/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.0665 - acc: 0.7445 - val_loss: 1.0975 - val_acc: 0.7250\n",
      "Epoch 285/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.0654 - acc: 0.7432 - val_loss: 1.0957 - val_acc: 0.7200\n",
      "Epoch 286/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 1.0640 - acc: 0.7440 - val_loss: 1.0936 - val_acc: 0.7220\n",
      "Epoch 287/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.0633 - acc: 0.7457 - val_loss: 1.0945 - val_acc: 0.7220\n",
      "Epoch 288/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.0626 - acc: 0.7460 - val_loss: 1.0928 - val_acc: 0.7180\n",
      "Epoch 289/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 1.0611 - acc: 0.7454 - val_loss: 1.0949 - val_acc: 0.7180\n",
      "Epoch 290/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.0603 - acc: 0.7457 - val_loss: 1.0916 - val_acc: 0.7240\n",
      "Epoch 291/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.0594 - acc: 0.7460 - val_loss: 1.0909 - val_acc: 0.7210\n",
      "Epoch 292/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.0585 - acc: 0.7472 - val_loss: 1.0906 - val_acc: 0.7190\n",
      "Epoch 293/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.0577 - acc: 0.7460 - val_loss: 1.0891 - val_acc: 0.7210\n",
      "Epoch 294/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.0563 - acc: 0.7480 - val_loss: 1.0873 - val_acc: 0.7270\n",
      "Epoch 295/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.0553 - acc: 0.7458 - val_loss: 1.0857 - val_acc: 0.7280\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.0545 - acc: 0.7454 - val_loss: 1.0878 - val_acc: 0.7210\n",
      "Epoch 297/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.0541 - acc: 0.7443 - val_loss: 1.0838 - val_acc: 0.7230\n",
      "Epoch 298/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.0527 - acc: 0.7457 - val_loss: 1.0847 - val_acc: 0.7230\n",
      "Epoch 299/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.0523 - acc: 0.7475 - val_loss: 1.0817 - val_acc: 0.7250\n",
      "Epoch 300/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.0509 - acc: 0.7469 - val_loss: 1.0808 - val_acc: 0.7260\n",
      "Epoch 301/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.0493 - acc: 0.7477 - val_loss: 1.0842 - val_acc: 0.7220\n",
      "Epoch 302/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.0488 - acc: 0.7472 - val_loss: 1.0824 - val_acc: 0.7210\n",
      "Epoch 303/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.0482 - acc: 0.7486 - val_loss: 1.0782 - val_acc: 0.7280\n",
      "Epoch 304/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 1.0467 - acc: 0.7440 - val_loss: 1.0778 - val_acc: 0.7250\n",
      "Epoch 305/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.0464 - acc: 0.7474 - val_loss: 1.0779 - val_acc: 0.7270\n",
      "Epoch 306/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.0450 - acc: 0.7491 - val_loss: 1.0797 - val_acc: 0.7270\n",
      "Epoch 307/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 1.0450 - acc: 0.7465 - val_loss: 1.0762 - val_acc: 0.7240\n",
      "Epoch 308/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 1.0437 - acc: 0.7495 - val_loss: 1.0801 - val_acc: 0.7230\n",
      "Epoch 309/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 1.0428 - acc: 0.7478 - val_loss: 1.0753 - val_acc: 0.7270\n",
      "Epoch 310/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.0421 - acc: 0.7498 - val_loss: 1.0779 - val_acc: 0.7270\n",
      "Epoch 311/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.0411 - acc: 0.7482 - val_loss: 1.0754 - val_acc: 0.7280\n",
      "Epoch 312/1000\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 1.0399 - acc: 0.7494 - val_loss: 1.0755 - val_acc: 0.7240\n",
      "Epoch 313/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.0393 - acc: 0.7506 - val_loss: 1.0749 - val_acc: 0.7240\n",
      "Epoch 314/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 1.0393 - acc: 0.7485 - val_loss: 1.0730 - val_acc: 0.7230\n",
      "Epoch 315/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.0381 - acc: 0.7489 - val_loss: 1.0715 - val_acc: 0.7280\n",
      "Epoch 316/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.0371 - acc: 0.7512 - val_loss: 1.0728 - val_acc: 0.7310\n",
      "Epoch 317/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.0364 - acc: 0.7497 - val_loss: 1.0685 - val_acc: 0.7250\n",
      "Epoch 318/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.0352 - acc: 0.7488 - val_loss: 1.0684 - val_acc: 0.7310\n",
      "Epoch 319/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.0346 - acc: 0.7512 - val_loss: 1.0703 - val_acc: 0.7280\n",
      "Epoch 320/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 1.0336 - acc: 0.7485 - val_loss: 1.0696 - val_acc: 0.7290\n",
      "Epoch 321/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.0333 - acc: 0.7506 - val_loss: 1.0668 - val_acc: 0.7290\n",
      "Epoch 322/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.0327 - acc: 0.7514 - val_loss: 1.0698 - val_acc: 0.7290\n",
      "Epoch 323/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.0315 - acc: 0.7498 - val_loss: 1.0640 - val_acc: 0.7280\n",
      "Epoch 324/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.0306 - acc: 0.7500 - val_loss: 1.0662 - val_acc: 0.7320\n",
      "Epoch 325/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.0304 - acc: 0.7508 - val_loss: 1.0672 - val_acc: 0.7270\n",
      "Epoch 326/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.0290 - acc: 0.7503 - val_loss: 1.0661 - val_acc: 0.7230\n",
      "Epoch 327/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.0280 - acc: 0.7505 - val_loss: 1.0628 - val_acc: 0.7340\n",
      "Epoch 328/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 1.0274 - acc: 0.7506 - val_loss: 1.0633 - val_acc: 0.7220\n",
      "Epoch 329/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.0276 - acc: 0.7506 - val_loss: 1.0621 - val_acc: 0.7220\n",
      "Epoch 330/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.0259 - acc: 0.7522 - val_loss: 1.0605 - val_acc: 0.7230\n",
      "Epoch 331/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.0257 - acc: 0.7515 - val_loss: 1.0600 - val_acc: 0.7250\n",
      "Epoch 332/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.0249 - acc: 0.7514 - val_loss: 1.0601 - val_acc: 0.7220\n",
      "Epoch 333/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.0244 - acc: 0.7514 - val_loss: 1.0575 - val_acc: 0.7280\n",
      "Epoch 334/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.0237 - acc: 0.7526 - val_loss: 1.0565 - val_acc: 0.7300\n",
      "Epoch 335/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.0223 - acc: 0.7531 - val_loss: 1.0600 - val_acc: 0.7300\n",
      "Epoch 336/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.0216 - acc: 0.7529 - val_loss: 1.0576 - val_acc: 0.7300\n",
      "Epoch 337/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.0213 - acc: 0.7537 - val_loss: 1.0576 - val_acc: 0.7190\n",
      "Epoch 338/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.0209 - acc: 0.7529 - val_loss: 1.0558 - val_acc: 0.7260\n",
      "Epoch 339/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.0194 - acc: 0.7509 - val_loss: 1.0574 - val_acc: 0.7270\n",
      "Epoch 340/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.0193 - acc: 0.7525 - val_loss: 1.0528 - val_acc: 0.7300\n",
      "Epoch 341/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.0186 - acc: 0.7520 - val_loss: 1.0565 - val_acc: 0.7270\n",
      "Epoch 342/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.0175 - acc: 0.7534 - val_loss: 1.0525 - val_acc: 0.7220\n",
      "Epoch 343/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.0166 - acc: 0.7526 - val_loss: 1.0519 - val_acc: 0.7240\n",
      "Epoch 344/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.0160 - acc: 0.7543 - val_loss: 1.0595 - val_acc: 0.7280\n",
      "Epoch 345/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.0162 - acc: 0.7526 - val_loss: 1.0500 - val_acc: 0.7310\n",
      "Epoch 346/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.0145 - acc: 0.7531 - val_loss: 1.0528 - val_acc: 0.7240\n",
      "Epoch 347/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.0145 - acc: 0.7535 - val_loss: 1.0568 - val_acc: 0.7220\n",
      "Epoch 348/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.0139 - acc: 0.7554 - val_loss: 1.0506 - val_acc: 0.7290\n",
      "Epoch 349/1000\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 1.0129 - acc: 0.7535 - val_loss: 1.0518 - val_acc: 0.7220\n",
      "Epoch 350/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.0126 - acc: 0.7534 - val_loss: 1.0496 - val_acc: 0.7270\n",
      "Epoch 351/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.0119 - acc: 0.7532 - val_loss: 1.0486 - val_acc: 0.7330\n",
      "Epoch 352/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.0111 - acc: 0.7543 - val_loss: 1.0461 - val_acc: 0.7330\n",
      "Epoch 353/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.0097 - acc: 0.7545 - val_loss: 1.0483 - val_acc: 0.7290\n",
      "Epoch 354/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.0091 - acc: 0.7545 - val_loss: 1.0456 - val_acc: 0.7310\n",
      "Epoch 355/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.0086 - acc: 0.7537 - val_loss: 1.0463 - val_acc: 0.7300\n",
      "Epoch 356/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.0079 - acc: 0.7538 - val_loss: 1.0429 - val_acc: 0.7360\n",
      "Epoch 357/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 1.0077 - acc: 0.7558 - val_loss: 1.0450 - val_acc: 0.7300\n",
      "Epoch 358/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.0068 - acc: 0.7558 - val_loss: 1.0422 - val_acc: 0.7340\n",
      "Epoch 359/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 1.0057 - acc: 0.7551 - val_loss: 1.0439 - val_acc: 0.7260\n",
      "Epoch 360/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.0054 - acc: 0.7546 - val_loss: 1.0456 - val_acc: 0.7310\n",
      "Epoch 361/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.0040 - acc: 0.7560 - val_loss: 1.0408 - val_acc: 0.7300\n",
      "Epoch 362/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.0037 - acc: 0.7563 - val_loss: 1.0417 - val_acc: 0.7240\n",
      "Epoch 363/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.0033 - acc: 0.7569 - val_loss: 1.0427 - val_acc: 0.7290\n",
      "Epoch 364/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.0035 - acc: 0.7555 - val_loss: 1.0403 - val_acc: 0.7230\n",
      "Epoch 365/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.0020 - acc: 0.7585 - val_loss: 1.0399 - val_acc: 0.7280\n",
      "Epoch 366/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.0019 - acc: 0.7543 - val_loss: 1.0399 - val_acc: 0.7270\n",
      "Epoch 367/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.0009 - acc: 0.7548 - val_loss: 1.0421 - val_acc: 0.7270\n",
      "Epoch 368/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.0006 - acc: 0.7566 - val_loss: 1.0397 - val_acc: 0.7330\n",
      "Epoch 369/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.9999 - acc: 0.7562 - val_loss: 1.0371 - val_acc: 0.7270\n",
      "Epoch 370/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9990 - acc: 0.7562 - val_loss: 1.0404 - val_acc: 0.7280\n",
      "Epoch 371/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9985 - acc: 0.7568 - val_loss: 1.0365 - val_acc: 0.7320\n",
      "Epoch 372/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.9979 - acc: 0.7549 - val_loss: 1.0368 - val_acc: 0.7310\n",
      "Epoch 373/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9971 - acc: 0.7575 - val_loss: 1.0389 - val_acc: 0.7290\n",
      "Epoch 374/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9967 - acc: 0.7580 - val_loss: 1.0337 - val_acc: 0.7250\n",
      "Epoch 375/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.9960 - acc: 0.7589 - val_loss: 1.0340 - val_acc: 0.7230\n",
      "Epoch 376/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9953 - acc: 0.7566 - val_loss: 1.0351 - val_acc: 0.7320\n",
      "Epoch 377/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9943 - acc: 0.7588 - val_loss: 1.0349 - val_acc: 0.7290\n",
      "Epoch 378/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9943 - acc: 0.7568 - val_loss: 1.0334 - val_acc: 0.7250\n",
      "Epoch 379/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.9937 - acc: 0.7566 - val_loss: 1.0335 - val_acc: 0.7310\n",
      "Epoch 380/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9933 - acc: 0.7562 - val_loss: 1.0323 - val_acc: 0.7300\n",
      "Epoch 381/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9922 - acc: 0.7586 - val_loss: 1.0317 - val_acc: 0.7300\n",
      "Epoch 382/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.9913 - acc: 0.7577 - val_loss: 1.0310 - val_acc: 0.7260\n",
      "Epoch 383/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9916 - acc: 0.7560 - val_loss: 1.0323 - val_acc: 0.7290\n",
      "Epoch 384/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9907 - acc: 0.7589 - val_loss: 1.0314 - val_acc: 0.7330\n",
      "Epoch 385/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.9900 - acc: 0.7571 - val_loss: 1.0322 - val_acc: 0.7280\n",
      "Epoch 386/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9900 - acc: 0.7606 - val_loss: 1.0408 - val_acc: 0.7310\n",
      "Epoch 387/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9901 - acc: 0.7568 - val_loss: 1.0291 - val_acc: 0.7310\n",
      "Epoch 388/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9887 - acc: 0.7585 - val_loss: 1.0284 - val_acc: 0.7330\n",
      "Epoch 389/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.9881 - acc: 0.7574 - val_loss: 1.0289 - val_acc: 0.7330\n",
      "Epoch 390/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.9866 - acc: 0.7605 - val_loss: 1.0343 - val_acc: 0.7240\n",
      "Epoch 391/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.9875 - acc: 0.7569 - val_loss: 1.0285 - val_acc: 0.7280\n",
      "Epoch 392/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9868 - acc: 0.7574 - val_loss: 1.0293 - val_acc: 0.7260\n",
      "Epoch 393/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9854 - acc: 0.7615 - val_loss: 1.0255 - val_acc: 0.7360\n",
      "Epoch 394/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9854 - acc: 0.7592 - val_loss: 1.0256 - val_acc: 0.7310\n",
      "Epoch 395/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.9846 - acc: 0.7575 - val_loss: 1.0319 - val_acc: 0.7310\n",
      "Epoch 396/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9844 - acc: 0.7589 - val_loss: 1.0247 - val_acc: 0.7310\n",
      "Epoch 397/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.9839 - acc: 0.7589 - val_loss: 1.0299 - val_acc: 0.7340\n",
      "Epoch 398/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9830 - acc: 0.7617 - val_loss: 1.0245 - val_acc: 0.7350\n",
      "Epoch 399/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9824 - acc: 0.7595 - val_loss: 1.0266 - val_acc: 0.7310\n",
      "Epoch 400/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9818 - acc: 0.7591 - val_loss: 1.0255 - val_acc: 0.7230\n",
      "Epoch 401/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9811 - acc: 0.7586 - val_loss: 1.0300 - val_acc: 0.7240\n",
      "Epoch 402/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.9817 - acc: 0.7632 - val_loss: 1.0249 - val_acc: 0.7310\n",
      "Epoch 403/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.9805 - acc: 0.7603 - val_loss: 1.0306 - val_acc: 0.7220\n",
      "Epoch 404/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9806 - acc: 0.7606 - val_loss: 1.0207 - val_acc: 0.7300\n",
      "Epoch 405/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9796 - acc: 0.7620 - val_loss: 1.0240 - val_acc: 0.7310\n",
      "Epoch 406/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9784 - acc: 0.7608 - val_loss: 1.0238 - val_acc: 0.7310\n",
      "Epoch 407/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9787 - acc: 0.7605 - val_loss: 1.0218 - val_acc: 0.7280\n",
      "Epoch 408/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9784 - acc: 0.7614 - val_loss: 1.0238 - val_acc: 0.7270\n",
      "Epoch 409/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9775 - acc: 0.7631 - val_loss: 1.0275 - val_acc: 0.7330\n",
      "Epoch 410/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9770 - acc: 0.7609 - val_loss: 1.0209 - val_acc: 0.7270\n",
      "Epoch 411/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.9759 - acc: 0.7620 - val_loss: 1.0214 - val_acc: 0.7330\n",
      "Epoch 412/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9760 - acc: 0.7603 - val_loss: 1.0229 - val_acc: 0.7210\n",
      "Epoch 413/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9753 - acc: 0.7626 - val_loss: 1.0197 - val_acc: 0.7350\n",
      "Epoch 414/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9754 - acc: 0.7655 - val_loss: 1.0185 - val_acc: 0.7330\n",
      "Epoch 415/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9750 - acc: 0.7628 - val_loss: 1.0191 - val_acc: 0.7320\n",
      "Epoch 416/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9737 - acc: 0.7623 - val_loss: 1.0197 - val_acc: 0.7260\n",
      "Epoch 417/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9741 - acc: 0.7608 - val_loss: 1.0159 - val_acc: 0.7350\n",
      "Epoch 418/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9729 - acc: 0.7640 - val_loss: 1.0152 - val_acc: 0.7370\n",
      "Epoch 419/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9724 - acc: 0.7628 - val_loss: 1.0169 - val_acc: 0.7260\n",
      "Epoch 420/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9726 - acc: 0.7635 - val_loss: 1.0183 - val_acc: 0.7350\n",
      "Epoch 421/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.9716 - acc: 0.7618 - val_loss: 1.0157 - val_acc: 0.7270\n",
      "Epoch 422/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9709 - acc: 0.7626 - val_loss: 1.0136 - val_acc: 0.7340\n",
      "Epoch 423/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9707 - acc: 0.7637 - val_loss: 1.0146 - val_acc: 0.7290\n",
      "Epoch 424/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.9709 - acc: 0.7608 - val_loss: 1.0166 - val_acc: 0.7360\n",
      "Epoch 425/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.9697 - acc: 0.7632 - val_loss: 1.0157 - val_acc: 0.7300\n",
      "Epoch 426/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9695 - acc: 0.7620 - val_loss: 1.0136 - val_acc: 0.7330\n",
      "Epoch 427/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9688 - acc: 0.7632 - val_loss: 1.0138 - val_acc: 0.7330\n",
      "Epoch 428/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9692 - acc: 0.7611 - val_loss: 1.0127 - val_acc: 0.7380\n",
      "Epoch 429/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9672 - acc: 0.7623 - val_loss: 1.0222 - val_acc: 0.7300\n",
      "Epoch 430/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9677 - acc: 0.7635 - val_loss: 1.0134 - val_acc: 0.7270\n",
      "Epoch 431/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9670 - acc: 0.7637 - val_loss: 1.0159 - val_acc: 0.7340\n",
      "Epoch 432/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9664 - acc: 0.7635 - val_loss: 1.0220 - val_acc: 0.7340\n",
      "Epoch 433/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9671 - acc: 0.7614 - val_loss: 1.0125 - val_acc: 0.7300\n",
      "Epoch 434/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9664 - acc: 0.7611 - val_loss: 1.0098 - val_acc: 0.7330\n",
      "Epoch 435/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9650 - acc: 0.7642 - val_loss: 1.0108 - val_acc: 0.7340\n",
      "Epoch 436/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9647 - acc: 0.7648 - val_loss: 1.0113 - val_acc: 0.7260\n",
      "Epoch 437/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9642 - acc: 0.7623 - val_loss: 1.0100 - val_acc: 0.7300\n",
      "Epoch 438/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9633 - acc: 0.7645 - val_loss: 1.0121 - val_acc: 0.7240\n",
      "Epoch 439/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9629 - acc: 0.7654 - val_loss: 1.0188 - val_acc: 0.7230\n",
      "Epoch 440/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9630 - acc: 0.7620 - val_loss: 1.0088 - val_acc: 0.7360\n",
      "Epoch 441/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9624 - acc: 0.7629 - val_loss: 1.0073 - val_acc: 0.7370\n",
      "Epoch 442/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9626 - acc: 0.7642 - val_loss: 1.0075 - val_acc: 0.7360\n",
      "Epoch 443/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9610 - acc: 0.7642 - val_loss: 1.0083 - val_acc: 0.7280\n",
      "Epoch 444/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9618 - acc: 0.7629 - val_loss: 1.0097 - val_acc: 0.7240\n",
      "Epoch 445/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9608 - acc: 0.7635 - val_loss: 1.0068 - val_acc: 0.7330\n",
      "Epoch 446/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9602 - acc: 0.7643 - val_loss: 1.0069 - val_acc: 0.7330\n",
      "Epoch 447/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9587 - acc: 0.7646 - val_loss: 1.0059 - val_acc: 0.7290\n",
      "Epoch 448/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.9594 - acc: 0.7632 - val_loss: 1.0077 - val_acc: 0.7340\n",
      "Epoch 449/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9585 - acc: 0.7678 - val_loss: 1.0065 - val_acc: 0.7280\n",
      "Epoch 450/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9582 - acc: 0.7646 - val_loss: 1.0042 - val_acc: 0.7340\n",
      "Epoch 451/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9574 - acc: 0.7660 - val_loss: 1.0080 - val_acc: 0.7260\n",
      "Epoch 452/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9573 - acc: 0.7640 - val_loss: 1.0050 - val_acc: 0.7290\n",
      "Epoch 453/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9563 - acc: 0.7663 - val_loss: 1.0054 - val_acc: 0.7280\n",
      "Epoch 454/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9563 - acc: 0.7658 - val_loss: 1.0049 - val_acc: 0.7260\n",
      "Epoch 455/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9565 - acc: 0.7654 - val_loss: 1.0031 - val_acc: 0.7300\n",
      "Epoch 456/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.9557 - acc: 0.7635 - val_loss: 1.0069 - val_acc: 0.7330\n",
      "Epoch 457/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9556 - acc: 0.7658 - val_loss: 1.0026 - val_acc: 0.7320\n",
      "Epoch 458/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.9558 - acc: 0.7662 - val_loss: 1.0050 - val_acc: 0.7330\n",
      "Epoch 459/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9539 - acc: 0.7669 - val_loss: 1.0056 - val_acc: 0.7400\n",
      "Epoch 460/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9538 - acc: 0.7668 - val_loss: 1.0027 - val_acc: 0.7350\n",
      "Epoch 461/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9537 - acc: 0.7675 - val_loss: 1.0067 - val_acc: 0.7370\n",
      "Epoch 462/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9533 - acc: 0.7652 - val_loss: 1.0071 - val_acc: 0.7310\n",
      "Epoch 463/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9529 - acc: 0.7668 - val_loss: 1.0035 - val_acc: 0.7340\n",
      "Epoch 464/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9540 - acc: 0.7651 - val_loss: 1.0066 - val_acc: 0.7370\n",
      "Epoch 465/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9522 - acc: 0.7672 - val_loss: 1.0017 - val_acc: 0.7300\n",
      "Epoch 466/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9518 - acc: 0.7678 - val_loss: 1.0001 - val_acc: 0.7340\n",
      "Epoch 467/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.9511 - acc: 0.7649 - val_loss: 1.0015 - val_acc: 0.7280\n",
      "Epoch 468/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9517 - acc: 0.7666 - val_loss: 1.0103 - val_acc: 0.7240\n",
      "Epoch 469/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.9504 - acc: 0.7666 - val_loss: 1.0020 - val_acc: 0.7370\n",
      "Epoch 470/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9501 - acc: 0.7669 - val_loss: 1.0010 - val_acc: 0.7330\n",
      "Epoch 471/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9491 - acc: 0.7695 - val_loss: 1.0022 - val_acc: 0.7320\n",
      "Epoch 472/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.9495 - acc: 0.7694 - val_loss: 1.0002 - val_acc: 0.7230\n",
      "Epoch 473/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 60us/step - loss: 0.9487 - acc: 0.7714 - val_loss: 0.9981 - val_acc: 0.7310\n",
      "Epoch 474/1000\n",
      "6500/6500 [==============================] - 0s 47us/step - loss: 0.9483 - acc: 0.7654 - val_loss: 0.9963 - val_acc: 0.7310\n",
      "Epoch 475/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.9479 - acc: 0.7669 - val_loss: 0.9953 - val_acc: 0.7350\n",
      "Epoch 476/1000\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 0.9469 - acc: 0.7688 - val_loss: 0.9982 - val_acc: 0.7330\n",
      "Epoch 477/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9468 - acc: 0.7692 - val_loss: 1.0031 - val_acc: 0.7360\n",
      "Epoch 478/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9469 - acc: 0.7682 - val_loss: 0.9995 - val_acc: 0.7360\n",
      "Epoch 479/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9473 - acc: 0.7692 - val_loss: 0.9951 - val_acc: 0.7310\n",
      "Epoch 480/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9468 - acc: 0.7665 - val_loss: 1.0000 - val_acc: 0.7290\n",
      "Epoch 481/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.9455 - acc: 0.7703 - val_loss: 0.9967 - val_acc: 0.7320\n",
      "Epoch 482/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9456 - acc: 0.7697 - val_loss: 0.9997 - val_acc: 0.7360\n",
      "Epoch 483/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9455 - acc: 0.7702 - val_loss: 0.9981 - val_acc: 0.7270\n",
      "Epoch 484/1000\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 0.9448 - acc: 0.7668 - val_loss: 0.9982 - val_acc: 0.7350\n",
      "Epoch 485/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9441 - acc: 0.7685 - val_loss: 0.9977 - val_acc: 0.7270\n",
      "Epoch 486/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9435 - acc: 0.7677 - val_loss: 0.9970 - val_acc: 0.7380\n",
      "Epoch 487/1000\n",
      "6500/6500 [==============================] - 0s 47us/step - loss: 0.9431 - acc: 0.7686 - val_loss: 0.9944 - val_acc: 0.7330\n",
      "Epoch 488/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.9435 - acc: 0.7686 - val_loss: 0.9958 - val_acc: 0.7300\n",
      "Epoch 489/1000\n",
      "6500/6500 [==============================] - 0s 47us/step - loss: 0.9425 - acc: 0.7692 - val_loss: 0.9961 - val_acc: 0.7340\n",
      "Epoch 490/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.9422 - acc: 0.7712 - val_loss: 0.9938 - val_acc: 0.7340\n",
      "Epoch 491/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.9416 - acc: 0.7705 - val_loss: 0.9938 - val_acc: 0.7370\n",
      "Epoch 492/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9419 - acc: 0.7708 - val_loss: 0.9958 - val_acc: 0.7390\n",
      "Epoch 493/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.9412 - acc: 0.7692 - val_loss: 1.0000 - val_acc: 0.7290\n",
      "Epoch 494/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9412 - acc: 0.7697 - val_loss: 0.9983 - val_acc: 0.7330\n",
      "Epoch 495/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9405 - acc: 0.7692 - val_loss: 1.0069 - val_acc: 0.7250\n",
      "Epoch 496/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9414 - acc: 0.7709 - val_loss: 0.9951 - val_acc: 0.7310\n",
      "Epoch 497/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.9405 - acc: 0.7675 - val_loss: 0.9934 - val_acc: 0.7370\n",
      "Epoch 498/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9406 - acc: 0.7685 - val_loss: 0.9893 - val_acc: 0.7320\n",
      "Epoch 499/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9393 - acc: 0.7723 - val_loss: 0.9924 - val_acc: 0.7360\n",
      "Epoch 500/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9391 - acc: 0.7711 - val_loss: 0.9916 - val_acc: 0.7320\n",
      "Epoch 501/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.9391 - acc: 0.7694 - val_loss: 0.9923 - val_acc: 0.7340\n",
      "Epoch 502/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.9383 - acc: 0.7692 - val_loss: 0.9914 - val_acc: 0.7330\n",
      "Epoch 503/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9374 - acc: 0.7714 - val_loss: 0.9917 - val_acc: 0.7300\n",
      "Epoch 504/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9371 - acc: 0.7717 - val_loss: 0.9898 - val_acc: 0.7300\n",
      "Epoch 505/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9372 - acc: 0.7738 - val_loss: 0.9903 - val_acc: 0.7330\n",
      "Epoch 506/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9368 - acc: 0.7718 - val_loss: 1.0023 - val_acc: 0.7270\n",
      "Epoch 507/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9375 - acc: 0.7706 - val_loss: 0.9886 - val_acc: 0.7270\n",
      "Epoch 508/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9361 - acc: 0.7732 - val_loss: 0.9887 - val_acc: 0.7340\n",
      "Epoch 509/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9360 - acc: 0.7734 - val_loss: 0.9892 - val_acc: 0.7440\n",
      "Epoch 510/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9362 - acc: 0.7715 - val_loss: 0.9944 - val_acc: 0.7360\n",
      "Epoch 511/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9356 - acc: 0.7706 - val_loss: 0.9941 - val_acc: 0.7360\n",
      "Epoch 512/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9355 - acc: 0.7720 - val_loss: 0.9912 - val_acc: 0.7310\n",
      "Epoch 513/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9353 - acc: 0.7731 - val_loss: 0.9870 - val_acc: 0.7350\n",
      "Epoch 514/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9346 - acc: 0.7718 - val_loss: 0.9908 - val_acc: 0.7330\n",
      "Epoch 515/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.9350 - acc: 0.7723 - val_loss: 1.0015 - val_acc: 0.7240\n",
      "Epoch 516/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9350 - acc: 0.7743 - val_loss: 0.9907 - val_acc: 0.7400\n",
      "Epoch 517/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9334 - acc: 0.7714 - val_loss: 0.9864 - val_acc: 0.7330\n",
      "Epoch 518/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.9337 - acc: 0.7722 - val_loss: 0.9929 - val_acc: 0.7390\n",
      "Epoch 519/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9335 - acc: 0.7737 - val_loss: 0.9894 - val_acc: 0.7330\n",
      "Epoch 520/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9328 - acc: 0.7725 - val_loss: 0.9921 - val_acc: 0.7260\n",
      "Epoch 521/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9329 - acc: 0.7708 - val_loss: 0.9949 - val_acc: 0.7280\n",
      "Epoch 522/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9328 - acc: 0.7709 - val_loss: 0.9870 - val_acc: 0.7380\n",
      "Epoch 523/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9325 - acc: 0.7749 - val_loss: 0.9873 - val_acc: 0.7330\n",
      "Epoch 524/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9320 - acc: 0.7715 - val_loss: 0.9876 - val_acc: 0.7370\n",
      "Epoch 525/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9313 - acc: 0.7738 - val_loss: 0.9908 - val_acc: 0.7280\n",
      "Epoch 526/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9315 - acc: 0.7760 - val_loss: 0.9914 - val_acc: 0.7360\n",
      "Epoch 527/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9315 - acc: 0.7734 - val_loss: 0.9904 - val_acc: 0.7280\n",
      "Epoch 528/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9310 - acc: 0.7718 - val_loss: 0.9860 - val_acc: 0.7340\n",
      "Epoch 529/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9309 - acc: 0.7748 - val_loss: 0.9879 - val_acc: 0.7300\n",
      "Epoch 530/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9302 - acc: 0.7742 - val_loss: 0.9928 - val_acc: 0.7320\n",
      "Epoch 531/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9298 - acc: 0.7755 - val_loss: 0.9894 - val_acc: 0.7320\n",
      "Epoch 532/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9298 - acc: 0.7757 - val_loss: 0.9895 - val_acc: 0.7360\n",
      "Epoch 533/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9285 - acc: 0.7749 - val_loss: 0.9857 - val_acc: 0.7360\n",
      "Epoch 534/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9285 - acc: 0.7738 - val_loss: 0.9903 - val_acc: 0.7350\n",
      "Epoch 535/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9286 - acc: 0.7754 - val_loss: 0.9892 - val_acc: 0.7330\n",
      "Epoch 536/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9289 - acc: 0.7752 - val_loss: 0.9833 - val_acc: 0.7380\n",
      "Epoch 537/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9277 - acc: 0.7737 - val_loss: 0.9830 - val_acc: 0.7350\n",
      "Epoch 538/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.9270 - acc: 0.7749 - val_loss: 0.9868 - val_acc: 0.7380\n",
      "Epoch 539/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.9284 - acc: 0.7758 - val_loss: 0.9871 - val_acc: 0.7370\n",
      "Epoch 540/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9281 - acc: 0.7738 - val_loss: 0.9855 - val_acc: 0.7390\n",
      "Epoch 541/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9270 - acc: 0.7771 - val_loss: 0.9852 - val_acc: 0.7370\n",
      "Epoch 542/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.9275 - acc: 0.7743 - val_loss: 0.9847 - val_acc: 0.7340\n",
      "Epoch 543/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.9255 - acc: 0.7754 - val_loss: 0.9874 - val_acc: 0.7330\n",
      "Epoch 544/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9267 - acc: 0.7757 - val_loss: 0.9835 - val_acc: 0.7360\n",
      "Epoch 545/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9264 - acc: 0.7758 - val_loss: 0.9845 - val_acc: 0.7340\n",
      "Epoch 546/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9258 - acc: 0.7754 - val_loss: 0.9888 - val_acc: 0.7340\n",
      "Epoch 547/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9255 - acc: 0.7763 - val_loss: 0.9846 - val_acc: 0.7350\n",
      "Epoch 548/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9254 - acc: 0.7748 - val_loss: 0.9845 - val_acc: 0.7400\n",
      "Epoch 549/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.9249 - acc: 0.7772 - val_loss: 0.9816 - val_acc: 0.7400\n",
      "Epoch 550/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9242 - acc: 0.7749 - val_loss: 0.9831 - val_acc: 0.7370\n",
      "Epoch 551/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.9238 - acc: 0.7780 - val_loss: 0.9886 - val_acc: 0.7380\n",
      "Epoch 552/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9245 - acc: 0.7752 - val_loss: 0.9981 - val_acc: 0.7320\n",
      "Epoch 553/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9243 - acc: 0.7771 - val_loss: 0.9817 - val_acc: 0.7410\n",
      "Epoch 554/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.9227 - acc: 0.7780 - val_loss: 0.9842 - val_acc: 0.7360\n",
      "Epoch 555/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.9233 - acc: 0.7780 - val_loss: 0.9822 - val_acc: 0.7390\n",
      "Epoch 556/1000\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 0.9229 - acc: 0.7774 - val_loss: 0.9821 - val_acc: 0.7320\n",
      "Epoch 557/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9226 - acc: 0.7757 - val_loss: 0.9813 - val_acc: 0.7330\n",
      "Epoch 558/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9226 - acc: 0.7785 - val_loss: 0.9806 - val_acc: 0.7360\n",
      "Epoch 559/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.9218 - acc: 0.7792 - val_loss: 0.9931 - val_acc: 0.7320\n",
      "Epoch 560/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9225 - acc: 0.7772 - val_loss: 0.9886 - val_acc: 0.7370\n",
      "Epoch 561/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9219 - acc: 0.7771 - val_loss: 0.9845 - val_acc: 0.7400\n",
      "Epoch 562/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9219 - acc: 0.7783 - val_loss: 0.9827 - val_acc: 0.7410\n",
      "Epoch 563/1000\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 0.9211 - acc: 0.7769 - val_loss: 0.9841 - val_acc: 0.7370\n",
      "Epoch 564/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9205 - acc: 0.7780 - val_loss: 0.9802 - val_acc: 0.7380\n",
      "Epoch 565/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.9198 - acc: 0.7783 - val_loss: 0.9809 - val_acc: 0.7340\n",
      "Epoch 566/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.9205 - acc: 0.7794 - val_loss: 0.9843 - val_acc: 0.7350\n",
      "Epoch 567/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.9200 - acc: 0.7768 - val_loss: 0.9825 - val_acc: 0.7370\n",
      "Epoch 568/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.9207 - acc: 0.7808 - val_loss: 0.9852 - val_acc: 0.7340\n",
      "Epoch 569/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9203 - acc: 0.7789 - val_loss: 0.9829 - val_acc: 0.7360\n",
      "Epoch 570/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9193 - acc: 0.7798 - val_loss: 0.9809 - val_acc: 0.7420\n",
      "Epoch 571/1000\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 0.9186 - acc: 0.7775 - val_loss: 0.9838 - val_acc: 0.7340\n",
      "Epoch 572/1000\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 0.9179 - acc: 0.7789 - val_loss: 0.9803 - val_acc: 0.7390\n",
      "Epoch 573/1000\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 0.9187 - acc: 0.7788 - val_loss: 0.9841 - val_acc: 0.7330\n",
      "Epoch 574/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.9187 - acc: 0.7785 - val_loss: 0.9761 - val_acc: 0.7380\n",
      "Epoch 575/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.9184 - acc: 0.7805 - val_loss: 0.9849 - val_acc: 0.7370\n",
      "Epoch 576/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9176 - acc: 0.7789 - val_loss: 0.9807 - val_acc: 0.7320\n",
      "Epoch 577/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9178 - acc: 0.7815 - val_loss: 0.9802 - val_acc: 0.7400\n",
      "Epoch 578/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9173 - acc: 0.7751 - val_loss: 0.9767 - val_acc: 0.7340\n",
      "Epoch 579/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.9169 - acc: 0.7783 - val_loss: 0.9795 - val_acc: 0.7410\n",
      "Epoch 580/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9177 - acc: 0.7802 - val_loss: 0.9771 - val_acc: 0.7370\n",
      "Epoch 581/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9158 - acc: 0.7806 - val_loss: 0.9786 - val_acc: 0.7380\n",
      "Epoch 582/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.9170 - acc: 0.7803 - val_loss: 0.9789 - val_acc: 0.7370\n",
      "Epoch 583/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9162 - acc: 0.7791 - val_loss: 0.9790 - val_acc: 0.7380\n",
      "Epoch 584/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9163 - acc: 0.7780 - val_loss: 0.9845 - val_acc: 0.7380\n",
      "Epoch 585/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9167 - acc: 0.7805 - val_loss: 0.9786 - val_acc: 0.7360\n",
      "Epoch 586/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9161 - acc: 0.7820 - val_loss: 0.9795 - val_acc: 0.7340\n",
      "Epoch 587/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.9146 - acc: 0.7806 - val_loss: 0.9806 - val_acc: 0.7390\n",
      "Epoch 588/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9151 - acc: 0.7805 - val_loss: 0.9801 - val_acc: 0.7310\n",
      "Epoch 589/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9149 - acc: 0.7806 - val_loss: 0.9758 - val_acc: 0.7430\n",
      "Epoch 590/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9145 - acc: 0.7811 - val_loss: 0.9779 - val_acc: 0.7420\n",
      "Epoch 591/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9138 - acc: 0.7809 - val_loss: 0.9833 - val_acc: 0.7330\n",
      "Epoch 592/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9144 - acc: 0.7808 - val_loss: 0.9796 - val_acc: 0.7360\n",
      "Epoch 593/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.9139 - acc: 0.7794 - val_loss: 0.9804 - val_acc: 0.7380\n",
      "Epoch 594/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9136 - acc: 0.7814 - val_loss: 0.9826 - val_acc: 0.7360\n",
      "Epoch 595/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9132 - acc: 0.7795 - val_loss: 0.9791 - val_acc: 0.7340\n",
      "Epoch 596/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.9131 - acc: 0.7809 - val_loss: 0.9797 - val_acc: 0.7400\n",
      "Epoch 597/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.9121 - acc: 0.7795 - val_loss: 0.9862 - val_acc: 0.7380\n",
      "Epoch 598/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9133 - acc: 0.7834 - val_loss: 0.9750 - val_acc: 0.7380\n",
      "Epoch 599/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.9115 - acc: 0.7829 - val_loss: 0.9747 - val_acc: 0.7430\n",
      "Epoch 600/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9117 - acc: 0.7803 - val_loss: 0.9736 - val_acc: 0.7360\n",
      "Epoch 601/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9114 - acc: 0.7822 - val_loss: 0.9819 - val_acc: 0.7360\n",
      "Epoch 602/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9116 - acc: 0.7832 - val_loss: 0.9768 - val_acc: 0.7410\n",
      "Epoch 603/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9114 - acc: 0.7838 - val_loss: 0.9803 - val_acc: 0.7380\n",
      "Epoch 604/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9113 - acc: 0.7840 - val_loss: 0.9776 - val_acc: 0.7360\n",
      "Epoch 605/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9113 - acc: 0.7829 - val_loss: 0.9819 - val_acc: 0.7350\n",
      "Epoch 606/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9108 - acc: 0.7809 - val_loss: 0.9828 - val_acc: 0.7440\n",
      "Epoch 607/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9113 - acc: 0.7828 - val_loss: 0.9747 - val_acc: 0.7400\n",
      "Epoch 608/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.9101 - acc: 0.7815 - val_loss: 0.9753 - val_acc: 0.7450\n",
      "Epoch 609/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9095 - acc: 0.7826 - val_loss: 0.9745 - val_acc: 0.7420\n",
      "Epoch 610/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.9100 - acc: 0.7805 - val_loss: 0.9755 - val_acc: 0.7380\n",
      "Epoch 611/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9087 - acc: 0.7829 - val_loss: 0.9736 - val_acc: 0.7350\n",
      "Epoch 612/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.9094 - acc: 0.7811 - val_loss: 0.9738 - val_acc: 0.7400\n",
      "Epoch 613/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9079 - acc: 0.7860 - val_loss: 0.9855 - val_acc: 0.7340\n",
      "Epoch 614/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9086 - acc: 0.7812 - val_loss: 0.9742 - val_acc: 0.7390\n",
      "Epoch 615/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9082 - acc: 0.7837 - val_loss: 0.9888 - val_acc: 0.7290\n",
      "Epoch 616/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.9095 - acc: 0.7818 - val_loss: 0.9840 - val_acc: 0.7360\n",
      "Epoch 617/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.9092 - acc: 0.7825 - val_loss: 0.9775 - val_acc: 0.7480\n",
      "Epoch 618/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9083 - acc: 0.7828 - val_loss: 0.9735 - val_acc: 0.7420\n",
      "Epoch 619/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9075 - acc: 0.7843 - val_loss: 0.9722 - val_acc: 0.7430\n",
      "Epoch 620/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9080 - acc: 0.7829 - val_loss: 0.9860 - val_acc: 0.7300\n",
      "Epoch 621/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9075 - acc: 0.7823 - val_loss: 0.9731 - val_acc: 0.7370\n",
      "Epoch 622/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9060 - acc: 0.7872 - val_loss: 0.9786 - val_acc: 0.7370\n",
      "Epoch 623/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9079 - acc: 0.7851 - val_loss: 0.9777 - val_acc: 0.7390\n",
      "Epoch 624/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9061 - acc: 0.7863 - val_loss: 0.9738 - val_acc: 0.7420\n",
      "Epoch 625/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9058 - acc: 0.7863 - val_loss: 0.9771 - val_acc: 0.7330\n",
      "Epoch 626/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.9069 - acc: 0.7874 - val_loss: 0.9770 - val_acc: 0.7400\n",
      "Epoch 627/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9052 - acc: 0.7875 - val_loss: 0.9826 - val_acc: 0.7340\n",
      "Epoch 628/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9053 - acc: 0.7883 - val_loss: 0.9754 - val_acc: 0.7400\n",
      "Epoch 629/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.9049 - acc: 0.7846 - val_loss: 0.9786 - val_acc: 0.7400\n",
      "Epoch 630/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9048 - acc: 0.7835 - val_loss: 0.9791 - val_acc: 0.7420\n",
      "Epoch 631/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.9043 - acc: 0.7860 - val_loss: 0.9745 - val_acc: 0.7370\n",
      "Epoch 632/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.9043 - acc: 0.7846 - val_loss: 0.9869 - val_acc: 0.7340\n",
      "Epoch 633/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9048 - acc: 0.7831 - val_loss: 0.9813 - val_acc: 0.7370\n",
      "Epoch 634/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.9040 - acc: 0.7851 - val_loss: 0.9722 - val_acc: 0.7400\n",
      "Epoch 635/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9032 - acc: 0.7883 - val_loss: 0.9726 - val_acc: 0.7380\n",
      "Epoch 636/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9033 - acc: 0.7894 - val_loss: 0.9735 - val_acc: 0.7400\n",
      "Epoch 637/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.9033 - acc: 0.7860 - val_loss: 0.9741 - val_acc: 0.7400\n",
      "Epoch 638/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9037 - acc: 0.7860 - val_loss: 0.9748 - val_acc: 0.7420\n",
      "Epoch 639/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9031 - acc: 0.7857 - val_loss: 0.9752 - val_acc: 0.7410\n",
      "Epoch 640/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.9033 - acc: 0.7863 - val_loss: 0.9711 - val_acc: 0.7390\n",
      "Epoch 641/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9027 - acc: 0.7882 - val_loss: 0.9717 - val_acc: 0.7460\n",
      "Epoch 642/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.9016 - acc: 0.7886 - val_loss: 0.9736 - val_acc: 0.7350\n",
      "Epoch 643/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.9032 - acc: 0.7871 - val_loss: 0.9757 - val_acc: 0.7390\n",
      "Epoch 644/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9017 - acc: 0.7865 - val_loss: 0.9745 - val_acc: 0.7430\n",
      "Epoch 645/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9012 - acc: 0.7857 - val_loss: 0.9894 - val_acc: 0.7340\n",
      "Epoch 646/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9016 - acc: 0.7865 - val_loss: 0.9814 - val_acc: 0.7400\n",
      "Epoch 647/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9021 - acc: 0.7880 - val_loss: 0.9736 - val_acc: 0.7410\n",
      "Epoch 648/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9008 - acc: 0.7866 - val_loss: 0.9731 - val_acc: 0.7410\n",
      "Epoch 649/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.9013 - acc: 0.7854 - val_loss: 0.9710 - val_acc: 0.7380\n",
      "Epoch 650/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9008 - acc: 0.7858 - val_loss: 0.9696 - val_acc: 0.7400\n",
      "Epoch 651/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9007 - acc: 0.7886 - val_loss: 0.9736 - val_acc: 0.7410\n",
      "Epoch 652/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9007 - acc: 0.7869 - val_loss: 0.9796 - val_acc: 0.7350\n",
      "Epoch 653/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9000 - acc: 0.7869 - val_loss: 0.9694 - val_acc: 0.7400\n",
      "Epoch 654/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9003 - acc: 0.7892 - val_loss: 0.9807 - val_acc: 0.7460\n",
      "Epoch 655/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9004 - acc: 0.7877 - val_loss: 0.9699 - val_acc: 0.7410\n",
      "Epoch 656/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8990 - acc: 0.7888 - val_loss: 0.9745 - val_acc: 0.7390\n",
      "Epoch 657/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8990 - acc: 0.7894 - val_loss: 0.9714 - val_acc: 0.7420\n",
      "Epoch 658/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8987 - acc: 0.7874 - val_loss: 0.9804 - val_acc: 0.7340\n",
      "Epoch 659/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8990 - acc: 0.7891 - val_loss: 0.9689 - val_acc: 0.7420\n",
      "Epoch 660/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8987 - acc: 0.7874 - val_loss: 0.9694 - val_acc: 0.7410\n",
      "Epoch 661/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8976 - acc: 0.7866 - val_loss: 0.9701 - val_acc: 0.7350\n",
      "Epoch 662/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8985 - acc: 0.7872 - val_loss: 0.9695 - val_acc: 0.7390\n",
      "Epoch 663/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8985 - acc: 0.7888 - val_loss: 0.9693 - val_acc: 0.7430\n",
      "Epoch 664/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8975 - acc: 0.7892 - val_loss: 0.9707 - val_acc: 0.7430\n",
      "Epoch 665/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8977 - acc: 0.7915 - val_loss: 0.9679 - val_acc: 0.7400\n",
      "Epoch 666/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8967 - acc: 0.7886 - val_loss: 0.9666 - val_acc: 0.7420\n",
      "Epoch 667/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8970 - acc: 0.7905 - val_loss: 0.9738 - val_acc: 0.7410\n",
      "Epoch 668/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8960 - acc: 0.7900 - val_loss: 0.9758 - val_acc: 0.7390\n",
      "Epoch 669/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8966 - acc: 0.7894 - val_loss: 0.9757 - val_acc: 0.7350\n",
      "Epoch 670/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8973 - acc: 0.7909 - val_loss: 0.9702 - val_acc: 0.7460\n",
      "Epoch 671/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8967 - acc: 0.7898 - val_loss: 0.9717 - val_acc: 0.7440\n",
      "Epoch 672/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8950 - acc: 0.7894 - val_loss: 0.9711 - val_acc: 0.7390\n",
      "Epoch 673/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8948 - acc: 0.7909 - val_loss: 0.9705 - val_acc: 0.7430\n",
      "Epoch 674/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8957 - acc: 0.7918 - val_loss: 0.9700 - val_acc: 0.7430\n",
      "Epoch 675/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8944 - acc: 0.7918 - val_loss: 0.9743 - val_acc: 0.7350\n",
      "Epoch 676/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8957 - acc: 0.7917 - val_loss: 0.9674 - val_acc: 0.7440\n",
      "Epoch 677/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8942 - acc: 0.7915 - val_loss: 0.9684 - val_acc: 0.7470\n",
      "Epoch 678/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.8941 - acc: 0.7922 - val_loss: 0.9697 - val_acc: 0.7370\n",
      "Epoch 679/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8938 - acc: 0.7958 - val_loss: 0.9709 - val_acc: 0.7380\n",
      "Epoch 680/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.8940 - acc: 0.7894 - val_loss: 0.9713 - val_acc: 0.7440\n",
      "Epoch 681/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8937 - acc: 0.7903 - val_loss: 0.9676 - val_acc: 0.7450\n",
      "Epoch 682/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.8932 - acc: 0.7906 - val_loss: 0.9707 - val_acc: 0.7430\n",
      "Epoch 683/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8938 - acc: 0.7937 - val_loss: 0.9729 - val_acc: 0.7420\n",
      "Epoch 684/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8944 - acc: 0.7931 - val_loss: 0.9718 - val_acc: 0.7400\n",
      "Epoch 685/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8937 - acc: 0.7923 - val_loss: 0.9707 - val_acc: 0.7450\n",
      "Epoch 686/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8941 - acc: 0.7895 - val_loss: 0.9698 - val_acc: 0.7380\n",
      "Epoch 687/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8923 - acc: 0.7912 - val_loss: 0.9695 - val_acc: 0.7380\n",
      "Epoch 688/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8933 - acc: 0.7918 - val_loss: 0.9697 - val_acc: 0.7390\n",
      "Epoch 689/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8935 - acc: 0.7940 - val_loss: 0.9766 - val_acc: 0.7420\n",
      "Epoch 690/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8925 - acc: 0.7929 - val_loss: 0.9684 - val_acc: 0.7460\n",
      "Epoch 691/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8911 - acc: 0.7923 - val_loss: 0.9743 - val_acc: 0.7430\n",
      "Epoch 692/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8922 - acc: 0.7920 - val_loss: 0.9667 - val_acc: 0.7440\n",
      "Epoch 693/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8904 - acc: 0.7914 - val_loss: 0.9744 - val_acc: 0.7450\n",
      "Epoch 694/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8912 - acc: 0.7909 - val_loss: 0.9651 - val_acc: 0.7480\n",
      "Epoch 695/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8918 - acc: 0.7943 - val_loss: 0.9647 - val_acc: 0.7440\n",
      "Epoch 696/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8904 - acc: 0.7932 - val_loss: 0.9755 - val_acc: 0.7420\n",
      "Epoch 697/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8905 - acc: 0.7934 - val_loss: 0.9706 - val_acc: 0.7450\n",
      "Epoch 698/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8906 - acc: 0.7922 - val_loss: 0.9651 - val_acc: 0.7420\n",
      "Epoch 699/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8900 - acc: 0.7932 - val_loss: 0.9654 - val_acc: 0.7460\n",
      "Epoch 700/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8911 - acc: 0.7935 - val_loss: 0.9656 - val_acc: 0.7420\n",
      "Epoch 701/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8891 - acc: 0.7942 - val_loss: 0.9681 - val_acc: 0.7470\n",
      "Epoch 702/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8888 - acc: 0.7938 - val_loss: 0.9745 - val_acc: 0.7450\n",
      "Epoch 703/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8895 - acc: 0.7943 - val_loss: 0.9690 - val_acc: 0.7400\n",
      "Epoch 704/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8895 - acc: 0.7920 - val_loss: 0.9737 - val_acc: 0.7420\n",
      "Epoch 705/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8890 - acc: 0.7955 - val_loss: 0.9700 - val_acc: 0.7420\n",
      "Epoch 706/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8881 - acc: 0.7940 - val_loss: 0.9716 - val_acc: 0.7430\n",
      "Epoch 707/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8882 - acc: 0.7949 - val_loss: 0.9717 - val_acc: 0.7480\n",
      "Epoch 708/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8885 - acc: 0.7951 - val_loss: 0.9661 - val_acc: 0.7460\n",
      "Epoch 709/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8878 - acc: 0.7938 - val_loss: 0.9721 - val_acc: 0.7460\n",
      "Epoch 710/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8890 - acc: 0.7929 - val_loss: 0.9634 - val_acc: 0.7520\n",
      "Epoch 711/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8864 - acc: 0.7972 - val_loss: 0.9729 - val_acc: 0.7500\n",
      "Epoch 712/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8875 - acc: 0.7952 - val_loss: 0.9731 - val_acc: 0.7410\n",
      "Epoch 713/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8879 - acc: 0.7934 - val_loss: 0.9643 - val_acc: 0.7440\n",
      "Epoch 714/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8869 - acc: 0.7945 - val_loss: 0.9710 - val_acc: 0.7420\n",
      "Epoch 715/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8866 - acc: 0.7934 - val_loss: 0.9647 - val_acc: 0.7460\n",
      "Epoch 716/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8866 - acc: 0.7980 - val_loss: 0.9735 - val_acc: 0.7480\n",
      "Epoch 717/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8885 - acc: 0.7971 - val_loss: 0.9643 - val_acc: 0.7440\n",
      "Epoch 718/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8866 - acc: 0.7965 - val_loss: 0.9831 - val_acc: 0.7350\n",
      "Epoch 719/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8879 - acc: 0.7951 - val_loss: 0.9687 - val_acc: 0.7420\n",
      "Epoch 720/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.8857 - acc: 0.7951 - val_loss: 0.9740 - val_acc: 0.7460\n",
      "Epoch 721/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8862 - acc: 0.7948 - val_loss: 0.9677 - val_acc: 0.7400\n",
      "Epoch 722/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8862 - acc: 0.7978 - val_loss: 0.9693 - val_acc: 0.7400\n",
      "Epoch 723/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8855 - acc: 0.7974 - val_loss: 0.9728 - val_acc: 0.7390\n",
      "Epoch 724/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8854 - acc: 0.7948 - val_loss: 0.9660 - val_acc: 0.7400\n",
      "Epoch 725/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8850 - acc: 0.7962 - val_loss: 0.9686 - val_acc: 0.7420\n",
      "Epoch 726/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8860 - acc: 0.7943 - val_loss: 0.9703 - val_acc: 0.7440\n",
      "Epoch 727/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8845 - acc: 0.7968 - val_loss: 0.9635 - val_acc: 0.7420\n",
      "Epoch 728/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8836 - acc: 0.7972 - val_loss: 0.9810 - val_acc: 0.7420\n",
      "Epoch 729/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8852 - acc: 0.7965 - val_loss: 0.9753 - val_acc: 0.7390\n",
      "Epoch 730/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8843 - acc: 0.7954 - val_loss: 0.9750 - val_acc: 0.7480\n",
      "Epoch 731/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8843 - acc: 0.7946 - val_loss: 0.9688 - val_acc: 0.7450\n",
      "Epoch 732/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8828 - acc: 0.7972 - val_loss: 0.9618 - val_acc: 0.7480\n",
      "Epoch 733/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8831 - acc: 0.7986 - val_loss: 0.9632 - val_acc: 0.7440\n",
      "Epoch 734/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8830 - acc: 0.7980 - val_loss: 0.9656 - val_acc: 0.7430\n",
      "Epoch 735/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8834 - acc: 0.7962 - val_loss: 0.9611 - val_acc: 0.7470\n",
      "Epoch 736/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8814 - acc: 0.8006 - val_loss: 0.9665 - val_acc: 0.7470\n",
      "Epoch 737/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8822 - acc: 0.7982 - val_loss: 0.9671 - val_acc: 0.7430\n",
      "Epoch 738/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8818 - acc: 0.7969 - val_loss: 0.9641 - val_acc: 0.7470\n",
      "Epoch 739/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8837 - acc: 0.7954 - val_loss: 0.9609 - val_acc: 0.7460\n",
      "Epoch 740/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8826 - acc: 0.7983 - val_loss: 0.9711 - val_acc: 0.7450\n",
      "Epoch 741/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8812 - acc: 0.7983 - val_loss: 0.9731 - val_acc: 0.7410\n",
      "Epoch 742/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8806 - acc: 0.8009 - val_loss: 0.9653 - val_acc: 0.7460\n",
      "Epoch 743/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8813 - acc: 0.7986 - val_loss: 0.9724 - val_acc: 0.7370\n",
      "Epoch 744/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8818 - acc: 0.7982 - val_loss: 0.9707 - val_acc: 0.7410\n",
      "Epoch 745/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8810 - acc: 0.7957 - val_loss: 0.9648 - val_acc: 0.7440\n",
      "Epoch 746/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8807 - acc: 0.7978 - val_loss: 0.9655 - val_acc: 0.7420\n",
      "Epoch 747/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8812 - acc: 0.8011 - val_loss: 0.9590 - val_acc: 0.7500\n",
      "Epoch 748/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8796 - acc: 0.7991 - val_loss: 0.9601 - val_acc: 0.7500\n",
      "Epoch 749/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8802 - acc: 0.8011 - val_loss: 0.9760 - val_acc: 0.7400\n",
      "Epoch 750/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8807 - acc: 0.7997 - val_loss: 0.9694 - val_acc: 0.7390\n",
      "Epoch 751/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8817 - acc: 0.7988 - val_loss: 0.9712 - val_acc: 0.7400\n",
      "Epoch 752/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8782 - acc: 0.7986 - val_loss: 0.9644 - val_acc: 0.7440\n",
      "Epoch 753/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8789 - acc: 0.7985 - val_loss: 0.9576 - val_acc: 0.7560\n",
      "Epoch 754/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8786 - acc: 0.7998 - val_loss: 0.9673 - val_acc: 0.7480\n",
      "Epoch 755/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8795 - acc: 0.7989 - val_loss: 0.9625 - val_acc: 0.7450\n",
      "Epoch 756/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8801 - acc: 0.7998 - val_loss: 0.9707 - val_acc: 0.7420\n",
      "Epoch 757/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8794 - acc: 0.7995 - val_loss: 0.9627 - val_acc: 0.7460\n",
      "Epoch 758/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8788 - acc: 0.8000 - val_loss: 0.9627 - val_acc: 0.7390\n",
      "Epoch 759/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8787 - acc: 0.7994 - val_loss: 0.9607 - val_acc: 0.7450\n",
      "Epoch 760/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8779 - acc: 0.8009 - val_loss: 0.9601 - val_acc: 0.7490\n",
      "Epoch 761/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8775 - acc: 0.7997 - val_loss: 0.9635 - val_acc: 0.7430\n",
      "Epoch 762/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8774 - acc: 0.8006 - val_loss: 0.9690 - val_acc: 0.7430\n",
      "Epoch 763/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8789 - acc: 0.7975 - val_loss: 0.9680 - val_acc: 0.7400\n",
      "Epoch 764/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8782 - acc: 0.7982 - val_loss: 0.9631 - val_acc: 0.7550\n",
      "Epoch 765/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8784 - acc: 0.8017 - val_loss: 0.9742 - val_acc: 0.7490\n",
      "Epoch 766/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8774 - acc: 0.8020 - val_loss: 0.9617 - val_acc: 0.7450\n",
      "Epoch 767/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8768 - acc: 0.8026 - val_loss: 0.9645 - val_acc: 0.7390\n",
      "Epoch 768/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8784 - acc: 0.8003 - val_loss: 0.9686 - val_acc: 0.7420\n",
      "Epoch 769/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8766 - acc: 0.8023 - val_loss: 0.9641 - val_acc: 0.7480\n",
      "Epoch 770/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8768 - acc: 0.8003 - val_loss: 0.9692 - val_acc: 0.7350\n",
      "Epoch 771/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8757 - acc: 0.8009 - val_loss: 0.9830 - val_acc: 0.7470\n",
      "Epoch 772/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8769 - acc: 0.7997 - val_loss: 0.9881 - val_acc: 0.7260\n",
      "Epoch 773/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8760 - acc: 0.8011 - val_loss: 0.9593 - val_acc: 0.7450\n",
      "Epoch 774/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8730 - acc: 0.8035 - val_loss: 0.9689 - val_acc: 0.7440\n",
      "Epoch 775/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8751 - acc: 0.8025 - val_loss: 0.9654 - val_acc: 0.7390\n",
      "Epoch 776/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8743 - acc: 0.8003 - val_loss: 0.9738 - val_acc: 0.7460\n",
      "Epoch 777/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8737 - acc: 0.8011 - val_loss: 0.9658 - val_acc: 0.7440\n",
      "Epoch 778/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8756 - acc: 0.8015 - val_loss: 0.9623 - val_acc: 0.7420\n",
      "Epoch 779/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8744 - acc: 0.8025 - val_loss: 0.9685 - val_acc: 0.7400\n",
      "Epoch 780/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8755 - acc: 0.8018 - val_loss: 0.9595 - val_acc: 0.7450\n",
      "Epoch 781/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8752 - acc: 0.8017 - val_loss: 0.9613 - val_acc: 0.7500\n",
      "Epoch 782/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8753 - acc: 0.8035 - val_loss: 0.9713 - val_acc: 0.7440\n",
      "Epoch 783/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8740 - acc: 0.8032 - val_loss: 0.9595 - val_acc: 0.7490\n",
      "Epoch 784/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8740 - acc: 0.8032 - val_loss: 0.9659 - val_acc: 0.7390\n",
      "Epoch 785/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8734 - acc: 0.8018 - val_loss: 0.9625 - val_acc: 0.7450\n",
      "Epoch 786/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8734 - acc: 0.8018 - val_loss: 0.9662 - val_acc: 0.7460\n",
      "Epoch 787/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8754 - acc: 0.7977 - val_loss: 0.9634 - val_acc: 0.7440\n",
      "Epoch 788/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8734 - acc: 0.8037 - val_loss: 0.9591 - val_acc: 0.7560\n",
      "Epoch 789/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8720 - acc: 0.8045 - val_loss: 0.9578 - val_acc: 0.7470\n",
      "Epoch 790/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8724 - acc: 0.8025 - val_loss: 0.9751 - val_acc: 0.7500\n",
      "Epoch 791/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8738 - acc: 0.8034 - val_loss: 0.9562 - val_acc: 0.7510\n",
      "Epoch 792/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8715 - acc: 0.8045 - val_loss: 0.9600 - val_acc: 0.7470\n",
      "Epoch 793/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8702 - acc: 0.8043 - val_loss: 0.9729 - val_acc: 0.7440\n",
      "Epoch 794/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8724 - acc: 0.8060 - val_loss: 0.9614 - val_acc: 0.7440\n",
      "Epoch 795/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8727 - acc: 0.8034 - val_loss: 0.9603 - val_acc: 0.7490\n",
      "Epoch 796/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8731 - acc: 0.8058 - val_loss: 0.9588 - val_acc: 0.7420\n",
      "Epoch 797/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8706 - acc: 0.8034 - val_loss: 0.9590 - val_acc: 0.7500\n",
      "Epoch 798/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8703 - acc: 0.8026 - val_loss: 0.9655 - val_acc: 0.7430\n",
      "Epoch 799/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8715 - acc: 0.8063 - val_loss: 0.9618 - val_acc: 0.7490\n",
      "Epoch 800/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8711 - acc: 0.8074 - val_loss: 0.9540 - val_acc: 0.7590\n",
      "Epoch 801/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8715 - acc: 0.8055 - val_loss: 0.9630 - val_acc: 0.7510\n",
      "Epoch 802/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8710 - acc: 0.8040 - val_loss: 0.9618 - val_acc: 0.7460\n",
      "Epoch 803/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8699 - acc: 0.8042 - val_loss: 0.9682 - val_acc: 0.7460\n",
      "Epoch 804/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8698 - acc: 0.8052 - val_loss: 0.9577 - val_acc: 0.7510\n",
      "Epoch 805/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8706 - acc: 0.8034 - val_loss: 0.9692 - val_acc: 0.7380\n",
      "Epoch 806/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8696 - acc: 0.8054 - val_loss: 0.9611 - val_acc: 0.7530\n",
      "Epoch 807/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8699 - acc: 0.8055 - val_loss: 0.9599 - val_acc: 0.7390\n",
      "Epoch 808/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8709 - acc: 0.8017 - val_loss: 0.9780 - val_acc: 0.7380\n",
      "Epoch 809/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8700 - acc: 0.8068 - val_loss: 0.9746 - val_acc: 0.7420\n",
      "Epoch 810/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8702 - acc: 0.8049 - val_loss: 0.9742 - val_acc: 0.7380\n",
      "Epoch 811/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8690 - acc: 0.8046 - val_loss: 0.9764 - val_acc: 0.7320\n",
      "Epoch 812/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8682 - acc: 0.8072 - val_loss: 0.9577 - val_acc: 0.7490\n",
      "Epoch 813/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8683 - acc: 0.8085 - val_loss: 0.9683 - val_acc: 0.7390\n",
      "Epoch 814/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8692 - acc: 0.8066 - val_loss: 0.9576 - val_acc: 0.7520\n",
      "Epoch 815/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8669 - acc: 0.8080 - val_loss: 0.9568 - val_acc: 0.7550\n",
      "Epoch 816/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8664 - acc: 0.8077 - val_loss: 0.9694 - val_acc: 0.7360\n",
      "Epoch 817/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8679 - acc: 0.8040 - val_loss: 0.9654 - val_acc: 0.7560\n",
      "Epoch 818/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8674 - acc: 0.8065 - val_loss: 0.9620 - val_acc: 0.7470\n",
      "Epoch 819/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8664 - acc: 0.8060 - val_loss: 0.9640 - val_acc: 0.7390\n",
      "Epoch 820/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8666 - acc: 0.8057 - val_loss: 0.9545 - val_acc: 0.7520\n",
      "Epoch 821/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8673 - acc: 0.8095 - val_loss: 0.9558 - val_acc: 0.7500\n",
      "Epoch 822/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8650 - acc: 0.8086 - val_loss: 0.9584 - val_acc: 0.7540\n",
      "Epoch 823/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8657 - acc: 0.8085 - val_loss: 0.9655 - val_acc: 0.7460\n",
      "Epoch 824/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8653 - acc: 0.8083 - val_loss: 0.9631 - val_acc: 0.7450\n",
      "Epoch 825/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8653 - acc: 0.8060 - val_loss: 0.9679 - val_acc: 0.7430\n",
      "Epoch 826/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8678 - acc: 0.8083 - val_loss: 0.9835 - val_acc: 0.7380\n",
      "Epoch 827/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8679 - acc: 0.8066 - val_loss: 0.9573 - val_acc: 0.7460\n",
      "Epoch 828/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8652 - acc: 0.8072 - val_loss: 0.9597 - val_acc: 0.7570\n",
      "Epoch 829/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8669 - acc: 0.8100 - val_loss: 0.9585 - val_acc: 0.7410\n",
      "Epoch 830/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8640 - acc: 0.8097 - val_loss: 0.9616 - val_acc: 0.7430\n",
      "Epoch 831/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8652 - acc: 0.8092 - val_loss: 0.9629 - val_acc: 0.7520\n",
      "Epoch 832/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8647 - acc: 0.8085 - val_loss: 0.9703 - val_acc: 0.7460\n",
      "Epoch 833/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8650 - acc: 0.8103 - val_loss: 0.9855 - val_acc: 0.7370\n",
      "Epoch 834/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8646 - acc: 0.8095 - val_loss: 0.9643 - val_acc: 0.7480\n",
      "Epoch 835/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8646 - acc: 0.8057 - val_loss: 0.9792 - val_acc: 0.7400\n",
      "Epoch 836/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8638 - acc: 0.8077 - val_loss: 0.9572 - val_acc: 0.7490\n",
      "Epoch 837/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8626 - acc: 0.8074 - val_loss: 0.9741 - val_acc: 0.7410\n",
      "Epoch 838/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8639 - acc: 0.8065 - val_loss: 0.9619 - val_acc: 0.7410\n",
      "Epoch 839/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8641 - acc: 0.8120 - val_loss: 0.9657 - val_acc: 0.7470\n",
      "Epoch 840/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8652 - acc: 0.8097 - val_loss: 0.9569 - val_acc: 0.7440\n",
      "Epoch 841/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8623 - acc: 0.8066 - val_loss: 0.9543 - val_acc: 0.7510\n",
      "Epoch 842/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8629 - acc: 0.8098 - val_loss: 0.9674 - val_acc: 0.7520\n",
      "Epoch 843/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8633 - acc: 0.8098 - val_loss: 0.9639 - val_acc: 0.7380\n",
      "Epoch 844/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8635 - acc: 0.8088 - val_loss: 0.9796 - val_acc: 0.7420\n",
      "Epoch 845/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8630 - acc: 0.8083 - val_loss: 0.9545 - val_acc: 0.7570\n",
      "Epoch 846/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8636 - acc: 0.8082 - val_loss: 0.9612 - val_acc: 0.7470\n",
      "Epoch 847/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8630 - acc: 0.8106 - val_loss: 0.9551 - val_acc: 0.7520\n",
      "Epoch 848/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8621 - acc: 0.8120 - val_loss: 0.9625 - val_acc: 0.7460\n",
      "Epoch 849/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8622 - acc: 0.8097 - val_loss: 0.9617 - val_acc: 0.7480\n",
      "Epoch 850/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8606 - acc: 0.8085 - val_loss: 0.9557 - val_acc: 0.7470\n",
      "Epoch 851/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8620 - acc: 0.8092 - val_loss: 0.9760 - val_acc: 0.7460\n",
      "Epoch 852/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8626 - acc: 0.8114 - val_loss: 0.9542 - val_acc: 0.7590\n",
      "Epoch 853/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8613 - acc: 0.8085 - val_loss: 0.9677 - val_acc: 0.7450\n",
      "Epoch 854/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8603 - acc: 0.8109 - val_loss: 0.9541 - val_acc: 0.7450\n",
      "Epoch 855/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8596 - acc: 0.8120 - val_loss: 0.9643 - val_acc: 0.7400\n",
      "Epoch 856/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8613 - acc: 0.8105 - val_loss: 0.9537 - val_acc: 0.7550\n",
      "Epoch 857/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8628 - acc: 0.8088 - val_loss: 0.9699 - val_acc: 0.7440\n",
      "Epoch 858/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8598 - acc: 0.8138 - val_loss: 0.9523 - val_acc: 0.7570\n",
      "Epoch 859/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8610 - acc: 0.8135 - val_loss: 0.9538 - val_acc: 0.7560\n",
      "Epoch 860/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8597 - acc: 0.8114 - val_loss: 0.9536 - val_acc: 0.7550\n",
      "Epoch 861/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8595 - acc: 0.8112 - val_loss: 0.9528 - val_acc: 0.7480\n",
      "Epoch 862/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8591 - acc: 0.8134 - val_loss: 0.9586 - val_acc: 0.7430\n",
      "Epoch 863/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8579 - acc: 0.8134 - val_loss: 0.9594 - val_acc: 0.7500\n",
      "Epoch 864/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8588 - acc: 0.8098 - val_loss: 0.9705 - val_acc: 0.7560\n",
      "Epoch 865/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8588 - acc: 0.8103 - val_loss: 0.9583 - val_acc: 0.7460\n",
      "Epoch 866/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8563 - acc: 0.8112 - val_loss: 0.9576 - val_acc: 0.7540\n",
      "Epoch 867/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8592 - acc: 0.8146 - val_loss: 0.9537 - val_acc: 0.7410\n",
      "Epoch 868/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8570 - acc: 0.8129 - val_loss: 0.9611 - val_acc: 0.7510\n",
      "Epoch 869/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8580 - acc: 0.8109 - val_loss: 0.9522 - val_acc: 0.7540\n",
      "Epoch 870/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8557 - acc: 0.8129 - val_loss: 0.9567 - val_acc: 0.7480\n",
      "Epoch 871/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8575 - acc: 0.8111 - val_loss: 0.9676 - val_acc: 0.7480\n",
      "Epoch 872/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8586 - acc: 0.8128 - val_loss: 0.9597 - val_acc: 0.7510\n",
      "Epoch 873/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8571 - acc: 0.8152 - val_loss: 0.9638 - val_acc: 0.7370\n",
      "Epoch 874/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8586 - acc: 0.8143 - val_loss: 0.9480 - val_acc: 0.7550\n",
      "Epoch 875/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8561 - acc: 0.8128 - val_loss: 0.9490 - val_acc: 0.7580\n",
      "Epoch 876/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8560 - acc: 0.8128 - val_loss: 0.9599 - val_acc: 0.7390\n",
      "Epoch 877/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8558 - acc: 0.8137 - val_loss: 0.9708 - val_acc: 0.7420\n",
      "Epoch 878/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8563 - acc: 0.8132 - val_loss: 0.9483 - val_acc: 0.7570\n",
      "Epoch 879/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8550 - acc: 0.8152 - val_loss: 0.9687 - val_acc: 0.7410\n",
      "Epoch 880/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8575 - acc: 0.8120 - val_loss: 0.9544 - val_acc: 0.7610\n",
      "Epoch 881/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8556 - acc: 0.8112 - val_loss: 0.9559 - val_acc: 0.7450\n",
      "Epoch 882/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8562 - acc: 0.8126 - val_loss: 0.9564 - val_acc: 0.7570\n",
      "Epoch 883/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8553 - acc: 0.8158 - val_loss: 0.9558 - val_acc: 0.7410\n",
      "Epoch 884/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8534 - acc: 0.8146 - val_loss: 0.9608 - val_acc: 0.7440\n",
      "Epoch 885/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8549 - acc: 0.8117 - val_loss: 0.9663 - val_acc: 0.7410\n",
      "Epoch 886/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8559 - acc: 0.8145 - val_loss: 0.9590 - val_acc: 0.7450\n",
      "Epoch 887/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8540 - acc: 0.8189 - val_loss: 0.9653 - val_acc: 0.7440\n",
      "Epoch 888/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8544 - acc: 0.8128 - val_loss: 0.9882 - val_acc: 0.7390\n",
      "Epoch 889/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8542 - acc: 0.8143 - val_loss: 0.9614 - val_acc: 0.7460\n",
      "Epoch 890/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8551 - acc: 0.8128 - val_loss: 0.9609 - val_acc: 0.7440\n",
      "Epoch 891/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8557 - acc: 0.8134 - val_loss: 0.9531 - val_acc: 0.7440\n",
      "Epoch 892/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8530 - acc: 0.8140 - val_loss: 0.9594 - val_acc: 0.7390\n",
      "Epoch 893/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.8551 - acc: 0.8168 - val_loss: 0.9578 - val_acc: 0.7470\n",
      "Epoch 894/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8549 - acc: 0.8149 - val_loss: 0.9734 - val_acc: 0.7440\n",
      "Epoch 895/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8540 - acc: 0.8154 - val_loss: 0.9544 - val_acc: 0.7530\n",
      "Epoch 896/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8524 - acc: 0.8154 - val_loss: 0.9585 - val_acc: 0.7480\n",
      "Epoch 897/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8529 - acc: 0.8118 - val_loss: 0.9589 - val_acc: 0.7420\n",
      "Epoch 898/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8545 - acc: 0.8166 - val_loss: 0.9549 - val_acc: 0.7490\n",
      "Epoch 899/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8513 - acc: 0.8154 - val_loss: 0.9566 - val_acc: 0.7530\n",
      "Epoch 900/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8515 - acc: 0.8137 - val_loss: 0.9603 - val_acc: 0.7450\n",
      "Epoch 901/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8518 - acc: 0.8128 - val_loss: 0.9615 - val_acc: 0.7450\n",
      "Epoch 902/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8504 - acc: 0.8154 - val_loss: 0.9527 - val_acc: 0.7500\n",
      "Epoch 903/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8520 - acc: 0.8168 - val_loss: 0.9547 - val_acc: 0.7560\n",
      "Epoch 904/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8512 - acc: 0.8135 - val_loss: 0.9564 - val_acc: 0.7460\n",
      "Epoch 905/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8514 - acc: 0.8166 - val_loss: 0.9554 - val_acc: 0.7420\n",
      "Epoch 906/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8510 - acc: 0.8172 - val_loss: 0.9528 - val_acc: 0.7550\n",
      "Epoch 907/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8510 - acc: 0.8151 - val_loss: 0.9555 - val_acc: 0.7410\n",
      "Epoch 908/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8509 - acc: 0.8169 - val_loss: 0.9557 - val_acc: 0.7440\n",
      "Epoch 909/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8488 - acc: 0.8151 - val_loss: 0.9535 - val_acc: 0.7640\n",
      "Epoch 910/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8502 - acc: 0.8151 - val_loss: 0.9714 - val_acc: 0.7430\n",
      "Epoch 911/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8518 - acc: 0.8149 - val_loss: 0.9482 - val_acc: 0.7530\n",
      "Epoch 912/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8477 - acc: 0.8198 - val_loss: 0.9766 - val_acc: 0.7490\n",
      "Epoch 913/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8520 - acc: 0.8152 - val_loss: 0.9634 - val_acc: 0.7470\n",
      "Epoch 914/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8488 - acc: 0.8171 - val_loss: 0.9490 - val_acc: 0.7550\n",
      "Epoch 915/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8486 - acc: 0.8154 - val_loss: 0.9599 - val_acc: 0.7450\n",
      "Epoch 916/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8511 - acc: 0.8137 - val_loss: 0.9523 - val_acc: 0.7520\n",
      "Epoch 917/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8500 - acc: 0.8163 - val_loss: 0.9671 - val_acc: 0.7420\n",
      "Epoch 918/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8486 - acc: 0.8195 - val_loss: 0.9501 - val_acc: 0.7450\n",
      "Epoch 919/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8486 - acc: 0.8174 - val_loss: 0.9752 - val_acc: 0.7450\n",
      "Epoch 920/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8499 - acc: 0.8152 - val_loss: 0.9545 - val_acc: 0.7490\n",
      "Epoch 921/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8488 - acc: 0.8157 - val_loss: 0.9761 - val_acc: 0.7390\n",
      "Epoch 922/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8479 - acc: 0.8151 - val_loss: 0.9624 - val_acc: 0.7420\n",
      "Epoch 923/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8478 - acc: 0.8171 - val_loss: 0.9558 - val_acc: 0.7530\n",
      "Epoch 924/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8477 - acc: 0.8172 - val_loss: 0.9574 - val_acc: 0.7410\n",
      "Epoch 925/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8475 - acc: 0.8158 - val_loss: 0.9765 - val_acc: 0.7510\n",
      "Epoch 926/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8472 - acc: 0.8205 - val_loss: 0.9736 - val_acc: 0.7470\n",
      "Epoch 927/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8484 - acc: 0.8174 - val_loss: 0.9462 - val_acc: 0.7550\n",
      "Epoch 928/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8455 - acc: 0.8166 - val_loss: 0.9508 - val_acc: 0.7470\n",
      "Epoch 929/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8480 - acc: 0.8154 - val_loss: 0.9490 - val_acc: 0.7590\n",
      "Epoch 930/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8447 - acc: 0.8189 - val_loss: 0.9527 - val_acc: 0.7440\n",
      "Epoch 931/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8435 - acc: 0.8178 - val_loss: 0.9685 - val_acc: 0.7510\n",
      "Epoch 932/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8448 - acc: 0.8148 - val_loss: 0.9650 - val_acc: 0.7460\n",
      "Epoch 933/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8490 - acc: 0.8202 - val_loss: 0.9511 - val_acc: 0.7460\n",
      "Epoch 934/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8447 - acc: 0.8186 - val_loss: 0.9471 - val_acc: 0.7550\n",
      "Epoch 935/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8451 - acc: 0.8171 - val_loss: 0.9537 - val_acc: 0.7500\n",
      "Epoch 936/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8439 - acc: 0.8194 - val_loss: 0.9545 - val_acc: 0.7550\n",
      "Epoch 937/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8467 - acc: 0.8185 - val_loss: 0.9562 - val_acc: 0.7540\n",
      "Epoch 938/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8448 - acc: 0.8214 - val_loss: 0.9517 - val_acc: 0.7540\n",
      "Epoch 939/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8426 - acc: 0.8235 - val_loss: 0.9483 - val_acc: 0.7550\n",
      "Epoch 940/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8443 - acc: 0.8206 - val_loss: 0.9549 - val_acc: 0.7530\n",
      "Epoch 941/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8457 - acc: 0.8222 - val_loss: 0.9861 - val_acc: 0.7400\n",
      "Epoch 942/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8459 - acc: 0.8182 - val_loss: 0.9495 - val_acc: 0.7570\n",
      "Epoch 943/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8447 - acc: 0.8198 - val_loss: 0.9476 - val_acc: 0.7580\n",
      "Epoch 944/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8414 - acc: 0.8220 - val_loss: 0.9523 - val_acc: 0.7490\n",
      "Epoch 945/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8425 - acc: 0.8186 - val_loss: 0.9450 - val_acc: 0.7570\n",
      "Epoch 946/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8423 - acc: 0.8202 - val_loss: 0.9467 - val_acc: 0.7590\n",
      "Epoch 947/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8427 - acc: 0.8205 - val_loss: 0.9619 - val_acc: 0.7460\n",
      "Epoch 948/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8420 - acc: 0.8182 - val_loss: 0.9473 - val_acc: 0.7540\n",
      "Epoch 949/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8413 - acc: 0.8188 - val_loss: 0.9524 - val_acc: 0.7550\n",
      "Epoch 950/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8416 - acc: 0.8228 - val_loss: 0.9582 - val_acc: 0.7460\n",
      "Epoch 951/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8412 - acc: 0.8232 - val_loss: 0.9543 - val_acc: 0.7520\n",
      "Epoch 952/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8421 - acc: 0.8205 - val_loss: 0.9627 - val_acc: 0.7420\n",
      "Epoch 953/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8408 - acc: 0.8202 - val_loss: 0.9448 - val_acc: 0.7550\n",
      "Epoch 954/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8396 - acc: 0.8222 - val_loss: 0.9541 - val_acc: 0.7410\n",
      "Epoch 955/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8407 - acc: 0.8198 - val_loss: 0.9766 - val_acc: 0.7250\n",
      "Epoch 956/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8438 - acc: 0.8191 - val_loss: 0.9447 - val_acc: 0.7560\n",
      "Epoch 957/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8432 - acc: 0.8163 - val_loss: 0.9598 - val_acc: 0.7590\n",
      "Epoch 958/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8405 - acc: 0.8202 - val_loss: 0.9587 - val_acc: 0.7500\n",
      "Epoch 959/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8406 - acc: 0.8188 - val_loss: 0.9591 - val_acc: 0.7440\n",
      "Epoch 960/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8402 - acc: 0.8217 - val_loss: 0.9481 - val_acc: 0.7560\n",
      "Epoch 961/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8408 - acc: 0.8209 - val_loss: 0.9539 - val_acc: 0.7580\n",
      "Epoch 962/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8397 - acc: 0.8195 - val_loss: 0.9523 - val_acc: 0.7590\n",
      "Epoch 963/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8387 - acc: 0.8205 - val_loss: 0.9470 - val_acc: 0.7540\n",
      "Epoch 964/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8400 - acc: 0.8215 - val_loss: 0.9802 - val_acc: 0.7400\n",
      "Epoch 965/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8391 - acc: 0.8228 - val_loss: 0.9650 - val_acc: 0.7410\n",
      "Epoch 966/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8391 - acc: 0.8222 - val_loss: 0.9481 - val_acc: 0.7450\n",
      "Epoch 967/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8381 - acc: 0.8220 - val_loss: 0.9436 - val_acc: 0.7520\n",
      "Epoch 968/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8389 - acc: 0.8202 - val_loss: 0.9463 - val_acc: 0.7550\n",
      "Epoch 969/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8388 - acc: 0.8205 - val_loss: 0.9446 - val_acc: 0.7550\n",
      "Epoch 970/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8379 - acc: 0.8195 - val_loss: 0.9555 - val_acc: 0.7480\n",
      "Epoch 971/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8413 - acc: 0.8198 - val_loss: 0.9643 - val_acc: 0.7430\n",
      "Epoch 972/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8388 - acc: 0.8191 - val_loss: 0.9468 - val_acc: 0.7590\n",
      "Epoch 973/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8395 - acc: 0.8203 - val_loss: 0.9486 - val_acc: 0.7520\n",
      "Epoch 974/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8364 - acc: 0.8254 - val_loss: 0.9513 - val_acc: 0.7440\n",
      "Epoch 975/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8378 - acc: 0.8212 - val_loss: 0.9438 - val_acc: 0.7560\n",
      "Epoch 976/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8376 - acc: 0.8217 - val_loss: 0.9520 - val_acc: 0.7610\n",
      "Epoch 977/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8376 - acc: 0.8209 - val_loss: 0.9482 - val_acc: 0.7560\n",
      "Epoch 978/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8363 - acc: 0.8225 - val_loss: 0.9557 - val_acc: 0.7570\n",
      "Epoch 979/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8386 - acc: 0.8182 - val_loss: 0.9712 - val_acc: 0.7400\n",
      "Epoch 980/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8393 - acc: 0.8223 - val_loss: 0.9437 - val_acc: 0.7560\n",
      "Epoch 981/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8366 - acc: 0.8208 - val_loss: 0.9517 - val_acc: 0.7460\n",
      "Epoch 982/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8380 - acc: 0.8203 - val_loss: 0.9450 - val_acc: 0.7580\n",
      "Epoch 983/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8373 - acc: 0.8268 - val_loss: 0.9469 - val_acc: 0.7480\n",
      "Epoch 984/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8365 - acc: 0.8218 - val_loss: 0.9416 - val_acc: 0.7540\n",
      "Epoch 985/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8345 - acc: 0.8258 - val_loss: 0.9420 - val_acc: 0.7550\n",
      "Epoch 986/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8342 - acc: 0.8231 - val_loss: 0.9415 - val_acc: 0.7540\n",
      "Epoch 987/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8333 - acc: 0.8268 - val_loss: 0.9523 - val_acc: 0.7570\n",
      "Epoch 988/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8344 - acc: 0.8235 - val_loss: 0.9460 - val_acc: 0.7580\n",
      "Epoch 989/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8347 - acc: 0.8246 - val_loss: 0.9440 - val_acc: 0.7580\n",
      "Epoch 990/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8343 - acc: 0.8232 - val_loss: 0.9516 - val_acc: 0.7440\n",
      "Epoch 991/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8342 - acc: 0.8254 - val_loss: 0.9399 - val_acc: 0.7580\n",
      "Epoch 992/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8352 - acc: 0.8240 - val_loss: 0.9487 - val_acc: 0.7480\n",
      "Epoch 993/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.8346 - acc: 0.8234 - val_loss: 0.9458 - val_acc: 0.7570\n",
      "Epoch 994/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.8333 - acc: 0.8235 - val_loss: 0.9466 - val_acc: 0.7550\n",
      "Epoch 995/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.8344 - acc: 0.8235 - val_loss: 0.9476 - val_acc: 0.7450\n",
      "Epoch 996/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.8326 - acc: 0.8249 - val_loss: 0.9507 - val_acc: 0.7530\n",
      "Epoch 997/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.8341 - acc: 0.8255 - val_loss: 0.9527 - val_acc: 0.7450\n",
      "Epoch 998/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.8332 - acc: 0.8222 - val_loss: 0.9479 - val_acc: 0.7650\n",
      "Epoch 999/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8317 - acc: 0.8257 - val_loss: 0.9594 - val_acc: 0.7440\n",
      "Epoch 1000/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8323 - acc: 0.8265 - val_loss: 0.9623 - val_acc: 0.7460\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=1000,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g,', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 31us/step\n",
      "2500/2500 [==============================] - 0s 27us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8508234373606168, 0.8223076923076923]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.976454968547821, 0.7632]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best we've seen so far, but we were training for quite a while! Let's see if dropout regularization can do even better and/or be more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\james\\Anaconda3\\envs\\learn-env-ext\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train on 6500 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "6500/6500 [==============================] - 1s 103us/step - loss: 1.9696 - acc: 0.1508 - val_loss: 1.9196 - val_acc: 0.1790\n",
      "Epoch 2/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 1.9501 - acc: 0.1618 - val_loss: 1.9076 - val_acc: 0.1980\n",
      "Epoch 3/200\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 1.9335 - acc: 0.1754 - val_loss: 1.8976 - val_acc: 0.2110\n",
      "Epoch 4/200\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 1.9201 - acc: 0.1851 - val_loss: 1.8875 - val_acc: 0.2260\n",
      "Epoch 5/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 1.9084 - acc: 0.1983 - val_loss: 1.8768 - val_acc: 0.2440\n",
      "Epoch 6/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 1.9012 - acc: 0.2108 - val_loss: 1.8653 - val_acc: 0.2620\n",
      "Epoch 7/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 1.8893 - acc: 0.2131 - val_loss: 1.8531 - val_acc: 0.2790\n",
      "Epoch 8/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 1.8781 - acc: 0.2288 - val_loss: 1.8385 - val_acc: 0.2970\n",
      "Epoch 9/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 1.8638 - acc: 0.2394 - val_loss: 1.8225 - val_acc: 0.3130\n",
      "Epoch 10/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 1.8522 - acc: 0.2432 - val_loss: 1.8053 - val_acc: 0.3210\n",
      "Epoch 11/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 1.8443 - acc: 0.2526 - val_loss: 1.7871 - val_acc: 0.3430\n",
      "Epoch 12/200\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 1.8288 - acc: 0.2672 - val_loss: 1.7666 - val_acc: 0.3600\n",
      "Epoch 13/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 1.8114 - acc: 0.2775 - val_loss: 1.7441 - val_acc: 0.3720\n",
      "Epoch 14/200\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 1.7979 - acc: 0.2871 - val_loss: 1.7226 - val_acc: 0.3830\n",
      "Epoch 15/200\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 1.7732 - acc: 0.2980 - val_loss: 1.6990 - val_acc: 0.4010\n",
      "Epoch 16/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 1.7649 - acc: 0.2988 - val_loss: 1.6778 - val_acc: 0.4160\n",
      "Epoch 17/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 1.7491 - acc: 0.3158 - val_loss: 1.6554 - val_acc: 0.4290\n",
      "Epoch 18/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 1.7294 - acc: 0.3225 - val_loss: 1.6331 - val_acc: 0.4360\n",
      "Epoch 19/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 1.7098 - acc: 0.3342 - val_loss: 1.6098 - val_acc: 0.4410\n",
      "Epoch 20/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 1.6979 - acc: 0.3385 - val_loss: 1.5897 - val_acc: 0.4560\n",
      "Epoch 21/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 1.6786 - acc: 0.3454 - val_loss: 1.5669 - val_acc: 0.4650\n",
      "Epoch 22/200\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 1.6664 - acc: 0.3502 - val_loss: 1.5471 - val_acc: 0.4730\n",
      "Epoch 23/200\n",
      "6500/6500 [==============================] - 0s 61us/step - loss: 1.6547 - acc: 0.3482 - val_loss: 1.5285 - val_acc: 0.4840\n",
      "Epoch 24/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 1.6414 - acc: 0.3594 - val_loss: 1.5096 - val_acc: 0.4870\n",
      "Epoch 25/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 1.6201 - acc: 0.3708 - val_loss: 1.4891 - val_acc: 0.4890\n",
      "Epoch 26/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 1.6136 - acc: 0.3666 - val_loss: 1.4720 - val_acc: 0.4990\n",
      "Epoch 27/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 1.5892 - acc: 0.3780 - val_loss: 1.4534 - val_acc: 0.5140\n",
      "Epoch 28/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 1.5880 - acc: 0.3814 - val_loss: 1.4401 - val_acc: 0.5150\n",
      "Epoch 29/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 1.5730 - acc: 0.3835 - val_loss: 1.4228 - val_acc: 0.5180\n",
      "Epoch 30/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 1.5400 - acc: 0.4011 - val_loss: 1.4048 - val_acc: 0.5280\n",
      "Epoch 31/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 1.5407 - acc: 0.4011 - val_loss: 1.3902 - val_acc: 0.5350\n",
      "Epoch 32/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 1.5218 - acc: 0.4005 - val_loss: 1.3749 - val_acc: 0.5350\n",
      "Epoch 33/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 1.5213 - acc: 0.4032 - val_loss: 1.3615 - val_acc: 0.5370\n",
      "Epoch 34/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 1.5249 - acc: 0.3972 - val_loss: 1.3519 - val_acc: 0.5400\n",
      "Epoch 35/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 1.4903 - acc: 0.4175 - val_loss: 1.3394 - val_acc: 0.5520\n",
      "Epoch 36/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 1.4893 - acc: 0.4117 - val_loss: 1.3258 - val_acc: 0.5530\n",
      "Epoch 37/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 1.4761 - acc: 0.4162 - val_loss: 1.3145 - val_acc: 0.5620\n",
      "Epoch 38/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 1.4622 - acc: 0.4271 - val_loss: 1.3021 - val_acc: 0.5570\n",
      "Epoch 39/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 1.4624 - acc: 0.4265 - val_loss: 1.2920 - val_acc: 0.5680\n",
      "Epoch 40/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 1.4374 - acc: 0.4314 - val_loss: 1.2794 - val_acc: 0.5760\n",
      "Epoch 41/200\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 1.4275 - acc: 0.4535 - val_loss: 1.2654 - val_acc: 0.5740\n",
      "Epoch 42/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 1.4307 - acc: 0.4391 - val_loss: 1.2550 - val_acc: 0.5850\n",
      "Epoch 43/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 1.4210 - acc: 0.4451 - val_loss: 1.2459 - val_acc: 0.5840\n",
      "Epoch 44/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 1.4032 - acc: 0.4525 - val_loss: 1.2353 - val_acc: 0.5890\n",
      "Epoch 45/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 1.4011 - acc: 0.4497 - val_loss: 1.2247 - val_acc: 0.6050\n",
      "Epoch 46/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 1.3808 - acc: 0.4654 - val_loss: 1.2134 - val_acc: 0.6040\n",
      "Epoch 47/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 1.3842 - acc: 0.4611 - val_loss: 1.2041 - val_acc: 0.6060\n",
      "Epoch 48/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 1.3742 - acc: 0.4625 - val_loss: 1.1950 - val_acc: 0.6170\n",
      "Epoch 49/200\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 1.3505 - acc: 0.4700 - val_loss: 1.1831 - val_acc: 0.6260\n",
      "Epoch 50/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 1.3605 - acc: 0.4675 - val_loss: 1.1748 - val_acc: 0.6200\n",
      "Epoch 51/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 1.3490 - acc: 0.4765 - val_loss: 1.1660 - val_acc: 0.6320\n",
      "Epoch 52/200\n",
      "6500/6500 [==============================] - 0s 60us/step - loss: 1.3328 - acc: 0.4746 - val_loss: 1.1555 - val_acc: 0.6320\n",
      "Epoch 53/200\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 1.3223 - acc: 0.4834 - val_loss: 1.1465 - val_acc: 0.6310\n",
      "Epoch 54/200\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 1.3268 - acc: 0.4855 - val_loss: 1.1375 - val_acc: 0.6310\n",
      "Epoch 55/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 1.3110 - acc: 0.4888 - val_loss: 1.1275 - val_acc: 0.6340\n",
      "Epoch 56/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 1.3082 - acc: 0.4928 - val_loss: 1.1200 - val_acc: 0.6340\n",
      "Epoch 57/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 1.3001 - acc: 0.4918 - val_loss: 1.1121 - val_acc: 0.6450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 1.2922 - acc: 0.4954 - val_loss: 1.1049 - val_acc: 0.6430\n",
      "Epoch 59/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 1.2774 - acc: 0.5042 - val_loss: 1.0972 - val_acc: 0.6400\n",
      "Epoch 60/200\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 1.2742 - acc: 0.4991 - val_loss: 1.0881 - val_acc: 0.6480\n",
      "Epoch 61/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 1.2722 - acc: 0.5125 - val_loss: 1.0815 - val_acc: 0.6520\n",
      "Epoch 62/200\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 1.2688 - acc: 0.5166 - val_loss: 1.0733 - val_acc: 0.6500\n",
      "Epoch 63/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 1.2499 - acc: 0.5220 - val_loss: 1.0640 - val_acc: 0.6510\n",
      "Epoch 64/200\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 1.2542 - acc: 0.5186 - val_loss: 1.0571 - val_acc: 0.6520\n",
      "Epoch 65/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 1.2329 - acc: 0.5242 - val_loss: 1.0478 - val_acc: 0.6560\n",
      "Epoch 66/200\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 1.2144 - acc: 0.5372 - val_loss: 1.0370 - val_acc: 0.6570\n",
      "Epoch 67/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 1.2295 - acc: 0.5252 - val_loss: 1.0301 - val_acc: 0.6580\n",
      "Epoch 68/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 1.2282 - acc: 0.5258 - val_loss: 1.0257 - val_acc: 0.6630\n",
      "Epoch 69/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 1.2187 - acc: 0.5346 - val_loss: 1.0201 - val_acc: 0.6600\n",
      "Epoch 70/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 1.2059 - acc: 0.5328 - val_loss: 1.0112 - val_acc: 0.6610\n",
      "Epoch 71/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 1.1997 - acc: 0.5422 - val_loss: 1.0038 - val_acc: 0.6620\n",
      "Epoch 72/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 1.2001 - acc: 0.5385 - val_loss: 0.9954 - val_acc: 0.6660\n",
      "Epoch 73/200\n",
      "6500/6500 [==============================] - 0s 48us/step - loss: 1.2000 - acc: 0.5452 - val_loss: 0.9897 - val_acc: 0.6660\n",
      "Epoch 74/200\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 1.1955 - acc: 0.5408 - val_loss: 0.9847 - val_acc: 0.6730\n",
      "Epoch 75/200\n",
      "6500/6500 [==============================] - 0s 48us/step - loss: 1.1797 - acc: 0.5455 - val_loss: 0.9784 - val_acc: 0.6800\n",
      "Epoch 76/200\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 1.1720 - acc: 0.5518 - val_loss: 0.9704 - val_acc: 0.6780\n",
      "Epoch 77/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 1.1650 - acc: 0.5537 - val_loss: 0.9637 - val_acc: 0.6790\n",
      "Epoch 78/200\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 1.1781 - acc: 0.5443 - val_loss: 0.9613 - val_acc: 0.6750\n",
      "Epoch 79/200\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 1.1459 - acc: 0.5549 - val_loss: 0.9523 - val_acc: 0.6770\n",
      "Epoch 80/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 1.1481 - acc: 0.5677 - val_loss: 0.9460 - val_acc: 0.6780\n",
      "Epoch 81/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 1.1491 - acc: 0.5634 - val_loss: 0.9411 - val_acc: 0.6790\n",
      "Epoch 82/200\n",
      "6500/6500 [==============================] - 0s 48us/step - loss: 1.1439 - acc: 0.5594 - val_loss: 0.9361 - val_acc: 0.6760\n",
      "Epoch 83/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 1.1270 - acc: 0.5703 - val_loss: 0.9292 - val_acc: 0.6790\n",
      "Epoch 84/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 1.1372 - acc: 0.5675 - val_loss: 0.9229 - val_acc: 0.6830\n",
      "Epoch 85/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 1.1297 - acc: 0.5762 - val_loss: 0.9182 - val_acc: 0.6840\n",
      "Epoch 86/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 1.1241 - acc: 0.5677 - val_loss: 0.9113 - val_acc: 0.6920\n",
      "Epoch 87/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 1.1242 - acc: 0.5688 - val_loss: 0.9078 - val_acc: 0.6930\n",
      "Epoch 88/200\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 1.0944 - acc: 0.5943 - val_loss: 0.9005 - val_acc: 0.6830\n",
      "Epoch 89/200\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 1.0972 - acc: 0.5766 - val_loss: 0.8952 - val_acc: 0.6830\n",
      "Epoch 90/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 1.0898 - acc: 0.5932 - val_loss: 0.8894 - val_acc: 0.6840\n",
      "Epoch 91/200\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 1.1067 - acc: 0.5769 - val_loss: 0.8862 - val_acc: 0.6890\n",
      "Epoch 92/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 1.1047 - acc: 0.5740 - val_loss: 0.8805 - val_acc: 0.6970\n",
      "Epoch 93/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 1.0818 - acc: 0.5869 - val_loss: 0.8751 - val_acc: 0.6930\n",
      "Epoch 94/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 1.0854 - acc: 0.5866 - val_loss: 0.8716 - val_acc: 0.6930\n",
      "Epoch 95/200\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 1.0749 - acc: 0.5932 - val_loss: 0.8681 - val_acc: 0.6880\n",
      "Epoch 96/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 1.0882 - acc: 0.5892 - val_loss: 0.8625 - val_acc: 0.6970\n",
      "Epoch 97/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 1.0772 - acc: 0.5895 - val_loss: 0.8580 - val_acc: 0.7000\n",
      "Epoch 98/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 1.0693 - acc: 0.5966 - val_loss: 0.8545 - val_acc: 0.6980\n",
      "Epoch 99/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 1.0597 - acc: 0.6080 - val_loss: 0.8505 - val_acc: 0.6970\n",
      "Epoch 100/200\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 1.0557 - acc: 0.5962 - val_loss: 0.8466 - val_acc: 0.6980\n",
      "Epoch 101/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 1.0527 - acc: 0.6015 - val_loss: 0.8435 - val_acc: 0.6960\n",
      "Epoch 102/200\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 1.0427 - acc: 0.6066 - val_loss: 0.8373 - val_acc: 0.7000\n",
      "Epoch 103/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 1.0434 - acc: 0.6108 - val_loss: 0.8331 - val_acc: 0.7030\n",
      "Epoch 104/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 1.0319 - acc: 0.6122 - val_loss: 0.8283 - val_acc: 0.7060\n",
      "Epoch 105/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 1.0401 - acc: 0.6065 - val_loss: 0.8263 - val_acc: 0.7000\n",
      "Epoch 106/200\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 1.0327 - acc: 0.6183 - val_loss: 0.8214 - val_acc: 0.7050\n",
      "Epoch 107/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 1.0294 - acc: 0.6166 - val_loss: 0.8172 - val_acc: 0.7040\n",
      "Epoch 108/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 1.0313 - acc: 0.6078 - val_loss: 0.8139 - val_acc: 0.7040\n",
      "Epoch 109/200\n",
      "6500/6500 [==============================] - 0s 48us/step - loss: 1.0268 - acc: 0.6194 - val_loss: 0.8125 - val_acc: 0.7060\n",
      "Epoch 110/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 1.0136 - acc: 0.6129 - val_loss: 0.8093 - val_acc: 0.7100\n",
      "Epoch 111/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 1.0105 - acc: 0.6242 - val_loss: 0.8056 - val_acc: 0.7090\n",
      "Epoch 112/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 1.0024 - acc: 0.6246 - val_loss: 0.8003 - val_acc: 0.7140\n",
      "Epoch 113/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 1.0138 - acc: 0.6229 - val_loss: 0.7999 - val_acc: 0.7070\n",
      "Epoch 114/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 1.0093 - acc: 0.6191 - val_loss: 0.7933 - val_acc: 0.7150\n",
      "Epoch 115/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.9992 - acc: 0.6271 - val_loss: 0.7914 - val_acc: 0.7130\n",
      "Epoch 116/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.9842 - acc: 0.6340 - val_loss: 0.7870 - val_acc: 0.7150\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 52us/step - loss: 0.9938 - acc: 0.6371 - val_loss: 0.7861 - val_acc: 0.7160\n",
      "Epoch 118/200\n",
      "6500/6500 [==============================] - 0s 48us/step - loss: 0.9855 - acc: 0.6302 - val_loss: 0.7849 - val_acc: 0.7150\n",
      "Epoch 119/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 0.9811 - acc: 0.6343 - val_loss: 0.7823 - val_acc: 0.7150\n",
      "Epoch 120/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.9929 - acc: 0.6280 - val_loss: 0.7778 - val_acc: 0.7150\n",
      "Epoch 121/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.9978 - acc: 0.6275 - val_loss: 0.7750 - val_acc: 0.7170\n",
      "Epoch 122/200\n",
      "6500/6500 [==============================] - 0s 48us/step - loss: 0.9835 - acc: 0.6323 - val_loss: 0.7723 - val_acc: 0.7150\n",
      "Epoch 123/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.9688 - acc: 0.6335 - val_loss: 0.7666 - val_acc: 0.7170\n",
      "Epoch 124/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.9801 - acc: 0.6320 - val_loss: 0.7662 - val_acc: 0.7160\n",
      "Epoch 125/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 0.9614 - acc: 0.6342 - val_loss: 0.7631 - val_acc: 0.7190\n",
      "Epoch 126/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.9586 - acc: 0.6398 - val_loss: 0.7592 - val_acc: 0.7190\n",
      "Epoch 127/200\n",
      "6500/6500 [==============================] - 0s 48us/step - loss: 0.9512 - acc: 0.6348 - val_loss: 0.7569 - val_acc: 0.7210\n",
      "Epoch 128/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.9577 - acc: 0.6405 - val_loss: 0.7514 - val_acc: 0.7230\n",
      "Epoch 129/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.9463 - acc: 0.6405 - val_loss: 0.7515 - val_acc: 0.7250\n",
      "Epoch 130/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.9470 - acc: 0.6422 - val_loss: 0.7480 - val_acc: 0.7240\n",
      "Epoch 131/200\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 0.9503 - acc: 0.6477 - val_loss: 0.7479 - val_acc: 0.7220\n",
      "Epoch 132/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.9305 - acc: 0.6554 - val_loss: 0.7461 - val_acc: 0.7220\n",
      "Epoch 133/200\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 0.9536 - acc: 0.6498 - val_loss: 0.7418 - val_acc: 0.7250\n",
      "Epoch 134/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 0.9374 - acc: 0.6503 - val_loss: 0.7395 - val_acc: 0.7230\n",
      "Epoch 135/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 0.9447 - acc: 0.6468 - val_loss: 0.7393 - val_acc: 0.7230\n",
      "Epoch 136/200\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 0.9321 - acc: 0.6509 - val_loss: 0.7352 - val_acc: 0.7280\n",
      "Epoch 137/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.9309 - acc: 0.6600 - val_loss: 0.7323 - val_acc: 0.7300\n",
      "Epoch 138/200\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 0.9261 - acc: 0.6574 - val_loss: 0.7324 - val_acc: 0.7310\n",
      "Epoch 139/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.9353 - acc: 0.6535 - val_loss: 0.7312 - val_acc: 0.7250\n",
      "Epoch 140/200\n",
      "6500/6500 [==============================] - 0s 48us/step - loss: 0.9226 - acc: 0.6549 - val_loss: 0.7274 - val_acc: 0.7300\n",
      "Epoch 141/200\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 0.9242 - acc: 0.6502 - val_loss: 0.7256 - val_acc: 0.7310\n",
      "Epoch 142/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 0.9271 - acc: 0.6535 - val_loss: 0.7264 - val_acc: 0.7290\n",
      "Epoch 143/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.9158 - acc: 0.6531 - val_loss: 0.7246 - val_acc: 0.7280\n",
      "Epoch 144/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.9170 - acc: 0.6623 - val_loss: 0.7224 - val_acc: 0.7330\n",
      "Epoch 145/200\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 0.9086 - acc: 0.6597 - val_loss: 0.7174 - val_acc: 0.7350\n",
      "Epoch 146/200\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 0.9048 - acc: 0.6625 - val_loss: 0.7153 - val_acc: 0.7330\n",
      "Epoch 147/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 0.9159 - acc: 0.6497 - val_loss: 0.7165 - val_acc: 0.7330\n",
      "Epoch 148/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 0.9090 - acc: 0.6571 - val_loss: 0.7112 - val_acc: 0.7350\n",
      "Epoch 149/200\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 0.8996 - acc: 0.6632 - val_loss: 0.7096 - val_acc: 0.7370\n",
      "Epoch 150/200\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 0.9121 - acc: 0.6622 - val_loss: 0.7109 - val_acc: 0.7290\n",
      "Epoch 151/200\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 0.9086 - acc: 0.6569 - val_loss: 0.7101 - val_acc: 0.7320\n",
      "Epoch 152/200\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 0.9121 - acc: 0.6555 - val_loss: 0.7064 - val_acc: 0.7330\n",
      "Epoch 153/200\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 0.8827 - acc: 0.6722 - val_loss: 0.7047 - val_acc: 0.7340\n",
      "Epoch 154/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.9080 - acc: 0.6548 - val_loss: 0.7047 - val_acc: 0.7330\n",
      "Epoch 155/200\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 0.8869 - acc: 0.6740 - val_loss: 0.7048 - val_acc: 0.7340\n",
      "Epoch 156/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.8776 - acc: 0.6778 - val_loss: 0.6999 - val_acc: 0.7360\n",
      "Epoch 157/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 0.8886 - acc: 0.6658 - val_loss: 0.6985 - val_acc: 0.7350\n",
      "Epoch 158/200\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 0.8737 - acc: 0.6803 - val_loss: 0.6970 - val_acc: 0.7360\n",
      "Epoch 159/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.8800 - acc: 0.6737 - val_loss: 0.6946 - val_acc: 0.7380\n",
      "Epoch 160/200\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 0.8835 - acc: 0.6731 - val_loss: 0.6950 - val_acc: 0.7320\n",
      "Epoch 161/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.8743 - acc: 0.6708 - val_loss: 0.6926 - val_acc: 0.7330\n",
      "Epoch 162/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.8711 - acc: 0.6751 - val_loss: 0.6918 - val_acc: 0.7370\n",
      "Epoch 163/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 0.8697 - acc: 0.6806 - val_loss: 0.6914 - val_acc: 0.7340\n",
      "Epoch 164/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 0.8768 - acc: 0.6751 - val_loss: 0.6900 - val_acc: 0.7360\n",
      "Epoch 165/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.8759 - acc: 0.6795 - val_loss: 0.6882 - val_acc: 0.7340\n",
      "Epoch 166/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.8644 - acc: 0.6849 - val_loss: 0.6863 - val_acc: 0.7380\n",
      "Epoch 167/200\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 0.8647 - acc: 0.6762 - val_loss: 0.6850 - val_acc: 0.7370\n",
      "Epoch 168/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.8551 - acc: 0.6754 - val_loss: 0.6826 - val_acc: 0.7390\n",
      "Epoch 169/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.8435 - acc: 0.6862 - val_loss: 0.6812 - val_acc: 0.7410\n",
      "Epoch 170/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 0.8595 - acc: 0.6815 - val_loss: 0.6796 - val_acc: 0.7400\n",
      "Epoch 171/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.8638 - acc: 0.6803 - val_loss: 0.6791 - val_acc: 0.7400\n",
      "Epoch 172/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 0.8494 - acc: 0.6765 - val_loss: 0.6761 - val_acc: 0.7360\n",
      "Epoch 173/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.8488 - acc: 0.6802 - val_loss: 0.6742 - val_acc: 0.7350\n",
      "Epoch 174/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.8668 - acc: 0.6789 - val_loss: 0.6764 - val_acc: 0.7390\n",
      "Epoch 175/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.8507 - acc: 0.6842 - val_loss: 0.6765 - val_acc: 0.7400\n",
      "Epoch 176/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 48us/step - loss: 0.8345 - acc: 0.6842 - val_loss: 0.6730 - val_acc: 0.7380\n",
      "Epoch 177/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 0.8414 - acc: 0.6848 - val_loss: 0.6713 - val_acc: 0.7370\n",
      "Epoch 178/200\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 0.8417 - acc: 0.6851 - val_loss: 0.6695 - val_acc: 0.7420\n",
      "Epoch 179/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.8360 - acc: 0.6874 - val_loss: 0.6692 - val_acc: 0.7380\n",
      "Epoch 180/200\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 0.8405 - acc: 0.6888 - val_loss: 0.6669 - val_acc: 0.7370\n",
      "Epoch 181/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.8237 - acc: 0.6889 - val_loss: 0.6642 - val_acc: 0.7400\n",
      "Epoch 182/200\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 0.8351 - acc: 0.6880 - val_loss: 0.6632 - val_acc: 0.7450\n",
      "Epoch 183/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 0.8401 - acc: 0.6826 - val_loss: 0.6644 - val_acc: 0.7410\n",
      "Epoch 184/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 0.8307 - acc: 0.6883 - val_loss: 0.6610 - val_acc: 0.7420\n",
      "Epoch 185/200\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 0.8123 - acc: 0.6883 - val_loss: 0.6623 - val_acc: 0.7400\n",
      "Epoch 186/200\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 0.8370 - acc: 0.6895 - val_loss: 0.6593 - val_acc: 0.7430\n",
      "Epoch 187/200\n",
      "6500/6500 [==============================] - 0s 48us/step - loss: 0.8200 - acc: 0.6929 - val_loss: 0.6592 - val_acc: 0.7430\n",
      "Epoch 188/200\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 0.8232 - acc: 0.6897 - val_loss: 0.6586 - val_acc: 0.7450\n",
      "Epoch 189/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 0.8237 - acc: 0.6900 - val_loss: 0.6561 - val_acc: 0.7400\n",
      "Epoch 190/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 0.8305 - acc: 0.6855 - val_loss: 0.6570 - val_acc: 0.7420\n",
      "Epoch 191/200\n",
      "6500/6500 [==============================] - 0s 60us/step - loss: 0.8070 - acc: 0.7052 - val_loss: 0.6548 - val_acc: 0.7410\n",
      "Epoch 192/200\n",
      "6500/6500 [==============================] - 0s 62us/step - loss: 0.8254 - acc: 0.6923 - val_loss: 0.6546 - val_acc: 0.7470\n",
      "Epoch 193/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.8127 - acc: 0.6940 - val_loss: 0.6528 - val_acc: 0.7500\n",
      "Epoch 194/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 0.8146 - acc: 0.6994 - val_loss: 0.6516 - val_acc: 0.7450\n",
      "Epoch 195/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 0.8106 - acc: 0.7002 - val_loss: 0.6516 - val_acc: 0.7410\n",
      "Epoch 196/200\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 0.8168 - acc: 0.7008 - val_loss: 0.6512 - val_acc: 0.7400\n",
      "Epoch 197/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.8210 - acc: 0.6886 - val_loss: 0.6521 - val_acc: 0.7450\n",
      "Epoch 198/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 0.8056 - acc: 0.6991 - val_loss: 0.6496 - val_acc: 0.7430\n",
      "Epoch 199/200\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 0.8002 - acc: 0.7045 - val_loss: 0.6486 - val_acc: 0.7440\n",
      "Epoch 200/200\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 0.7951 - acc: 0.7020 - val_loss: 0.6465 - val_acc: 0.7440\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dropout(0.3, input_shape=(2000,)))\n",
    "model.add(layers.Dense(50, activation='relu')) #2 hidden layers\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 24us/step\n",
      "2500/2500 [==============================] - 0s 28us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5051752584714156, 0.8295384615384616]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6459788105487824, 0.7648]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! the variance did become higher again compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, one of the solutions to high variance was just getting more data. We actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple our data set, and see what happens. Note that we are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/120\n",
      "33000/33000 [==============================] - 1s 43us/step - loss: 1.9211 - acc: 0.1885 - val_loss: 1.8696 - val_acc: 0.2450\n",
      "Epoch 2/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 1.7983 - acc: 0.3033 - val_loss: 1.7136 - val_acc: 0.3710\n",
      "Epoch 3/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 1.5961 - acc: 0.4459 - val_loss: 1.4872 - val_acc: 0.5023\n",
      "Epoch 4/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 1.3628 - acc: 0.5688 - val_loss: 1.2696 - val_acc: 0.5937\n",
      "Epoch 5/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 1.1650 - acc: 0.6391 - val_loss: 1.1001 - val_acc: 0.6397\n",
      "Epoch 6/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 1.0175 - acc: 0.6734 - val_loss: 0.9753 - val_acc: 0.6763\n",
      "Epoch 7/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.9132 - acc: 0.6961 - val_loss: 0.8876 - val_acc: 0.6917\n",
      "Epoch 8/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.8399 - acc: 0.7121 - val_loss: 0.8279 - val_acc: 0.7053\n",
      "Epoch 9/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.7870 - acc: 0.7234 - val_loss: 0.7847 - val_acc: 0.7163\n",
      "Epoch 10/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.7475 - acc: 0.7347 - val_loss: 0.7525 - val_acc: 0.7210\n",
      "Epoch 11/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.7168 - acc: 0.7454 - val_loss: 0.7258 - val_acc: 0.7300\n",
      "Epoch 12/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.6921 - acc: 0.7508 - val_loss: 0.7055 - val_acc: 0.7360\n",
      "Epoch 13/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.6718 - acc: 0.7588 - val_loss: 0.6912 - val_acc: 0.7347\n",
      "Epoch 14/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.6538 - acc: 0.7631 - val_loss: 0.6744 - val_acc: 0.7447\n",
      "Epoch 15/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.6383 - acc: 0.7680 - val_loss: 0.6633 - val_acc: 0.7430\n",
      "Epoch 16/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.6249 - acc: 0.7735 - val_loss: 0.6533 - val_acc: 0.7530\n",
      "Epoch 17/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.6123 - acc: 0.7776 - val_loss: 0.6435 - val_acc: 0.7550\n",
      "Epoch 18/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.6013 - acc: 0.7807 - val_loss: 0.6360 - val_acc: 0.7570\n",
      "Epoch 19/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.5909 - acc: 0.7860 - val_loss: 0.6306 - val_acc: 0.7527\n",
      "Epoch 20/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.5816 - acc: 0.7896 - val_loss: 0.6242 - val_acc: 0.7610\n",
      "Epoch 21/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.5729 - acc: 0.7916 - val_loss: 0.6177 - val_acc: 0.7587\n",
      "Epoch 22/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.5646 - acc: 0.7954 - val_loss: 0.6118 - val_acc: 0.7697\n",
      "Epoch 23/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.5566 - acc: 0.7977 - val_loss: 0.6082 - val_acc: 0.7637\n",
      "Epoch 24/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.5495 - acc: 0.8000 - val_loss: 0.6046 - val_acc: 0.7690\n",
      "Epoch 25/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.5426 - acc: 0.8033 - val_loss: 0.6004 - val_acc: 0.7687\n",
      "Epoch 26/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.5363 - acc: 0.8058 - val_loss: 0.5968 - val_acc: 0.7713\n",
      "Epoch 27/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.5297 - acc: 0.8086 - val_loss: 0.5915 - val_acc: 0.7733\n",
      "Epoch 28/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.5239 - acc: 0.8113 - val_loss: 0.5884 - val_acc: 0.7753\n",
      "Epoch 29/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.5182 - acc: 0.8141 - val_loss: 0.5856 - val_acc: 0.7787\n",
      "Epoch 30/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.5128 - acc: 0.8162 - val_loss: 0.5833 - val_acc: 0.7767\n",
      "Epoch 31/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.5072 - acc: 0.8174 - val_loss: 0.5806 - val_acc: 0.7780\n",
      "Epoch 32/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.5024 - acc: 0.8193 - val_loss: 0.5792 - val_acc: 0.7790\n",
      "Epoch 33/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.4977 - acc: 0.8216 - val_loss: 0.5773 - val_acc: 0.7767\n",
      "Epoch 34/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.4930 - acc: 0.8231 - val_loss: 0.5735 - val_acc: 0.7813\n",
      "Epoch 35/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4882 - acc: 0.8260 - val_loss: 0.5730 - val_acc: 0.7813\n",
      "Epoch 36/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.4846 - acc: 0.8262 - val_loss: 0.5713 - val_acc: 0.7833\n",
      "Epoch 37/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.4802 - acc: 0.8295 - val_loss: 0.5683 - val_acc: 0.7830\n",
      "Epoch 38/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.4760 - acc: 0.8306 - val_loss: 0.5713 - val_acc: 0.7843\n",
      "Epoch 39/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.4719 - acc: 0.8325 - val_loss: 0.5667 - val_acc: 0.7857\n",
      "Epoch 40/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.4682 - acc: 0.8339 - val_loss: 0.5671 - val_acc: 0.7887\n",
      "Epoch 41/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.4646 - acc: 0.8349 - val_loss: 0.5647 - val_acc: 0.7880\n",
      "Epoch 42/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.4607 - acc: 0.8373 - val_loss: 0.5625 - val_acc: 0.7883\n",
      "Epoch 43/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.4574 - acc: 0.8370 - val_loss: 0.5613 - val_acc: 0.7880\n",
      "Epoch 44/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4538 - acc: 0.8392 - val_loss: 0.5608 - val_acc: 0.7910\n",
      "Epoch 45/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4507 - acc: 0.8413 - val_loss: 0.5615 - val_acc: 0.7897\n",
      "Epoch 46/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.4473 - acc: 0.8417 - val_loss: 0.5595 - val_acc: 0.7943\n",
      "Epoch 47/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.4440 - acc: 0.8425 - val_loss: 0.5604 - val_acc: 0.7923\n",
      "Epoch 48/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.4409 - acc: 0.8445 - val_loss: 0.5598 - val_acc: 0.7900\n",
      "Epoch 49/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4382 - acc: 0.8453 - val_loss: 0.5580 - val_acc: 0.7927\n",
      "Epoch 50/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4353 - acc: 0.8458 - val_loss: 0.5581 - val_acc: 0.7907\n",
      "Epoch 51/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.4324 - acc: 0.8469 - val_loss: 0.5568 - val_acc: 0.7923\n",
      "Epoch 52/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.4294 - acc: 0.8483 - val_loss: 0.5594 - val_acc: 0.7937\n",
      "Epoch 53/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.4265 - acc: 0.8484 - val_loss: 0.5565 - val_acc: 0.7950\n",
      "Epoch 54/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.4238 - acc: 0.8506 - val_loss: 0.5553 - val_acc: 0.7930\n",
      "Epoch 55/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.4210 - acc: 0.8512 - val_loss: 0.5547 - val_acc: 0.7910\n",
      "Epoch 56/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.4188 - acc: 0.8521 - val_loss: 0.5551 - val_acc: 0.7920\n",
      "Epoch 57/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.4162 - acc: 0.8529 - val_loss: 0.5559 - val_acc: 0.7923\n",
      "Epoch 58/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4136 - acc: 0.8539 - val_loss: 0.5540 - val_acc: 0.7887\n",
      "Epoch 59/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4113 - acc: 0.8541 - val_loss: 0.5560 - val_acc: 0.7917\n",
      "Epoch 60/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.4088 - acc: 0.8557 - val_loss: 0.5557 - val_acc: 0.7923\n",
      "Epoch 61/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4062 - acc: 0.8568 - val_loss: 0.5562 - val_acc: 0.7907\n",
      "Epoch 62/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4044 - acc: 0.8570 - val_loss: 0.5560 - val_acc: 0.7920\n",
      "Epoch 63/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4016 - acc: 0.8575 - val_loss: 0.5556 - val_acc: 0.7897\n",
      "Epoch 64/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3998 - acc: 0.8590 - val_loss: 0.5546 - val_acc: 0.7873\n",
      "Epoch 65/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3970 - acc: 0.8597 - val_loss: 0.5568 - val_acc: 0.7900\n",
      "Epoch 66/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3952 - acc: 0.8604 - val_loss: 0.5580 - val_acc: 0.7907\n",
      "Epoch 67/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3929 - acc: 0.8606 - val_loss: 0.5580 - val_acc: 0.7897\n",
      "Epoch 68/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.3909 - acc: 0.8609 - val_loss: 0.5568 - val_acc: 0.7897\n",
      "Epoch 69/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.3886 - acc: 0.8607 - val_loss: 0.5569 - val_acc: 0.7910\n",
      "Epoch 70/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3870 - acc: 0.8628 - val_loss: 0.5563 - val_acc: 0.7910\n",
      "Epoch 71/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3848 - acc: 0.8632 - val_loss: 0.5600 - val_acc: 0.7910\n",
      "Epoch 72/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3828 - acc: 0.8639 - val_loss: 0.5577 - val_acc: 0.7903\n",
      "Epoch 73/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3811 - acc: 0.8649 - val_loss: 0.5578 - val_acc: 0.7893\n",
      "Epoch 74/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3788 - acc: 0.8650 - val_loss: 0.5568 - val_acc: 0.7900\n",
      "Epoch 75/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3773 - acc: 0.8661 - val_loss: 0.5578 - val_acc: 0.7903\n",
      "Epoch 76/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3752 - acc: 0.8673 - val_loss: 0.5584 - val_acc: 0.7890\n",
      "Epoch 77/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3732 - acc: 0.8685 - val_loss: 0.5612 - val_acc: 0.7930\n",
      "Epoch 78/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3715 - acc: 0.8690 - val_loss: 0.5609 - val_acc: 0.7927\n",
      "Epoch 79/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3696 - acc: 0.8687 - val_loss: 0.5601 - val_acc: 0.7900\n",
      "Epoch 80/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3681 - acc: 0.8699 - val_loss: 0.5604 - val_acc: 0.7890\n",
      "Epoch 81/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3665 - acc: 0.8704 - val_loss: 0.5593 - val_acc: 0.7917\n",
      "Epoch 82/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3645 - acc: 0.8709 - val_loss: 0.5616 - val_acc: 0.7907\n",
      "Epoch 83/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3628 - acc: 0.8721 - val_loss: 0.5629 - val_acc: 0.7910\n",
      "Epoch 84/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3611 - acc: 0.8718 - val_loss: 0.5628 - val_acc: 0.7910\n",
      "Epoch 85/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3594 - acc: 0.8731 - val_loss: 0.5622 - val_acc: 0.7893\n",
      "Epoch 86/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3577 - acc: 0.8735 - val_loss: 0.5636 - val_acc: 0.7893\n",
      "Epoch 87/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3562 - acc: 0.8736 - val_loss: 0.5646 - val_acc: 0.7900\n",
      "Epoch 88/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3548 - acc: 0.8745 - val_loss: 0.5654 - val_acc: 0.7897\n",
      "Epoch 89/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3534 - acc: 0.8748 - val_loss: 0.5655 - val_acc: 0.7903\n",
      "Epoch 90/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3511 - acc: 0.8757 - val_loss: 0.5668 - val_acc: 0.7920\n",
      "Epoch 91/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.3499 - acc: 0.8762 - val_loss: 0.5660 - val_acc: 0.7927\n",
      "Epoch 92/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.3485 - acc: 0.8767 - val_loss: 0.5667 - val_acc: 0.7917\n",
      "Epoch 93/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.3468 - acc: 0.8769 - val_loss: 0.5667 - val_acc: 0.7937\n",
      "Epoch 94/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.3452 - acc: 0.8782 - val_loss: 0.5686 - val_acc: 0.7923\n",
      "Epoch 95/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.3441 - acc: 0.8782 - val_loss: 0.5684 - val_acc: 0.7907\n",
      "Epoch 96/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.3423 - acc: 0.8787 - val_loss: 0.5687 - val_acc: 0.7903\n",
      "Epoch 97/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3405 - acc: 0.8806 - val_loss: 0.5703 - val_acc: 0.7897\n",
      "Epoch 98/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3397 - acc: 0.8803 - val_loss: 0.5700 - val_acc: 0.7950\n",
      "Epoch 99/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3381 - acc: 0.8801 - val_loss: 0.5719 - val_acc: 0.7930\n",
      "Epoch 100/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3363 - acc: 0.8813 - val_loss: 0.5726 - val_acc: 0.7923\n",
      "Epoch 101/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3352 - acc: 0.8818 - val_loss: 0.5735 - val_acc: 0.7887\n",
      "Epoch 102/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3339 - acc: 0.8816 - val_loss: 0.5739 - val_acc: 0.7900\n",
      "Epoch 103/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3324 - acc: 0.8832 - val_loss: 0.5751 - val_acc: 0.7920\n",
      "Epoch 104/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3310 - acc: 0.8839 - val_loss: 0.5755 - val_acc: 0.7927\n",
      "Epoch 105/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3299 - acc: 0.8835 - val_loss: 0.5749 - val_acc: 0.7933\n",
      "Epoch 106/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3282 - acc: 0.8850 - val_loss: 0.5786 - val_acc: 0.7867\n",
      "Epoch 107/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3267 - acc: 0.8852 - val_loss: 0.5779 - val_acc: 0.7937\n",
      "Epoch 108/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3252 - acc: 0.8849 - val_loss: 0.5773 - val_acc: 0.7927\n",
      "Epoch 109/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3242 - acc: 0.8860 - val_loss: 0.5819 - val_acc: 0.7900\n",
      "Epoch 110/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3229 - acc: 0.8862 - val_loss: 0.5841 - val_acc: 0.7893\n",
      "Epoch 111/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3219 - acc: 0.8865 - val_loss: 0.5845 - val_acc: 0.7923\n",
      "Epoch 112/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3205 - acc: 0.8884 - val_loss: 0.5817 - val_acc: 0.7920\n",
      "Epoch 113/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3188 - acc: 0.8878 - val_loss: 0.5862 - val_acc: 0.7903\n",
      "Epoch 114/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3174 - acc: 0.8894 - val_loss: 0.5839 - val_acc: 0.7913\n",
      "Epoch 115/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3166 - acc: 0.8897 - val_loss: 0.5886 - val_acc: 0.7910\n",
      "Epoch 116/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3150 - acc: 0.8891 - val_loss: 0.5870 - val_acc: 0.7907\n",
      "Epoch 117/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3137 - acc: 0.8895 - val_loss: 0.5905 - val_acc: 0.7923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3125 - acc: 0.8908 - val_loss: 0.5891 - val_acc: 0.7907\n",
      "Epoch 119/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3117 - acc: 0.8907 - val_loss: 0.5898 - val_acc: 0.7933\n",
      "Epoch 120/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3102 - acc: 0.8915 - val_loss: 0.5887 - val_acc: 0.7917\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 24us/step\n",
      "4000/4000 [==============================] - 0s 30us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.30688166384082854, 0.8916363636363637]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.56402936065197, 0.803]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, we were able to get a fairly similar validation accuracy of 89.67 (compared to 88.55 in obtained in the first model in this lab). Our test set accuracy went up from 75.8 to a staggering 80.225% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lesson, we not only built an initial deep-learning model, we then used a validation set to tune our model using various types of regularization. From here, we'll continue to describe more practice and theory regarding tuning and optimizing deep-learning networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env-ext",
   "language": "python",
   "name": "learn-env-ext"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
